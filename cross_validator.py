#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èŒç§°è¯„å®¡ææ–™äº¤å‰æ£€éªŒç³»ç»Ÿ - æ ¸å¿ƒæ¨¡å—
"""

import os
import pandas as pd
import zipfile
import tempfile
import shutil
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass, field
from pathlib import Path
import re
from datetime import datetime
import json
import threading
import random
import queue
import hashlib

from google import genai
from google.genai import types
from pypdf import PdfReader
import concurrent.futures
from functools import partial
import time

# å¯¼å…¥æ”¹è¿›çš„ç¼“å­˜ç®¡ç†å™¨
from cache_manager import SmartCacheManager

class APIRotator:
    """
    APIè½®è¯¢ç®¡ç†å™¨ - ç®¡ç†å¤šä¸ªAPIå¯†é’¥çš„è½®è¯¢ä½¿ç”¨
    """
    def __init__(self, api_keys: List[str]):
        self.api_keys = api_keys if api_keys else []
        self.current_index = 0
        self.clients = {}
        self.usage_count = {key: 0 for key in self.api_keys}
        self.last_use_time = {key: 0.0 for key in self.api_keys}
        self.error_count = {key: 0 for key in self.api_keys}
        # ğŸš€ æ–°å¢ï¼šæ€§èƒ½ç›‘æ§ç›¸å…³å±æ€§
        self.response_times = {key: [] for key in self.api_keys}  # å­˜å‚¨æœ€è¿‘10æ¬¡å“åº”æ—¶é—´
        self.avg_response_time = {key: 0.0 for key in self.api_keys}  # å¹³å‡å“åº”æ—¶é—´
        self.success_count = {key: 0 for key in self.api_keys}  # æˆåŠŸè°ƒç”¨æ¬¡æ•°
        self.blacklisted = set()
        self.lock = threading.Lock()
        
        # åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ·ç«¯
        for api_key in self.api_keys:
            try:
                self.clients[api_key] = genai.Client(api_key=api_key)
            except Exception as e:
                print(f"âš ï¸ APIå¯†é’¥åˆå§‹åŒ–å¤±è´¥: {api_key[:10]}... - {e}")
                self.blacklisted.add(api_key)
    
    def get_next_client(self) -> Tuple[genai.Client, str]:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„APIå®¢æˆ·ç«¯ - ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨è½®è¯¢ç­–ç•¥å‡å°‘é”ç«äº‰"""
        with self.lock:
            if not self.api_keys:
                raise ValueError("æœªé…ç½®ä»»ä½•APIå¯†é’¥")
            
            # è¿‡æ»¤æ‰è¢«é»‘åå•çš„API
            available_keys = [key for key in self.api_keys if key not in self.blacklisted]
            
            if not available_keys:
                # å¦‚æœæ‰€æœ‰APIéƒ½è¢«é»‘åå•äº†ï¼Œé‡ç½®é»‘åå•ï¼ˆå¯èƒ½æ˜¯ä¸´æ—¶é—®é¢˜ï¼‰
                self.blacklisted.clear()
                self.error_count = {key: 0 for key in self.api_keys}
                available_keys = self.api_keys
            
            # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è½®è¯¢ç­–ç•¥è€Œéæœ€å°‘ä½¿ç”¨æ¬¡æ•°ï¼Œä» O(n) ä¼˜åŒ–åˆ° O(1)
            if len(available_keys) == 1:
                selected_key = available_keys[0]
            else:
                # æ‰¾åˆ°å½“å‰ç´¢å¼•å¯¹åº”çš„å¯ç”¨å¯†é’¥
                available_indices = [self.api_keys.index(key) for key in available_keys]
                
                # ä»å½“å‰ç´¢å¼•å¼€å§‹æ‰¾ä¸‹ä¸€ä¸ªå¯ç”¨çš„
                next_index = self.current_index
                while next_index not in available_indices:
                    next_index = (next_index + 1) % len(self.api_keys)
                    if next_index == self.current_index:  # é˜²æ­¢æ— é™å¾ªç¯
                        next_index = available_indices[0]
                        break
                
                selected_key = self.api_keys[next_index]
                # æ›´æ–°ä¸‹æ¬¡çš„ç´¢å¼•
                self.current_index = (next_index + 1) % len(self.api_keys)
            
            # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡ï¼ˆä¿ç•™ç»Ÿè®¡åŠŸèƒ½ï¼‰
            self.usage_count[selected_key] += 1
            self.last_use_time[selected_key] = time.time()
            
            return self.clients[selected_key], selected_key
    
    def report_error(self, api_key: str, error: Exception):
        """æŠ¥å‘ŠAPIè°ƒç”¨é”™è¯¯"""
        with self.lock:
            self.error_count[api_key] += 1
            
            # å¦‚æœæŸä¸ªAPIè¿ç»­é”™è¯¯è¶…è¿‡3æ¬¡ï¼Œä¸´æ—¶åŠ å…¥é»‘åå•
            if self.error_count[api_key] >= 3:
                self.blacklisted.add(api_key)
                print(f"ğŸš« APIå¯†é’¥ä¸´æ—¶ç¦ç”¨: {api_key[:10]}... (è¿ç»­é”™è¯¯{self.error_count[api_key]}æ¬¡)")
    
    def report_success(self, api_key: str, response_time: float = 0.0):
        """æŠ¥å‘ŠAPIè°ƒç”¨æˆåŠŸï¼ˆä¼˜åŒ–ï¼šå¢åŠ å“åº”æ—¶é—´ç»Ÿè®¡ï¼‰"""
        with self.lock:
            # æˆåŠŸè°ƒç”¨åé‡ç½®é”™è¯¯è®¡æ•°
            if self.error_count[api_key] > 0:
                self.error_count[api_key] = max(0, self.error_count[api_key] - 1)
            
            # å¦‚æœé”™è¯¯æ¬¡æ•°é™åˆ°0ï¼Œä»é»‘åå•ç§»é™¤
            if self.error_count[api_key] == 0 and api_key in self.blacklisted:
                self.blacklisted.remove(api_key)
            
            # ğŸš€ æ–°å¢ï¼šè®°å½•æˆåŠŸç»Ÿè®¡å’Œå“åº”æ—¶é—´
            self.success_count[api_key] += 1
            
            if response_time > 0:
                # è®°å½•å“åº”æ—¶é—´ï¼ˆä¿ç•™æœ€è¿‘10æ¬¡ï¼‰
                if len(self.response_times[api_key]) >= 10:
                    self.response_times[api_key].pop(0)  # ç§»é™¤æœ€æ—§çš„è®°å½•
                self.response_times[api_key].append(response_time)
                
                # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
                if self.response_times[api_key]:
                    self.avg_response_time[api_key] = sum(self.response_times[api_key]) / len(self.response_times[api_key])
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–APIä½¿ç”¨çŠ¶æ€ï¼ˆä¼˜åŒ–ï¼šå¢åŠ æ€§èƒ½ç»Ÿè®¡ï¼‰"""
        with self.lock:
            return {
                "total_apis": len(self.api_keys),
                "available_apis": len(self.api_keys) - len(self.blacklisted),
                "blacklisted_apis": len(self.blacklisted),
                "usage_stats": dict(self.usage_count),
                "error_stats": dict(self.error_count),
                # ğŸš€ æ–°å¢ï¼šæ€§èƒ½ç›‘æ§ç»Ÿè®¡
                "success_stats": dict(self.success_count),
                "avg_response_times": dict(self.avg_response_time),
                "performance_ranking": self._get_performance_ranking()
            }
    
    def _get_performance_ranking(self) -> List[str]:
        """æ ¹æ®æ€§èƒ½æ’åˆ—APIï¼ˆå“åº”æ—¶é—´ä¼˜å…ˆï¼ŒæˆåŠŸç‡æ¬¡ä¹‹ï¼‰"""
        available_keys = [key for key in self.api_keys if key not in self.blacklisted]
        
        # æŒ‰å¹³å‡å“åº”æ—¶é—´æ’åºï¼ˆå“åº”æ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼‰
        def performance_score(api_key):
            avg_time = self.avg_response_time[api_key]
            success_rate = self.success_count[api_key] / max(1, self.usage_count[api_key])
            # ç»¼åˆè¯„åˆ†ï¼šå“åº”æ—¶é—´è¶ŠçŸ­ã€æˆåŠŸç‡è¶Šé«˜è¶Šå¥½
            if avg_time > 0:
                return success_rate / avg_time  # æˆåŠŸç‡/å“åº”æ—¶é—´
            else:
                return success_rate  # æ²¡æœ‰å“åº”æ—¶é—´æ•°æ®æ—¶åªçœ‹æˆåŠŸç‡
        
        return sorted(available_keys, key=performance_score, reverse=True)

@dataclass
class RuleItem:
    åºå·: int; æ–‡ä»¶ç±»å‹: str; æ ¸å¿ƒé—®é¢˜: str; è¡¥å……è§„åˆ™: str = ""; è§„åˆ™å†…å®¹: str = ""; ä¼˜å…ˆçº§: str = "ä¸­"; å¤‡æ³¨: str = ""; å¡«å†™äºº: str = ""; source_file: str = ""

@dataclass
class MaterialInfo:
    id: int; name: str; file_path: Optional[str] = None; content: Optional[str] = None; is_empty: bool = True
    core_info: Dict[str, Any] = field(default_factory=dict); rule_violations: List[str] = field(default_factory=list)
    processing_method: str = "æœªå¤„ç†"; applicable_rules: List[RuleItem] = field(default_factory=list)

class CrossValidator:
    MATERIAL_NAMES = {
        1: "æ•™è‚²ç»å†", 2: "å·¥ä½œç»å†", 3: "ç»§ç»­æ•™è‚²(åŸ¹è®­æƒ…å†µ)", 4: "å­¦æœ¯æŠ€æœ¯å…¼èŒæƒ…å†µ",
        5: "è·å¥–æƒ…å†µ", 6: "è·å¾—è£èª‰ç§°å·æƒ…å†µ", 7: "ä¸»æŒå‚ä¸ç§‘ç ”é¡¹ç›®(åŸºé‡‘)æƒ…å†µ", 8: "ä¸»æŒå‚ä¸å·¥ç¨‹æŠ€æœ¯é¡¹ç›®æƒ…å†µ",
        9: "è®ºæ–‡", 10: "è‘—(è¯‘)ä½œ(æ•™æ)", 11: "ä¸“åˆ©(è‘—ä½œæƒ)æƒ…å†µ", 12: "ä¸»æŒå‚ä¸æŒ‡å®šæ ‡å‡†æƒ…å†µ",
        13: "æˆæœè¢«æ‰¹ç¤ºã€é‡‡çº³ã€è¿ç”¨å’Œæ¨å¹¿æƒ…å†µ", 14: "èµ„è´¨è¯ä¹¦", 15: "å¥–æƒ©æƒ…å†µ", 16: "è€ƒæ ¸æƒ…å†µ", 17: "ç”³æŠ¥ææ–™é™„ä»¶ä¿¡æ¯"
    }
    
    def __init__(self, api_key: Optional[str] = None, api_keys: Optional[List[str]] = None, rules_dir: str = "rules", progress_callback: Optional[Callable[[str], None]] = None, cache_config: Optional[Dict[str, Any]] = None):
        # é¦–å…ˆè®¾ç½®å›è°ƒå‡½æ•°
        self.progress_callback = progress_callback or (lambda msg: None)
        
        # APIè½®è¯¢é…ç½®
        if api_keys and len(api_keys) > 1:
            # ä½¿ç”¨å¤šä¸ªAPIå¯†é’¥è½®è¯¢
            self.api_rotator = APIRotator(api_keys)
            self.use_rotation = True
            self._log(f"ğŸ”„ å¯ç”¨APIè½®è¯¢æ¨¡å¼: {len(api_keys)}ä¸ªAPIå¯†é’¥")
        elif api_key:
            # ä½¿ç”¨å•ä¸ªAPIå¯†é’¥
            self.api_rotator = APIRotator([api_key])
            self.use_rotation = False
            self._log(f"ğŸ”‘ ä½¿ç”¨å•ä¸ªAPIå¯†é’¥æ¨¡å¼")
        else:
            raise ValueError("å¿…é¡»æä¾› api_key æˆ– api_keys å‚æ•°")
        
        self.rules_dir = Path(rules_dir)
        # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯ä½œä¸ºé»˜è®¤å®¢æˆ·ç«¯
        self.client, _ = self.api_rotator.get_next_client()
        self.materials: Dict[int, MaterialInfo] = {i: MaterialInfo(id=i, name=self.MATERIAL_NAMES[i]) for i in range(1, 18)}
        self.rules: List[RuleItem] = []
        self.high_priority_violations: List[str] = []
        self.validation_results = {"empty_materials": [], "final_report": ""}
        
        # åˆå§‹åŒ–æ”¹è¿›çš„ç¼“å­˜ç®¡ç†å™¨
        cache_config = cache_config or {}
        self.cache_manager = SmartCacheManager(
            cache_dir=cache_config.get('cache_dir'),
            max_age_hours=cache_config.get('max_age_hours', 24),
            max_memory_items=cache_config.get('max_memory_items', 100),
            max_disk_size_mb=cache_config.get('max_disk_size_mb', 1000),
            enable_redis=cache_config.get('enable_redis', False),
            redis_url=cache_config.get('redis_url'),
            progress_callback=self.progress_callback
        )
        
        # ä¿æŒå…¼å®¹æ€§ï¼Œä½†ä½¿ç”¨æ–°çš„ç¼“å­˜ç®¡ç†å™¨
        # self.content_cache: Dict[str, str] = {}  # å·²è¢« SmartCacheManager æ›¿ä»£
        
        # æ·»åŠ é€Ÿç‡é™åˆ¶ç›¸å…³å±æ€§
        self.last_api_call_time = 0
        self.min_call_interval = 0.1  # æœ€å°è°ƒç”¨é—´éš”ä¸º0.1ç§’
        self.progress_callback("åˆå§‹åŒ–äº¤å‰æ£€éªŒç³»ç»Ÿ...")
        self.load_rules()

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self.cache_manager.get_stats()
    
    def print_cache_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        self.cache_manager.print_stats()
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.cache_manager.clear()
        self._log("ğŸ§¤ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")
    
    def _log(self, message: str):
        self.progress_callback(message)

    def _rotated_api_call(self, call_func: Callable, max_retries: int = 3) -> Any:
        """
        ä½¿ç”¨APIè½®è¯¢æœºåˆ¶è¿›è¡ŒAPIè°ƒç”¨ï¼ˆä¼˜åŒ–ï¼šå¢åŠ å“åº”æ—¶é—´ç›‘æ§ï¼‰
        """
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                # è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„å®¢æˆ·ç«¯
                client, api_key = self.api_rotator.get_next_client()
                
                # é€Ÿç‡é™åˆ¶
                current_time = time.time()
                if current_time - self.last_api_call_time < self.min_call_interval:
                    time.sleep(self.min_call_interval - (current_time - self.last_api_call_time))
                
                # ğŸš€ ä¼˜åŒ–ï¼šè®°å½•å“åº”æ—¶é—´
                call_start_time = time.time()
                
                # æ‰§è¡ŒAPIè°ƒç”¨
                result = call_func(client)
                
                # è®¡ç®—å“åº”æ—¶é—´
                response_time = time.time() - call_start_time
                self.last_api_call_time = time.time()
                
                # æŠ¥å‘ŠæˆåŠŸï¼ˆåŒ…å«å“åº”æ—¶é—´ï¼‰
                self.api_rotator.report_success(api_key, response_time)
                
                if self.use_rotation:
                    # è®°å½•æˆåŠŸä½¿ç”¨çš„APIå’Œæ€§èƒ½ä¿¡æ¯
                    self._log(f"  ğŸ”„ APIè½®è¯¢: ä½¿ç”¨ {api_key[:10]}... (å“åº” {response_time:.2f}s, å°è¯• {attempt+1}/{max_retries})")
                
                return result
                
            except Exception as e:
                last_exception = e
                
                # æŠ¥å‘Šé”™è¯¯
                current_api_key = ""
                if hasattr(self, 'api_rotator'):
                    try:
                        client, current_api_key = self.api_rotator.get_next_client()  # è·å–å½“å‰ä½¿ç”¨çš„key
                        self.api_rotator.report_error(current_api_key, e)
                    except Exception:
                        current_api_key = "unknown"
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºé€Ÿç‡é™åˆ¶é”™è¯¯
                error_str = str(e).lower()
                if any(keyword in error_str for keyword in ['rate limit', 'quota', 'too many requests', '429']):
                    self._log(f"  âš ï¸ APIé€Ÿç‡é™åˆ¶: {current_api_key[:10]}... ç­‰å¾…åé‡è¯• ({attempt+1}/{max_retries})")
                    # é€Ÿç‡é™åˆ¶æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                    wait_time = (2 ** attempt) * (1 + random.uniform(0, 0.5))  # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                    time.sleep(min(wait_time, 60))  # æœ€å¤šç­‰å¾…60ç§’
                elif 'invalid api key' in error_str or 'api_key_invalid' in error_str:
                    self._log(f"  âŒ APIå¯†é’¥æ— æ•ˆ: {current_api_key[:10]}... å°è¯•ä¸‹ä¸€ä¸ª")
                    # APIå¯†é’¥æ— æ•ˆæ—¶ä¸ç­‰å¾…ï¼Œç›´æ¥å°è¯•ä¸‹ä¸€ä¸ª
                    pass
                else:
                    self._log(f"  âš ï¸ APIè°ƒç”¨å¤±è´¥: {str(e)[:50]}... ç­‰å¾…åé‡è¯• ({attempt+1}/{max_retries})")
                    time.sleep(2 ** attempt)  # æ™®é€šé”™è¯¯çš„æŒ‡æ•°é€€é¿
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥åï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
        raise last_exception or Exception(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")

    def _smart_truncate_content(self, content: str, max_length: int = 200000, head_size: int = 5000, tail_size: int = 5000) -> str:
        """
        æ™ºèƒ½æˆªå–å†…å®¹ï¼šå½“å†…å®¹è¶…è¿‡max_lengthæ—¶ï¼Œä¿ç•™å¤´éƒ¨head_sizeå­—ç¬¦å’Œå°¾éƒ¨tail_sizeå­—ç¬¦
        
        Args:
            content: è¦æˆªå–çš„å†…å®¹
            max_length: æœ€å¤§é•¿åº¦é˜ˆå€¼ï¼Œé»˜è®¤200000å­—ç¬¦
            head_size: å¤´éƒ¨ä¿ç•™å­—ç¬¦æ•°ï¼Œé»˜è®¤5000å­—ç¬¦
            tail_size: å°¾éƒ¨ä¿ç•™å­—ç¬¦æ•°ï¼Œé»˜è®¤5000å­—ç¬¦
            
        Returns:
            æˆªå–åçš„å†…å®¹
        """
        if not content or len(content) <= max_length:
            return content
            
        # è®¡ç®—çœç•¥çš„å­—ç¬¦æ•°
        omitted_chars = len(content) - head_size - tail_size
        
        # æ„å»ºæˆªå–åçš„å†…å®¹
        head_part = content[:head_size]
        tail_part = content[-tail_size:] if tail_size > 0 else ""
        
        truncated_content = (
            head_part + 
            f"\n\n[... ä¸­é—´çœç•¥ {omitted_chars:,} ä¸ªå­—ç¬¦ ...]\n\n" + 
            tail_part
        )
        
        return truncated_content

    def _safe_basename(self, file_path: str) -> str:
        """å®‰å…¨åœ°è·å–æ–‡ä»¶åï¼Œç¡®ä¿ä¸­æ–‡æ–‡ä»¶åæ­£ç¡®æ˜¾ç¤º"""
        try:
            # è·å–åŸºæœ¬æ–‡ä»¶å
            basename = os.path.basename(file_path)
            
            # å¤„ç†å­—èŠ‚æ•°ç»„æ ¼å¼
            if isinstance(basename, bytes):
                # å°è¯•ç”¨ä¸åŒç¼–ç è§£ç 
                for encoding in ['utf-8', 'gbk', 'cp936', 'cp437']:
                    try:
                        return basename.decode(encoding)
                    except UnicodeDecodeError:
                        continue
                # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†
                return basename.decode('utf-8', errors='replace')
            
            # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼ï¼Œæ¸…ç†å¯èƒ½å­˜åœ¨çš„é—®é¢˜å­—ç¬¦
            # ç§»é™¤æˆ–æ›¿æ¢ä¸å¯è¯»å­—ç¬¦
            cleaned_name = ''.join(c if ord(c) < 65536 and c.isprintable() or c in 'ä¸€-é¿¿' else '_' for c in basename)
            
            # å¦‚æœæ¸…ç†åçš„æ–‡ä»¶åä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›é»˜è®¤å€¼
            if not cleaned_name or len(cleaned_name) < 2:
                return "æœªçŸ¥æ–‡ä»¶"
                
            return cleaned_name
            
        except Exception as e:
            # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return f"æ–‡ä»¶å¤„ç†é”™è¯¯_{str(e)[:10]}"

    def _detect_zip_encoding(self, zip_path: str) -> str:
        """æ£€æµ‹ZIPæ–‡ä»¶çš„ç¼–ç æ–¹å¼"""
        try:
            with zipfile.ZipFile(zip_path, 'r') as zf:
                # æ£€æŸ¥æ–‡ä»¶åä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡æˆ–ç‰¹æ®Šå­—ç¬¦
                has_chinese_files = False
                has_encoding_issues = False
                
                for file_info in zf.filelist:
                    filename = file_info.filename
                    
                    # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                    if any('ä¸€' <= c <= 'é¿¿' for c in filename):
                        has_chinese_files = True
                    
                    # æ£€æµ‹æ˜¯å¦åŒ…å«ç¼–ç é—®é¢˜çš„å­—ç¬¦
                    if any(ord(c) > 127 and not ('ä¸€' <= c <= 'é¿¿') for c in filename):
                        has_encoding_issues = True
                
                if has_chinese_files and not has_encoding_issues:
                    return "utf-8"  # æ­£å¸¸çš„UTF-8ç¼–ç 
                elif has_encoding_issues:
                    return "mixed"  # æ··åˆç¼–ç ï¼Œéœ€è¦ä¿®å¤
                else:
                    return "ascii"  # çº¯ ASCIIæ–‡ä»¶å
                    
        except Exception:
            return "unknown"  # æ— æ³•æ£€æµ‹

    def _try_alternative_zip_extraction(self, zip_path: str, temp_dir: str) -> bool:
        """å°è¯•æ›¿ä»£çš„ZIPè§£å‹æ–¹æ³•"""
        self._log(f"  ğŸ”„ å°è¯•æ›¿ä»£è§£å‹æ–¹æ³•...")
        
        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨ä¸åŒçš„zipfileå‚æ•°
            import codecs
            
            # å°è¯•ä½¿ç”¨cp437ç¼–ç è¯»å–
            with zipfile.ZipFile(zip_path, 'r') as zf:
                success_count = 0
                total_count = len([f for f in zf.filelist if not f.filename.endswith('/')])
                
                for file_info in zf.filelist:
                    if file_info.filename.endswith('/'):
                        continue
                        
                    try:
                        # å°è¯•ç¼–ç è½¬æ¢
                        original_name = file_info.filename
                        
                        # å°è¯•å¤šç§è§£ç æ–¹æ¡ˆ
                        for encoding in ['utf-8', 'gbk', 'cp936', 'big5']:
                            try:
                                # å…ˆç¼–ç ä¸ºcp437ï¼Œå†ç”¨ç›®æ ‡ç¼–ç è§£ç 
                                encoded_bytes = original_name.encode('cp437')
                                decoded_name = encoded_bytes.decode(encoding)
                                
                                # æ£€æŸ¥è§£ç åæ˜¯å¦åŒ…å«ä¸­æ–‡
                                if any('ä¸€' <= c <= 'é¿¿' for c in decoded_name):
                                    # ä½¿ç”¨è§£ç åçš„æ–‡ä»¶å
                                    file_info.filename = decoded_name
                                    self._log(f"    ğŸ”§ ç¼–ç ä¿®å¤: {original_name[:20]}... -> {decoded_name[:20]}...")
                                    break
                                    
                            except (UnicodeDecodeError, UnicodeEncodeError):
                                continue
                        
                        # ç¡®ä¿ç›®å½•å­˜åœ¨
                        file_dir = os.path.dirname(os.path.join(temp_dir, file_info.filename))
                        if file_dir:
                            os.makedirs(file_dir, exist_ok=True)
                        
                        # è§£å‹æ–‡ä»¶
                        extracted_path = zf.extract(file_info, temp_dir)
                        success_count += 1
                        
                    except Exception as e:
                        # è®°å½•ä½†ä¸åœæ­¢å¤„ç†
                        self._log(f"    âš ï¸ è·³è¿‡æ–‡ä»¶: {self._safe_basename(file_info.filename)} - {str(e)[:30]}...")
                
                self._log(f"  ğŸ“Š æ›¿ä»£è§£å‹ç»“æœ: {success_count}/{total_count} æ–‡ä»¶æˆåŠŸ")
                return success_count > 0
                
        except Exception as e:
            self._log(f"  âŒ æ›¿ä»£è§£å‹æ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
            return False

    def _normalize_filename(self, filename: str) -> str:
        """æ ‡å‡†åŒ–æ–‡ä»¶åï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç é—®é¢˜"""
        try:
            # ç§»é™¤æˆ–æ›¿æ¢é—®é¢˜å­—ç¬¦
            # åˆ—å‡ºä¸€äº›å¸¸è§çš„é—®é¢˜å­—ç¬¦
            problem_chars = {
                'ï¼ˆ': '(',  # å…¨è§’æ‹¬å·
                'ï¼‰': ')',
                'ï¼Œ': ',',  # å…¨è§’é€—å·
                'ã€': ',',  # ä¸­æ–‡é€—å·
                'ï¼š': ':',  # å…¨è§’å†’å·
                'ï¼›': ';',  # å…¨è§’åˆ†å·
                'â€œ': '"', # ä¸­æ–‡å¼•å·
                'â€': '"',
                'â€˜': "'",
                'â€™': "'",
            }
            
            normalized = filename
            for old_char, new_char in problem_chars.items():
                normalized = normalized.replace(old_char, new_char)
            
            # ç§»é™¤ä¸å¯è§å­—ç¬¦å’Œæ§åˆ¶å­—ç¬¦
            normalized = ''.join(c for c in normalized if c.isprintable() or ord(c) >= 0x4e00)
            
            return normalized if normalized else "æ¸…ç†åçš„æ–‡ä»¶"
            
        except Exception:
            return filename  # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›åŸæ–‡ä»¶å
        """æ ‡å‡†åŒ–æ–‡ä»¶åï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç é—®é¢˜"""
        try:
            # ç§»é™¤æˆ–æ›¿æ¢é—®é¢˜å­—ç¬¦
            # åˆ—å‡ºä¸€äº›å¸¸è§çš„é—®é¢˜å­—ç¬¦
            problem_chars = {
                'ï¼ˆ': '(',  # å…¨è§’æ‹¬å·
                'ï¼‰': ')',
                'ï¼Œ': ',',  # å…¨è§’é€—å·
                'ã€': ',',  # ä¸­æ–‡é€—å·
                'ï¼š': ':',  # å…¨è§’å†’å·
                'ï¼›': ';',  # å…¨è§’åˆ†å·
                'â€œ': '"', # ä¸­æ–‡å¼•å·
                'â€': '"',
                'â€˜': "'",
                'â€™': "'",
            }
            
            normalized = filename
            for old_char, new_char in problem_chars.items():
                normalized = normalized.replace(old_char, new_char)
            
            # ç§»é™¤ä¸å¯è§å­—ç¬¦å’Œæ§åˆ¶å­—ç¬¦
            normalized = ''.join(c for c in normalized if c.isprintable() or ord(c) >= 0x4e00)
            
            return normalized if normalized else "æ¸…ç†åçš„æ–‡ä»¶"
            
        except Exception:
            return filename  # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›åŸæ–‡ä»¶å

    def _load_markdown_rules(self, md_file: Path):
        try:
            content = md_file.read_text(encoding='utf-8')
            rule_blocks = re.split(r'\n#\s+', content)
            if not rule_blocks[0].startswith('#'): rule_blocks[0] = '# ' + rule_blocks[0]
            md_rule_count = 0
            for i, block in enumerate(rule_blocks):
                if not block.strip(): continue
                lines = block.strip().split('\n')
                core_issue, rule_content = lines[0].strip().lstrip('# '), "\n".join(lines[1:]).strip()
                if core_issue and rule_content:
                    self.rules.append(RuleItem(åºå·=1000 + i, æ–‡ä»¶ç±»å‹="æ•´ä½“", æ ¸å¿ƒé—®é¢˜=core_issue, è§„åˆ™å†…å®¹=rule_content, ä¼˜å…ˆçº§="æé«˜", å¤‡æ³¨="æ¥è‡ªé€šç”¨è§„åˆ™.md"))
                    md_rule_count += 1
            self._log(f"  âœ… æˆåŠŸåŠ è½½ {md_rule_count} æ¡é€šç”¨Markdownè§„åˆ™")
        except Exception as e:
            self._log(f"âŒ åŠ è½½é€šç”¨è§„åˆ™.mdå¤±è´¥: {e}")

    def load_rules(self):
        self._log("ğŸ“‹ å¼€å§‹åŠ è½½è§„åˆ™é›†...")
        markdown_rule_file = self.rules_dir / "é€šç”¨è§„åˆ™.md"
        if markdown_rule_file.exists(): self._load_markdown_rules(markdown_rule_file)
        else: self._log("  â„¹ï¸ æœªæ‰¾åˆ°é€šç”¨è§„åˆ™.mdæ–‡ä»¶ï¼Œè·³è¿‡åŠ è½½ã€‚")
        for excel_file in self.rules_dir.glob("*.xlsx"):
            self._log(f"  ğŸ“„ æ­£åœ¨åŠ è½½Excelè§„åˆ™: {excel_file.name}")
            self._load_single_rule_file(excel_file)
        self._log(f"ğŸ“Š è§„åˆ™é›†åŠ è½½å®Œæˆ: å…± {len(self.rules)} æ¡è§„åˆ™")

    def _load_single_rule_file(self, rules_file: Path):
        try:
            df = pd.read_excel(rules_file)
            file_name = rules_file.name  # è·å–æ–‡ä»¶å
            for _, row in df.iterrows():
                try:
                    # å®‰å…¨åœ°æ£€æŸ¥pandasçš„nanå€¼
                    åºå·_val = row.get('åºå·')
                    æ ¸å¿ƒé—®é¢˜_val = row.get('æ ¸å¿ƒé—®é¢˜')
                    
                    # ä½¿ç”¨bool()å‡½æ•°æ˜¾å¼è½¬æ¢æ¥é¿å…pandasçš„__bool__é—®é¢˜
                    if bool(pd.notna(åºå·_val)) and bool(pd.notna(æ ¸å¿ƒé—®é¢˜_val)):
                        # å®‰å…¨çš„ç±»å‹è½¬æ¢
                        try:
                            åºå·_int = int(åºå·_val) if åºå·_val is not None else 0
                        except (ValueError, TypeError):
                            åºå·_int = 0
                            
                        rule = RuleItem(
                            åºå·=åºå·_int, 
                            æ–‡ä»¶ç±»å‹=str(row.get('æ–‡ä»¶ç±»å‹', '')).strip(), 
                            æ ¸å¿ƒé—®é¢˜=str(æ ¸å¿ƒé—®é¢˜_val).strip(), 
                            è§„åˆ™å†…å®¹=str(row.get('è§„åˆ™å†…å®¹ï¼ˆè¶Šè¯¦ç»†è¶Šå¥½ï¼‰', '')).strip(), 
                            ä¼˜å…ˆçº§=str(row.get('ä¼˜å…ˆçº§', 'ä¸­')).strip(),
                            source_file=file_name  # è®°å½•æ¥æºæ–‡ä»¶
                        )
                        self.rules.append(rule)
                except Exception as e:
                    self._log(f"    âš ï¸ è·³è¿‡æ— æ•ˆè§„åˆ™è¡Œ: {e}")
                    continue
        except Exception as e:
            self._log(f"âŒ åŠ è½½Excelè§„åˆ™ {rules_file.name} å¤±è´¥: {e}")

    def process_materials_from_zip(self, zip_path: str, temp_dir: str):
        self._log(f"ğŸ“¦ å¼€å§‹ä» {os.path.basename(zip_path)} æå–ææ–™...")
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref: 
                # ä¿®å¤ä¸­æ–‡æ–‡ä»¶åä¹±ç é—®é¢˜ - å¢å¼ºç‰ˆ
                fixed_count = 0
                total_files = len(zip_ref.filelist)
                self._log(f"  ğŸ“ æ£€æµ‹åˆ° {total_files} ä¸ªæ–‡ä»¶/ç›®å½•")
                
                # åˆ›å»ºä¸€ä¸ªæ–°çš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ï¼Œä¿®å¤ç¼–ç é—®é¢˜
                for i, file_info in enumerate(zip_ref.filelist):
                    original_filename = file_info.filename
                    
                    # è·³è¿‡ç©ºæ–‡ä»¶å
                    if not original_filename:
                        continue
                        
                    # æ˜¾ç¤ºå¤„ç†è¿›åº¦
                    if total_files > 10 and i % max(1, total_files // 10) == 0:
                        self._log(f"  ğŸ”„ å¤„ç†æ–‡ä»¶åè¿›åº¦: {i+1}/{total_files}")
                    
                    # å°è¯•å¤šç§ç¼–ç ä¿®å¤æ–¹æ¡ˆ
                    try:
                        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡æˆ–ç‰¹æ®Šå­—ç¬¦
                        has_chinese = any(ord(c) > 127 for c in original_filename if isinstance(c, str))
                        
                        if has_chinese:
                            # å°è¯•ä¸åŒçš„ç¼–ç è½¬æ¢æ–¹æ¡ˆ
                            encoding_attempts = [
                                ('cp437', 'gbk'),      # å¸¸è§çš„Windows ZIPç¼–ç é—®é¢˜
                                ('cp437', 'utf-8'),    # å¦ä¸€ç§å¯èƒ½çš„ç¼–ç 
                                ('cp437', 'cp936'),    # ä¸­æ–‡Windowsç¼–ç 
                                ('latin1', 'gbk'),     # Latin1åˆ°GBK
                                ('iso-8859-1', 'utf-8') # ISOåˆ°UTF-8
                            ]
                            
                            for from_enc, to_enc in encoding_attempts:
                                try:
                                    # å°è¯•ç¼–ç è½¬æ¢
                                    encoded_bytes = original_filename.encode(from_enc)
                                    fixed_filename = encoded_bytes.decode(to_enc)
                                    
                                    # éªŒè¯è½¬æ¢åçš„æ–‡ä»¶åæ˜¯å¦åˆç†
                                    if fixed_filename != original_filename and len(fixed_filename) > 0:
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                                        if any('ä¸€' <= c <= 'é¿¿' for c in fixed_filename):
                                            file_info.filename = fixed_filename
                                            fixed_count += 1
                                            self._log(f"  ğŸ”§ ä¿®å¤æ–‡ä»¶åç¼–ç  ({from_enc}->{to_enc}): {original_filename[:30]}... -> {fixed_filename[:30]}...")
                                            break
                                            
                                except (UnicodeEncodeError, UnicodeDecodeError, UnicodeError):
                                    continue
                            
                            # å¦‚æœæ‰€æœ‰ç¼–ç è½¬æ¢éƒ½å¤±è´¥ï¼Œå°è¯•æ¸…ç†æ–‡ä»¶å
                            if file_info.filename == original_filename:
                                cleaned_filename = self._normalize_filename(original_filename)
                                if cleaned_filename != original_filename:
                                    file_info.filename = cleaned_filename
                                    fixed_count += 1
                                    self._log(f"  ğŸ§¼ æ¸…ç†æ–‡ä»¶å: {original_filename[:30]}... -> {cleaned_filename[:30]}...")
                                    
                    except Exception as e:
                        self._log(f"  âš ï¸ å¤„ç†æ–‡ä»¶åæ—¶å‡ºé”™: {original_filename[:20]}... - {e}")
                
                if fixed_count > 0:
                    self._log(f"  âœ… æˆåŠŸä¿®å¤ {fixed_count} ä¸ªæ–‡ä»¶åçš„ç¼–ç é—®é¢˜")
                else:
                    self._log(f"  ğŸ“ æœªå‘ç°éœ€è¦ä¿®å¤çš„æ–‡ä»¶åç¼–ç é—®é¢˜")
                
                # æ‰§è¡Œè§£å‹ï¼Œä½¿ç”¨å®¹é”™æ¨¡å¼
                try:
                    self._log(f"  ğŸ“¦ å¼€å§‹è§£å‹æ–‡ä»¶...")
                    zip_ref.extractall(temp_dir)
                    self._log(f"  âœ… æ–‡ä»¶è§£å‹å®Œæˆ")
                    
                except Exception as extract_error:
                    error_msg = str(extract_error)
                    self._log(f"  âš ï¸ æ ‡å‡†è§£å‹å¤±è´¥: {error_msg}")
                    
                    # åˆ†æé”™è¯¯ç±»å‹
                    if "There is no item named" in error_msg:
                        self._log(f"  ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶åç¼–ç é—®é¢˜ï¼Œå°è¯•é€ä¸ªæ–‡ä»¶è§£å‹...")
                    else:
                        self._log(f"  ğŸ” æ£€æµ‹åˆ°å…¶ä»–è§£å‹é—®é¢˜ï¼Œå°è¯•æ›¿ä»£æ–¹æ¡ˆ...")
                    
                    # é€ä¸ªæ–‡ä»¶è§£å‹ï¼Œè·³è¿‡æœ‰é—®é¢˜çš„æ–‡ä»¶
                    success_count = 0
                    error_count = 0
                    
                    for file_info in zip_ref.filelist:
                        try:
                            # è·³è¿‡ç›®å½•
                            if file_info.filename.endswith('/'):
                                continue
                                
                            # ç¡®ä¿ç›®å½•å­˜åœ¨
                            file_dir = os.path.dirname(os.path.join(temp_dir, file_info.filename))
                            if file_dir:
                                os.makedirs(file_dir, exist_ok=True)
                            
                            # è§£å‹å•ä¸ªæ–‡ä»¶
                            zip_ref.extract(file_info, temp_dir)
                            success_count += 1
                            
                        except Exception as file_error:
                            error_count += 1
                            safe_filename = self._safe_basename(file_info.filename)
                            self._log(f"    âŒ è·³è¿‡æ— æ³•è§£å‹çš„æ–‡ä»¶: {safe_filename} - {str(file_error)[:50]}...")
                    
                    self._log(f"  ğŸ“Š è§£å‹ç»“æœ: æˆåŠŸ {success_count} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡ {error_count} ä¸ªé—®é¢˜æ–‡ä»¶")
                    
                    if success_count == 0:
                        raise Exception(f"æ— æ³•è§£å‹ä»»ä½•æ–‡ä»¶ï¼ŒZIPæ–‡ä»¶å¯èƒ½æŸåæˆ–ç¼–ç ä¸å…¼å®¹")
                        
        except Exception as e:
            self._log(f"âŒ è§£å‹ZIPæ–‡ä»¶å¤±è´¥: {e}")
            self._log(f"ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            self._log(f"  1. æ£€æŸ¥ZIPæ–‡ä»¶æ˜¯å¦å®Œæ•´æ— æŸå")
            self._log(f"  2. å°è¯•ä½¿ç”¨7-Zipæˆ–WinRARé‡æ–°æ‰“åŒ…æ–‡ä»¶")
            self._log(f"  3. ç¡®ä¿æ–‡ä»¶åä¸åŒ…å«ç‰¹æ®Šå­—ç¬¦å¦‚: < > : \" | ? * ")
            self._log(f"  4. å°è¯•ä½¿ç”¨UTF-8ç¼–ç é‡æ–°åˆ›å»ºZIPæ–‡ä»¶")
            return
        material_files = self._map_files_to_materials(temp_dir)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†PDFæ–‡ä»¶ï¼Œå®ç°å¤§æ–‡ä»¶ç»å¯¹ä¼˜å…ˆçš„å¹¶å‘ç­–ç•¥
        optimal_workers = min(8, len(self.api_rotator.api_keys)) if len(self.api_rotator.api_keys) > 1 else 3  # æå‡å¹¶å‘æ•°
        
        # æ™ºèƒ½åˆ†ç»„ï¼šæŒ‰æ–‡ä»¶å¤§å°ä¸¥æ ¼åˆ†ç»„ï¼Œç¡®ä¿å¤§æ–‡ä»¶ç»å¯¹ä¼˜å…ˆ
        large_materials = {}  # â‰¥5MBçš„ææ–™ç»„
        medium_materials = {}  # 1-5MBçš„ææ–™ç»„  
        small_materials = {}  # <1MBçš„ææ–™ç»„
        
        for mid, files in material_files.items():
            if not files:
                continue
                
            # è®¡ç®—ææ–™ç»„çš„æ€»æ–‡ä»¶å¤§å°
            total_size = 0
            for file_path in files:
                try:
                    total_size += os.path.getsize(file_path)
                except:
                    pass
            
            # æŒ‰æ€»å¤§å°åˆ†ç»„ï¼ˆç¡®ä¿å¤§ææ–™ä¼˜å…ˆå¤„ç†ï¼‰
            if total_size >= 5 * 1024 * 1024:  # â‰¥5MB
                large_materials[mid] = files
            elif total_size >= 1 * 1024 * 1024:  # 1-5MB  
                medium_materials[mid] = files
            else:  # <1MB
                small_materials[mid] = files
        
        self._log(f"  ğŸ“Š ææ–™æ™ºèƒ½åˆ†ç»„: å¤§ææ–™ç»„{len(large_materials)}ä¸ª > ä¸­ææ–™ç»„{len(medium_materials)}ä¸ª > å°ææ–™ç»„{len(small_materials)}ä¸ª")
        
        # ä¸‰é˜¶æ®µå¹¶å‘å¤„ç†ç­–ç•¥ï¼šä¸¥æ ¼æŒ‰å¤§å°ä¼˜å…ˆ
        all_material_groups = [
            (large_materials, "ğŸš€ ã€ç¬¬ä¸€é˜¶æ®µã€‘å¤§ææ–™ç»„å¹¶å‘å¤„ç†", optimal_workers),
            (medium_materials, "âš¡ ã€ç¬¬äºŒé˜¶æ®µã€‘ä¸­ææ–™ç»„å¹¶å‘å¤„ç†", max(optimal_workers//2, 2)),
            (small_materials, "ğŸ“ ã€ç¬¬ä¸‰é˜¶æ®µã€‘å°ææ–™ç»„å¹¶å‘å¤„ç†", max(optimal_workers//3, 1))
        ]
        
        total_processed = 0
        for materials_group, phase_name, workers in all_material_groups:
            if not materials_group:
                continue
                
            self._log(f"  {phase_name}: {len(materials_group)}ä¸ªææ–™, ä½¿ç”¨{workers}ä¸ªå·¥ä½œçº¿ç¨‹")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
                extract_func = partial(self._extract_single_file_content_wrapper)
                
                # æäº¤å½“å‰é˜¶æ®µçš„æ‰€æœ‰ä»»åŠ¡
                future_to_mid = {}
                for mid, files in materials_group.items():
                    future = executor.submit(extract_func, mid, files)
                    future_to_mid[future] = mid
                
                # æ”¶é›†å½“å‰é˜¶æ®µç»“æœï¼Œæ”¹è¿›è¶…æ—¶å¤„ç†
                phase_completed = 0
                unfinished_futures = set(future_to_mid.keys())
                
                try:
                    for future in concurrent.futures.as_completed(future_to_mid, timeout=600):  # 10åˆ†é’Ÿé˜¶æ®µè¶…æ—¶
                        mid = future_to_mid[future]
                        unfinished_futures.discard(future)  # ç§»é™¤å·²å®Œæˆçš„future
                        
                        try:
                            content = future.result(timeout=180)  # å•ä»»åŠ¡3åˆ†é’Ÿè¶…æ—¶
                            if content and len(content.strip()) > 50:
                                self.materials[mid].is_empty = False
                                self.materials[mid].content = content
                                phase_completed += 1
                                total_processed += 1
                                self._log(f"  âœ… ææ–™{mid}å¤„ç†å®Œæˆ ({phase_completed}/{len(materials_group)}): {len(content.strip())}å­—ç¬¦")
                            else:
                                self._log(f"  âš ï¸ ææ–™{mid}å†…å®¹è¿‡å°‘ ({phase_completed}/{len(materials_group)})")
                                self.materials[mid].content = content or f"ææ–™{mid}å†…å®¹ä¸ºç©º"
                                phase_completed += 1
                                total_processed += 1
                        except Exception as e:
                            error_msg = str(e)[:100]
                            self._log(f"  âŒ ææ–™{mid}å¤„ç†å¤±è´¥ ({phase_completed}/{len(materials_group)}): {error_msg}...")
                            self.materials[mid].content = f"ææ–™{mid}å¤„ç†å¤±è´¥: {error_msg}"
                            phase_completed += 1
                            total_processed += 1
                            
                except concurrent.futures.TimeoutError:
                    self._log(f"  â° {phase_name}é˜¶æ®µè¶…æ—¶ï¼Œæœ‰{len(unfinished_futures)}ä¸ªä»»åŠ¡æœªå®Œæˆ")
                    
                    # å¤„ç†æœªå®Œæˆçš„futures
                    for future in unfinished_futures:
                        mid = future_to_mid[future]
                        try:
                            # å°è¯•å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                            if not future.done():
                                future.cancel()
                            self.materials[mid].content = f"ææ–™{mid}å¤„ç†è¶…æ—¶"
                            phase_completed += 1
                            total_processed += 1
                            self._log(f"  â° ææ–™{mid}è¶…æ—¶å¤„ç† ({phase_completed}/{len(materials_group)})")
                        except Exception as e:
                            self._log(f"  âŒ ææ–™{mid}è¶…æ—¶å¤„ç†å¤±è´¥: {e}")
                
                # ç¡®ä¿æ‰€æœ‰ææ–™éƒ½è¢«å¤„ç†ï¼ˆå…œåº•å¤„ç†ï¼‰
                for mid in materials_group.keys():
                    if self.materials[mid].content is None:
                        self.materials[mid].content = f"ææ–™{mid}æœªè¢«å¤„ç†"
                        phase_completed += 1
                        total_processed += 1
                        self._log(f"  âš ï¸ ææ–™{mid}å…œåº•å¤„ç† ({phase_completed}/{len(materials_group)})")
            
            self._log(f"  ğŸ {phase_name}å®Œæˆ: {phase_completed}/{len(materials_group)}ä¸ªææ–™")
        
        self._log(f"  ğŸ“Š ä¸‰é˜¶æ®µå¤„ç†å®Œæˆ: æ€»è®¡å¤„ç† {total_processed} ä¸ªææ–™")
        
        # æ£€æŸ¥ææ–™å®Œæ•´æ€§å’Œè§„åˆ™åŒ¹é…
        self.check_empty_materials()
        self._match_rules_to_materials()
        
        # è¾“å‡ºå¤„ç†ç»“æœæ‘˜è¦
        total_materials = len(self.materials)
        valid_materials = sum(1 for m in self.materials.values() if not m.is_empty)
        empty_materials = total_materials - valid_materials
        
        self._log(f"  ğŸ“ˆ å¤„ç†ç»“æœæ‘˜è¦:")
        self._log(f"    - æœ‰æ•ˆææ–™: {valid_materials}/{total_materials}")
        self._log(f"    - ç©ºææ–™: {empty_materials}/{total_materials}")
        self._log(f"    - åŠ è½½è§„åˆ™: {len(self.rules)} æ¡")
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        self.print_cache_stats()
        
        if valid_materials == 0:
            self._log(f"  âš ï¸ è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆææ–™ï¼Œå°†ç”Ÿæˆç©ºæŠ¥å‘Š")
        else:
            self._log(f"  âœ… ç³»ç»Ÿå°†ç»§ç»­å¤„ç† {valid_materials} ä¸ªæœ‰æ•ˆææ–™")
        
        # å¼ºåˆ¶æ¨è¿›æ ‡è®°
        self._force_continue = True
    
    def _fallback_individual_extraction(self, zip_ref, temp_dir: str):
        """å¤‡ç”¨çš„é€ä¸ªæ–‡ä»¶è§£å‹æ–¹æ³•"""
        self._log(f"  ğŸ”„ å°è¯•é€ä¸ªæ–‡ä»¶è§£å‹...")
        success_count = 0
        error_count = 0
        
        for file_info in zip_ref.filelist:
            try:
                # è·³è¿‡ç›®å½•
                if file_info.filename.endswith('/'):
                    continue
                    
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                file_dir = os.path.dirname(os.path.join(temp_dir, file_info.filename))
                if file_dir:
                    os.makedirs(file_dir, exist_ok=True)
                
                # è§£å‹å•ä¸ªæ–‡ä»¶
                zip_ref.extract(file_info, temp_dir)
                success_count += 1
                
            except Exception as file_error:
                error_count += 1
                safe_filename = self._safe_basename(file_info.filename)
                self._log(f"    âŒ è·³è¿‡æ— æ³•è§£å‹çš„æ–‡ä»¶: {safe_filename} - {str(file_error)[:50]}...")
        
        self._log(f"  ğŸ“Š è§£å‹ç»“æœ: æˆåŠŸ {success_count} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡ {error_count} ä¸ªé—®é¢˜æ–‡ä»¶")
        
        if success_count == 0:
            raise Exception(f"æ— æ³•è§£å‹ä»»ä½•æ–‡ä»¶ï¼ŒZIPæ–‡ä»¶å¯èƒ½æŸåæˆ–ç¼–ç ä¸å…¼å®¹")
    
    def _extract_single_file_content_wrapper(self, mid: int, files: List[str]) -> str:
        """åŒ…è£…å™¨å‡½æ•°ï¼Œä¼˜åŒ–å¤„ç†é¡ºåºï¼šä¸¥æ ¼æŒ‰å¤§æ–‡ä»¶ä¼˜å…ˆï¼Œå¢å¼ºå¤šAPIå¹¶å‘å¤„ç†"""
        contents = []
        material_name = self.MATERIAL_NAMES.get(mid, f"ææ–™{mid}")
        processed_files = set()  # é˜²æ­¢é‡å¤å¤„ç†
        
        try:
            self._log(f"    ğŸ”„ å¼€å§‹å¤„ç†ææ–™{mid}({material_name}): {len(files)}ä¸ªæ–‡ä»¶ [å¤§æ–‡ä»¶ä¼˜å…ˆ+å¢å¼ºå¹¶å‘æ¨¡å¼]")
            
            # å»é‡å¤„ç†ï¼šç¡®ä¿åŒä¸€ä¸ªæ–‡ä»¶ä¸ä¼šè¢«é‡å¤å¤„ç†
            unique_files = list(dict.fromkeys(files))  # ä¿æŒé¡ºåºçš„å»é‡
            if len(unique_files) != len(files):
                self._log(f"      ğŸ”„ å‘ç°é‡å¤æ–‡ä»¶ï¼Œå»é‡å: {len(unique_files)}ä¸ªæ–‡ä»¶")
                files = unique_files
            
            # æ™ºèƒ½æ–‡ä»¶æ’åºï¼šä¸¥æ ¼æŒ‰å¤§å°é™åºï¼Œç¡®ä¿å¤§æ–‡ä»¶ç»å¯¹ä¼˜å…ˆ
            try:
                files_with_size = []
                for file_path in files:
                    try:
                        size = os.path.getsize(file_path)
                        files_with_size.append((file_path, size))
                    except:
                        files_with_size.append((file_path, 0))
                
                # ä¸¥æ ¼æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼ˆå¤§æ–‡ä»¶ç»å¯¹ä¼˜å…ˆï¼‰
                files_with_size.sort(key=lambda x: x[1], reverse=True)
                files = [f[0] for f in files_with_size]
                
                # æ˜¾ç¤ºæ–‡ä»¶å¤„ç†é¡ºåº
                total_size = sum(f[1] for f in files_with_size)
                self._log(f"      ğŸ“ æ–‡ä»¶æ€»å¤§å°: {total_size/1024/1024:.1f}MB")
                for i, (file_path, size) in enumerate(files_with_size[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªæœ€å¤§çš„æ–‡ä»¶
                    filename = self._safe_basename(file_path)
                    self._log(f"      ğŸ“‹ å¤„ç†é¡ºåº #{i+1}: {filename} ({size/1024/1024:.1f}MB)")
                    
            except Exception as e:
                self._log(f"      âš ï¸ æ–‡ä»¶å¤§å°æ£€æŸ¥å¤±è´¥ï¼Œä½¿ç”¨åŸé¡ºåº: {e}")
            
            total_content_length = 0  # è·Ÿè¸ªæ€»å†…å®¹é•¿åº¦
            max_total_length = 300000  # å•ä¸ªææ–™æœ€å¤§æ€»é•¿åº¦
            
            # é‡æ–°åˆ†ç±»æ–‡ä»¶ï¼šè°ƒæ•´é˜ˆå€¼ï¼Œæ›´å¤šæ–‡ä»¶å¯ä»¥å¹¶å‘å¤„ç†
            large_files = []  # å¤§æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºé«˜ä¼˜å…ˆçº§å¹¶å‘å¤„ç†
            medium_files = []  # ä¸­ç­‰æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºä¸­ä¼˜å…ˆçº§å¹¶å‘å¤„ç†
            small_files = []  # å°æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºä½ä¼˜å…ˆçº§å¤„ç†
            
            large_file_threshold = 5 * 1024 * 1024   # 5MBä»¥ä¸Šä¸ºå¤§æ–‡ä»¶
            medium_file_threshold = 1 * 1024 * 1024  # 1MB-5MBä¸ºä¸­ç­‰æ–‡ä»¶
            
            for file_path in files:
                try:
                    file_size = os.path.getsize(file_path)
                    if file_path.endswith('.pdf'):
                        if file_size >= large_file_threshold:
                            large_files.append(file_path)
                        elif file_size >= medium_file_threshold:
                            medium_files.append(file_path)
                        else:
                            small_files.append(file_path)
                    else:
                        small_files.append(file_path)  # éPDFæ–‡ä»¶å½“ä½œå°æ–‡ä»¶
                except:
                    small_files.append(file_path)  # æ— æ³•è·å–å¤§å°çš„æ–‡ä»¶å½“ä½œå°æ–‡ä»¶å¤„ç†
            
            self._log(f"      ğŸ“„ æ™ºèƒ½åˆ†ç±»: å¤§æ–‡ä»¶({len(large_files)}) > ä¸­ç­‰æ–‡ä»¶({len(medium_files)}) > å°æ–‡ä»¶({len(small_files)})")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šé«˜ä¼˜å…ˆçº§å¤„ç†å¤§æ–‡ä»¶ï¼ˆâ‰¥5MBï¼‰
            if large_files:
                self._log(f"      ğŸš€ ã€ç¬¬ä¸€é˜¶æ®µã€‘å¤§æ–‡ä»¶å¹¶å‘å¤„ç†: {len(large_files)} ä¸ªæ–‡ä»¶")
                large_file_contents = self._process_files_with_enhanced_concurrency(large_files, mid, "å¤§æ–‡ä»¶")
                contents.extend(large_file_contents)
                
                # æ›´æ–°å·²å¤„ç†æ–‡ä»¶å’Œå†…å®¹é•¿åº¦
                for file_path in large_files:
                    processed_files.add(file_path)
                for content_item in large_file_contents:
                    total_content_length += len(content_item)
            
            # ç¬¬äºŒé˜¶æ®µï¼šä¸­ä¼˜å…ˆçº§å¤„ç†ä¸­ç­‰æ–‡ä»¶ï¼ˆ1-5MBï¼‰
            if medium_files and total_content_length < max_total_length * 0.8:  # è¿˜æœ‰80%ä»¥ä¸Šç©ºé—´æ—¶å¤„ç†
                self._log(f"      âš¡ ã€ç¬¬äºŒé˜¶æ®µã€‘ä¸­ç­‰æ–‡ä»¶å¹¶å‘å¤„ç†: {len(medium_files)} ä¸ªæ–‡ä»¶")
                medium_file_contents = self._process_files_with_enhanced_concurrency(medium_files, mid, "ä¸­ç­‰æ–‡ä»¶")
                contents.extend(medium_file_contents)
                
                # æ›´æ–°å·²å¤„ç†æ–‡ä»¶å’Œå†…å®¹é•¿åº¦
                for file_path in medium_files:
                    processed_files.add(file_path)
                for content_item in medium_file_contents:
                    total_content_length += len(content_item)
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šä½ä¼˜å…ˆçº§å¤„ç†å°æ–‡ä»¶ï¼ˆ<1MBï¼‰ï¼Œé‡‡ç”¨æ‰¹é‡å¹¶å‘æˆ–ä¸²è¡Œ
            if small_files and total_content_length < max_total_length * 0.9:  # è¿˜æœ‰90%ä»¥ä¸Šç©ºé—´æ—¶å¤„ç†
                remaining_small_files = [f for f in small_files if f not in processed_files]
                if remaining_small_files:
                    if len(remaining_small_files) > 3 and len(self.api_rotator.api_keys) > 2:
                        # å¦‚æœå°æ–‡ä»¶è¾ƒå¤šä¸”æœ‰è¶³å¤ŸAPIï¼Œä½¿ç”¨ä½å¼ºåº¦å¹¶å‘
                        self._log(f"      ğŸ”¥ ã€ç¬¬ä¸‰é˜¶æ®µã€‘å°æ–‡ä»¶æ‰¹é‡å¹¶å‘å¤„ç†: {len(remaining_small_files)} ä¸ªæ–‡ä»¶")
                        small_file_contents = self._process_files_with_enhanced_concurrency(remaining_small_files, mid, "å°æ–‡ä»¶")
                        contents.extend(small_file_contents)
                        
                        for file_path in remaining_small_files:
                            processed_files.add(file_path)
                        for content_item in small_file_contents:
                            total_content_length += len(content_item)
                    else:
                        # å¦åˆ™ä½¿ç”¨ä¼˜åŒ–çš„ä¸²è¡Œå¤„ç†
                        self._log(f"      ğŸ“ ã€ç¬¬ä¸‰é˜¶æ®µã€‘å°æ–‡ä»¶ä¸²è¡Œå¤„ç†: {len(remaining_small_files)} ä¸ªæ–‡ä»¶")
                        for i, file_path in enumerate(remaining_small_files, 1):
                            filename = self._safe_basename(file_path)
                            self._log(f"      ğŸ“„ å¤„ç†å°æ–‡ä»¶ {i}/{len(remaining_small_files)}: {filename}")
                            
                            try:
                                # ä½¿ç”¨æ›´ç²¾ç¡®çš„ç¼“å­˜é”®
                                import hashlib
                                file_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
                                cache_prefix = f"material_{mid}_file_{file_hash}"
                                
                                # æ£€æŸ¥ç¼“å­˜
                                cached_content = self.cache_manager.get(file_path, cache_prefix)
                                if cached_content:
                                    content = cached_content
                                    self._log(f"      ğŸ’¾ ä½¿ç”¨ç¼“å­˜: {filename}")
                                else:
                                    # æå–å†…å®¹
                                    if file_path.endswith('.pdf'):
                                        content = self._extract_pdf_content(file_path)
                                    else:
                                        content = f"è·³è¿‡éPDFæ–‡ä»¶: {filename}"
                                    
                                    # å†…å®¹é•¿åº¦æ§åˆ¶ï¼ˆä½¿ç”¨å¤´å°¾æˆªå–ï¼‰
                                    if content and len(content) > 200000:
                                        self._log(f"      âš ï¸ å°æ–‡ä»¶å†…å®¹è¿‡å¤§ï¼Œå¤´å°¾æˆªå–: {filename} ({len(content)}å­—ç¬¦)")
                                        content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                                    
                                    # å­˜å…¥ç¼“å­˜
                                    if content and content.strip():
                                        self.cache_manager.set(file_path, content, cache_prefix)
                                
                                if content and content.strip():
                                    content_length = len(content)
                                    if total_content_length + content_length > max_total_length:
                                        remaining_space = max_total_length - total_content_length
                                        if remaining_space > 3000:
                                            content = content[:remaining_space-1000] + f"\n\n[æ€»é•¿åº¦é™åˆ¶æˆªå–]"
                                            content_length = len(content)
                                        else:
                                            self._log(f"      âš ï¸ ææ–™æ€»é•¿åº¦å·²è¾¾é™åˆ¶ï¼Œåœæ­¢å¤„ç†: {filename}")
                                            break
                                    
                                    # æ ¼å¼åŒ–å†…å®¹
                                    formatted_content = f"--- æ–‡ä»¶: {filename} ---\n{content}"
                                    contents.append(formatted_content)
                                    total_content_length += content_length
                                    processed_files.add(file_path)
                                    
                                    self._log(f"      âœ… å°æ–‡ä»¶å¤„ç†æˆåŠŸ: {filename} ({content_length}å­—ç¬¦)")
                                else:
                                    self._log(f"      âš ï¸ å°æ–‡ä»¶å†…å®¹ä¸ºç©º: {filename}")
                                    contents.append(f"--- æ–‡ä»¶: {filename} ---\næ–‡ä»¶å†…å®¹ä¸ºç©º")
                                    processed_files.add(file_path)
                                    
                            except Exception as e:
                                error_msg = str(e)
                                self._log(f"      âŒ å°æ–‡ä»¶å¤„ç†å¤±è´¥: {filename} - {error_msg[:50]}...")
                                contents.append(f"--- æ–‡ä»¶: {filename} ---\næ–‡ä»¶å¤„ç†å¤±è´¥: {error_msg}")
                                processed_files.add(file_path)
            
            # åˆå¹¶æ‰€æœ‰å†…å®¹ï¼ˆå†æ¬¡éªŒè¯æ€»é•¿åº¦ï¼‰
            if contents:
                combined_content = "\n\n".join(contents)
                # æœ€ç»ˆé•¿åº¦æ£€æŸ¥
                if len(combined_content) > max_total_length:
                    self._log(f"    âš ï¸ åˆå¹¶åå†…å®¹ä»ç„¶è¿‡å¤§ï¼Œæœ€ç»ˆæˆªå–: {len(combined_content)}å­—ç¬¦")
                    combined_content = combined_content[:max_total_length] + f"\n\n[æ³¨æ„ï¼šææ–™æ€»é•¿åº¦è¶…é™ï¼Œå·²æœ€ç»ˆæˆªå–åˆ°{max_total_length//1000}Kå­—ç¬¦]"
            else:
                combined_content = f"ææ–™{mid}({material_name})æ— å¯ç”¨å†…å®¹"
            
            self._log(f"    ğŸ“Š ææ–™{mid}({material_name})å¤„ç†å®Œæˆ: æ€»è®¡{len(combined_content)}å­—ç¬¦ [é˜²å †å å¤„ç†]")
            return combined_content
            
        except Exception as e:
            error_msg = f"ææ–™{mid}({material_name})æ•´ä½“å¤„ç†å¤±è´¥: {str(e)}"
            self._log(f"    âŒ {error_msg}")
            return error_msg

    def _map_files_to_materials(self, base_dir: str) -> Dict[int, List[str]]:
        """å°†æ–‡ä»¶æ˜ å°„åˆ°ææ–™ç±»åˆ«ï¼Œç¡®ä¿æ¯ä¸ªæ–‡ä»¶åªæ˜ å°„åˆ°ä¸€ä¸ªææ–™"""
        material_files = {i: [] for i in range(1, 18)}
        file_mapping_log = {}  # è®°å½•æ–‡ä»¶æ˜ å°„æ—¥å¿—
        
        for root, _, files in os.walk(base_dir):
            for file in files:
                if file.startswith('.') or file.startswith('~'): 
                    continue
                    
                file_path = os.path.join(root, file)
                folder_name = os.path.basename(root)
                
                # ä½¿ç”¨æ”¹è¿›çš„ææ–™è¯†åˆ«é€»è¾‘
                material_id = self._identify_material(folder_name, file)
                
                if material_id:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»è¢«æ˜ å°„è¿‡
                    if file_path in file_mapping_log:
                        existing_mid = file_mapping_log[file_path]
                        self._log(f"  âš ï¸ æ–‡ä»¶é‡å¤æ˜ å°„: {self._safe_basename(file_path)} - å·²æ˜ å°„åˆ°ææ–™{existing_mid}ï¼Œè·³è¿‡ææ–™{material_id}")
                        continue
                    
                    # æ˜ å°„æ–‡ä»¶åˆ°ææ–™
                    material_files[material_id].append(file_path)
                    file_mapping_log[file_path] = material_id
                    
                    # è®°å½•æ˜ å°„è¯¦æƒ…
                    self._log(f"  ğŸ“‹ æ–‡ä»¶æ˜ å°„: {self._safe_basename(file_path)} -> ææ–™{material_id}({self.MATERIAL_NAMES.get(material_id, f'ææ–™{material_id}')})")
                else:
                    self._log(f"  â“ æœªè¯†åˆ«æ–‡ä»¶: {self._safe_basename(file_path)} (æ–‡ä»¶å¤¹: {folder_name})")
        
        # è¾“å‡ºæ˜ å°„ç»Ÿè®¡
        total_mapped = sum(len(files) for files in material_files.values())
        self._log(f"  ğŸ“Š æ–‡ä»¶æ˜ å°„ç»Ÿè®¡: æ€»è®¡ {total_mapped} ä¸ªæ–‡ä»¶æ˜ å°„åˆ° {len([mid for mid, files in material_files.items() if files])} ä¸ªææ–™ç±»åˆ«")
        
        return material_files

    def _identify_material(self, folder_name: str, filename: str) -> Optional[int]:
        """æ›´ç²¾ç¡®çš„ææ–™è¯†åˆ«é€»è¾‘ï¼Œä¼˜å…ˆçº§ï¼šæ•°å­—å‰ç¼€ > æ–‡ä»¶å¤¹åç§° > æ–‡ä»¶åå…³é”®è¯"""
        text_to_check = f"{folder_name} {filename}".lower()
        
        # ä¼˜å…ˆçº§ 1: æ£€æŸ¥æ•°å­—å‰ç¼€ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        match = re.match(r'^(\d+)', text_to_check)
        if match:
            material_id = int(match.group(1))
            if 1 <= material_id <= 17:
                return material_id
        
        # ä¼˜å…ˆçº§ 2: æ£€æŸ¥æ–‡ä»¶å¤¹åç§°ä¸­çš„æ•°å­—
        folder_match = re.search(r'(\d+)', folder_name)
        if folder_match:
            material_id = int(folder_match.group(1))
            if 1 <= material_id <= 17:
                return material_id
        
        # ä¼˜å…ˆçº§ 3: æ£€æŸ¥æ–‡ä»¶åä¸­çš„æ•°å­—
        file_match = re.search(r'(\d+)', filename)
        if file_match:
            material_id = int(file_match.group(1))
            if 1 <= material_id <= 17:
                return material_id
        
        # ä¼˜å…ˆçº§ 4: æ ¹æ®å…³é”®è¯åŒ¹é…ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„å…³é”®è¯åŒ¹é…ï¼Œé¿å…è¯¯åŒ¹é…
        keyword_mapping = {
            1: ["æ•™è‚²ç»å†", "å­¦å†", "æ¯•ä¸š"],
            2: ["å·¥ä½œç»å†", "å·¥ä½œå•ä½", "ä»»èŒ"],
            3: ["ç»§ç»­æ•™è‚²", "åŸ¹è®­æƒ…å†µ", "åŸ¹è®­"],
            4: ["å­¦æœ¯æŠ€æœ¯å…¼èŒ", "å…¼èŒæƒ…å†µ", "å…¼èŒ"],
            5: ["è·å¥–æƒ…å†µ", "å¥–åŠ±", "è·å¥–"],
            6: ["è£èª‰ç§°å·", "è£èª‰"],
            7: ["ç§‘ç ”é¡¹ç›®", "åŸºé‡‘æƒ…å†µ", "ç§‘ç ”"],
            8: ["å·¥ç¨‹æŠ€æœ¯é¡¹ç›®", "å·¥ç¨‹é¡¹ç›®"],
            9: ["è®ºæ–‡"],
            10: ["è‘—ä½œ", "è¯‘ä½œ", "æ•™æ"],
            11: ["ä¸“åˆ©", "è‘—ä½œæƒ"],
            12: ["æŒ‡å®šæ ‡å‡†", "æ ‡å‡†æƒ…å†µ"],
            13: ["æˆæœè¢«æ‰¹ç¤º", "é‡‡çº³", "è¿ç”¨", "æ¨å¹¿"],
            14: ["èµ„è´¨è¯ä¹¦", "è¯ä¹¦"],
            15: ["å¥–æƒ©æƒ…å†µ", "å¥–æƒ©"],
            16: ["è€ƒæ ¸æƒ…å†µ", "è€ƒæ ¸"],
            17: ["ç”³æŠ¥ææ–™é™„ä»¶", "é™„ä»¶ä¿¡æ¯", "é™„ä»¶"]
        }
        
        # åªåœ¨å…³é”®è¯å®Œå…¨åŒ¹é…æ—¶æ‰è¿”å›ç»“æœï¼Œé¿å…éƒ¨åˆ†åŒ¹é…å¯¼è‡´çš„è¯¯åˆ¤
        for mid, keywords in keyword_mapping.items():
            for keyword in keywords:
                if keyword in text_to_check:
                    # åŒé‡éªŒè¯ï¼šç¡®ä¿å…³é”®è¯ä¸æ˜¯æ›´å¤§å•è¯çš„ä¸€éƒ¨åˆ†
                    if len(keyword) >= 3:  # åªæ¥å—è¾ƒé•¿çš„å…³é”®è¯ï¼Œé¿å…è¯¯åŒ¹é…
                        return mid
        
        return None

    def _extract_single_file_content(self, file_path: str, cache_prefix: str = "") -> str:
        """æå–å•ä¸ªæ–‡ä»¶å†…å®¹ï¼Œä½¿ç”¨æ”¹è¿›çš„ç¼“å­˜ç­–ç•¥"""
        
        # æ£€æŸ¥æ”¹è¿›çš„ç¼“å­˜ç®¡ç†å™¨
        cached_content = self.cache_manager.get(file_path, cache_prefix)
        if cached_content:
            self._log(f"    - [æ™ºèƒ½ç¼“å­˜] ä½¿ç”¨ç¼“å­˜å†…å®¹: {self._safe_basename(file_path)}")
            return cached_content
        
        # æå–å†…å®¹
        if file_path.endswith('.pdf'):
            content = self._extract_pdf_content(file_path)
        else:
            content = f"è·³è¿‡éPDFæ–‡ä»¶: {self._safe_basename(file_path)}"
        
        # å°†ç»“æœå­˜å…¥æ”¹è¿›çš„ç¼“å­˜ç®¡ç†å™¨
        if content and content.strip():
            self.cache_manager.set(file_path, content, cache_prefix)
        
        return content

    def _extract_pdf_content(self, pdf_path: str) -> str:
        filename = self._safe_basename(pdf_path)
        # ç›´æ¥ä½¿ç”¨AIè¯†åˆ«ï¼Œå®Œæ•´æå–æ‰€æœ‰å†…å®¹
        self._log(f"    - [AIå®Œæ•´è¯†åˆ«] {filename}")
        return self._extract_pdf_with_ai(pdf_path)

    def _get_pdf_page_count(self, pdf_path: str) -> int:
        """è·å–PDFæ–‡ä»¶çš„é¡µæ•°"""
        try:
            from pypdf import PdfReader
            reader = PdfReader(pdf_path)
            return len(reader.pages)
        except Exception as e:
            self._log(f"    - [è­¦å‘Š] æ— æ³•è·å–PDFé¡µæ•°: {self._safe_basename(pdf_path)} - {e}")
            return 0
    
    def _split_pdf_pages(self, pdf_path: str, pages_per_chunk: int = 5) -> List[Tuple[int, int]]:
        """å°†PDFåˆ†å‰²æˆå¤šä¸ªé¡µé¢èŒƒå›´ç”¨äºå¹¶å‘å¤„ç†ï¼Œç¡®ä¿æ‰€æœ‰é¡µé¢éƒ½è¢«åŒ…å«ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        try:
            total_pages = self._get_pdf_page_count(pdf_path)
            filename = self._safe_basename(pdf_path)
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿é¡µæ•°æ£€æŸ¥å‡†ç¡®
            if total_pages <= 0:
                self._log(f"    - [åˆ†ç‰‡é”™è¯¯] æ— æ³•è·å–æœ‰æ•ˆé¡µæ•°: {filename}")
                return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯é»˜è®¤èŒƒå›´
            
            if total_pages <= pages_per_chunk:
                # å¦‚æœæ€»é¡µæ•°ä¸è¶…è¿‡åˆ†ç‰‡å¤§å°ï¼Œç›´æ¥è¿”å›æ•´ä¸ªæ–‡æ¡£
                self._log(f"    - [åˆ†ç‰‡ä¼˜åŒ–] {total_pages}é¡µâ‰¤{pages_per_chunk}é¡µï¼Œæ— éœ€åˆ†ç‰‡")
                return [(1, total_pages)]
            
            chunks = []
            
            # ğŸš€ ç”Ÿæˆåˆ†ç‰‡ï¼Œç¡®ä¿å®Œæ•´è¦†ç›–
            for start_page in range(1, total_pages + 1, pages_per_chunk):
                end_page = min(start_page + pages_per_chunk - 1, total_pages)
                chunks.append((start_page, end_page))
                
                # è®°å½•åˆ†ç‰‡ä¿¡æ¯
                page_count = end_page - start_page + 1
                self._log(f"    - [åˆ†ç‰‡{len(chunks)}] ç¬¬{start_page}-{end_page}é¡µï¼ˆ{page_count}é¡µï¼‰")
            
            # âœ… éªŒè¯åˆ†ç‰‡å®Œæ•´æ€§
            total_covered_pages = sum(end - start + 1 for start, end in chunks)
            if total_covered_pages != total_pages:
                self._log(f"    - [éªŒè¯å¤±è´¥] æ€»é¡µæ•°{total_pages}ï¼Œåˆ†ç‰‡è¦†ç›–{total_covered_pages}é¡µï¼é‡æ–°ç”Ÿæˆ...")
                # é‡æ–°ç”Ÿæˆæ›´ä¿å®ˆçš„åˆ†ç‰‡
                chunks = [(i, min(i + pages_per_chunk - 1, total_pages)) for i in range(1, total_pages + 1, pages_per_chunk)]
                new_covered = sum(end - start + 1 for start, end in chunks)
                self._log(f"    - [é‡æ–°éªŒè¯] ä¿®æ­£åè¦†ç›–{new_covered}é¡µ")
            else:
                self._log(f"    - [éªŒè¯æˆåŠŸ] {filename} å…±{total_pages}é¡µï¼Œåˆ†ä¸º{len(chunks)}ä¸ªåˆ†ç‰‡ï¼Œå…¨éƒ¨è¦†ç›–")
            
            return chunks
            
        except Exception as e:
            self._log(f"    - [åˆ†ç‰‡å¼‚å¸¸] PDFåˆ†é¡µå¤±è´¥: {self._safe_basename(pdf_path)} - {e}")
            return []  # å¼‚å¸¸æ—¶è¿”å›ç©ºåˆ—è¡¨
    
    def _extract_pdf_pages_concurrent(self, pdf_path: str, page_ranges: List[Tuple[int, int]]) -> str:
        """å¹¶å‘å¤„ç†PDFçš„ä¸åŒé¡µé¢èŒƒå›´ï¼Œé˜²æ­¢å†…å®¹å †å ï¼ˆä½¿ç”¨ä»»åŠ¡é˜Ÿåˆ—åŠ¨æ€åˆ†é…ï¼‰"""
        filename = self._safe_basename(pdf_path)
        self._log(f"    - [å¹¶å‘] å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ—æ¨¡å¼ï¼Œ{len(page_ranges)} ä¸ªä»»åŠ¡ç­‰å¾…APIè®¤é¢†: {filename}")
        
        import queue
        import threading
        
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœå­˜å‚¨
        task_queue = queue.Queue()
        results = {}  # {task_id: (start_page, content)}
        results_lock = threading.Lock()
        processed_tasks = set()  # è·Ÿè¸ªå·²å¤„ç†çš„ä»»åŠ¡IDï¼Œé˜²æ­¢é‡å¤
        
        # å°†æ‰€æœ‰ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        for i, (start_page, end_page) in enumerate(page_ranges):
            task_queue.put((i, start_page, end_page))
        
        def worker_thread(worker_id):
            """å·¥ä½œçº¿ç¨‹ï¼Œä¸»åŠ¨è®¤é¢†ä»»åŠ¡"""
            processed_count = 0
            
            while True:
                try:
                    # è®¤é¢†ä»»åŠ¡ï¼ˆè¶…æ—¶1ç§’æœªè·å–åˆ°ä»»åŠ¡å°±é€€å‡ºï¼‰
                    task_id, start_page, end_page = task_queue.get(timeout=1)
                    
                    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å¤„ç†è¿‡ï¼Œé˜²æ­¢é‡å¤
                    with results_lock:
                        if task_id in processed_tasks:
                            self._log(f"    - [Worker-{worker_id}] ä»»åŠ¡ {task_id+1} å·²è¢«å¤„ç†ï¼Œè·³è¿‡")
                            task_queue.task_done()
                            continue
                        processed_tasks.add(task_id)
                    
                    self._log(f"    - [Worker-{worker_id}] è®¤é¢†ä»»åŠ¡ {task_id+1}: ç¬¬{start_page}-{end_page}é¡µ")
                    
                    try:
                        # æ‰§è¡Œä»»åŠ¡
                        content = self._extract_single_page_range(pdf_path, start_page, end_page, worker_id)
                        
                        # å†…å®¹é•¿åº¦æ£€æŸ¥ï¼Œé˜²æ­¢å•ä¸ªåˆ†ç‰‡è¿‡å¤§ï¼ˆä½¿ç”¨å¤´å°¾æˆªå–ï¼‰
                        if len(content) > 200000:  # å¦‚æœå•ä¸ªåˆ†ç‰‡è¶…è¿‡200Kå­—ç¬¦
                            self._log(f"    - [Worker-{worker_id}] è­¦å‘Š: ç¬¬{start_page}-{end_page}é¡µå†…å®¹è¿‡å¤§ ({len(content)}å­—ç¬¦)ï¼Œå¤´å°¾æˆªå–é˜²æ­¢å †å ")
                            content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                        
                        # å­˜å‚¨ç»“æœ
                        with results_lock:
                            results[task_id] = (start_page, content)
                        
                        processed_count += 1
                        self._log(f"    - [Worker-{worker_id}] å®Œæˆä»»åŠ¡ {task_id+1}: {len(content)} å­—ç¬¦")
                        
                    except Exception as e:
                        error_msg = f"[ç¬¬{start_page}-{end_page}é¡µå¤„ç†å¤±è´¥ï¼š{str(e)[:100]}]"
                        self._log(f"    - [Worker-{worker_id}] ä»»åŠ¡ {task_id+1} æ‰§è¡Œå¤±è´¥: {str(e)[:50]}...")
                        
                        # å­˜å‚¨é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯é‡æ–°å…¥é˜Ÿï¼Œé¿å…æ— é™å¾ªç¯
                        with results_lock:
                            results[task_id] = (start_page, error_msg)
                    
                    # æ ‡è®°ä»»åŠ¡å®Œæˆ
                    task_queue.task_done()
                    
                except queue.Empty:
                    # æ²¡æœ‰æ›´å¤šä»»åŠ¡ï¼Œé€€å‡º
                    self._log(f"    - [Worker-{worker_id}] æ²¡æœ‰æ›´å¤šä»»åŠ¡ï¼Œçº¿ç¨‹é€€å‡º (å¤„ç†äº†{processed_count}ä¸ªä»»åŠ¡)")
                    break
                except Exception as e:
                    self._log(f"    - [Worker-{worker_id}] çº¿ç¨‹å¼‚å¸¸: {str(e)[:50]}...")
                    try:
                        task_queue.task_done()
                    except:
                        pass
                    break
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹ï¼ˆä¼˜åŒ–ï¼šæœ€å¤§åŒ–APIåˆ©ç”¨ç‡ï¼Œç§»é™¤ä¿å®ˆé™åˆ¶ï¼‰
        max_workers = min(len(page_ranges), len(self.api_rotator.api_keys) * 2, 12)  # æ¯ä¸ªAPIæœ€å¤š2ä¸ªçº¿ç¨‹ï¼Œæœ€å¤š12ä¸ªå¹¶å‘
        workers = []
        
        self._log(f"    - [ç®¡ç†] å¯åŠ¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹å¤„ç† {len(page_ranges)} ä¸ªä»»åŠ¡")
        
        for i in range(max_workers):
            worker = threading.Thread(target=worker_thread, args=(i+1,))
            worker.daemon = True
            worker.start()
            workers.append(worker)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤šç­‰å¾…300ç§’ï¼‰
        try:
            task_queue.join()  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            self._log(f"    - [ç®¡ç†] æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        except KeyboardInterrupt:
            self._log(f"    - [ç®¡ç†] ç”¨æˆ·ä¸­æ–­å¤„ç†")
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        for worker in workers:
            worker.join(timeout=5)  # æœ€å¤šç­‰å¾…5ç§’
        
        # æŒ‰é¡µé¢é¡ºåºæ’åºå¹¶åˆå¹¶ç»“æœï¼Œç¡®ä¿å®Œæ•´æ€§å’Œè¿ç»­æ€§
        if not results:
            self._log(f"    - [åˆå¹¶é”™è¯¯] æ‰€æœ‰åˆ†ç‰‡éƒ½å¤„ç†å¤±è´¥: {filename}")
            return f"[å¹¶å‘å¤„ç†å¤±è´¥ï¼šæ‰€æœ‰ä»»åŠ¡éƒ½æœªèƒ½å®Œæˆ] - {filename}"
        
        sorted_results = sorted(results.items(), key=lambda x: x[1][0])  # æŒ‰èµ·å§‹é¡µé¢æ’åº
        combined_parts = []
        total_length = 0
        max_combined_length = 500000  # åˆå¹¶åçš„æœ€å¤§é•¿åº¦é™åˆ¶
        
        # æ£€æŸ¥é¡µé¢è¿ç»­æ€§å¹¶åˆå¹¶å†…å®¹
        expected_page = 1
        missing_pages = []
        
        for task_id, (start_page, content) in sorted_results:
            # æ£€æŸ¥é¡µé¢æ˜¯å¦è¿ç»­
            if start_page > expected_page:
                missing_range = list(range(expected_page, start_page))
                missing_pages.extend(missing_range)
                self._log(f"    - [è¿ç»­æ€§æ£€æŸ¥] ç¼ºå¤±é¡µé¢: {missing_range}")
            
            # æ£€æŸ¥åˆå¹¶åé•¿åº¦æ˜¯å¦ä¼šè¶…é™
            if total_length + len(content) > max_combined_length:
                remaining_space = max_combined_length - total_length
                if remaining_space > 5000:  # è¿˜æœ‰è¶³å¤Ÿç©ºé—´
                    content = content[:remaining_space-1000] + f"\n\n[æ³¨æ„ï¼šæ€»é•¿åº¦é™åˆ¶ï¼Œå·²æˆªå–å‰©ä½™{remaining_space//1000}Kå­—ç¬¦]"
                else:
                    self._log(f"    - [åˆå¹¶] å·²è¾¾åˆ°æ€»é•¿åº¦é™åˆ¶ï¼Œåœæ­¢æ·»åŠ æ›´å¤šå†…å®¹")
                    break
            
            # æ¸…ç†å†…å®¹ä¸­çš„é‡å¤é¡µé¢æ ‡è®°ï¼ˆå¦‚æœAIé‡å¤äº†ï¼‰
            content = self._clean_page_content(content, start_page)
            combined_parts.append(content)
            total_length += len(content)
            
            # æ›´æ–°æœŸæœ›é¡µé¢
            task_range = next((r for r in page_ranges if r[0] == start_page), None)
            if task_range:
                expected_page = task_range[1] + 1
        
        # æ„å»ºå®Œæ•´çš„åˆå¹¶å†…å®¹
        if missing_pages:
            header = f"[åˆ†ç‰‡å¤„ç†å®Œæˆ - ç¼ºå¤±é¡µé¢: {missing_pages[:10]}{'...' if len(missing_pages) > 10 else ''}]\n\n"
        else:
            header = f"[åˆ†ç‰‡å¤„ç†å®Œæˆ - é¡µé¢è¿ç»­]\n\n"
        
        combined_content = header + "\n\n--- é¡µé¢åˆ†å‰²çº¿ ---\n\n".join(combined_parts)
        
        completed_tasks = len(results)
        total_tasks = len(page_ranges)
        
        self._log(f"    - [åˆå¹¶] æˆåŠŸå®Œæˆ {completed_tasks}/{total_tasks} ä¸ªä»»åŠ¡ï¼Œåˆå¹¶å†…å®¹é•¿åº¦: {len(combined_content)} å­—ç¬¦")
        
        # æœ€ç»ˆé•¿åº¦æ£€æŸ¥
        if len(combined_content) > max_combined_length:
            self._log(f"    - [æœ€ç»ˆæ£€æŸ¥] åˆå¹¶å†…å®¹ä»ç„¶è¿‡å¤§ï¼Œæœ€ç»ˆæˆªå–: {len(combined_content)}å­—ç¬¦")
            combined_content = combined_content[:max_combined_length] + f"\n\n[æ³¨æ„ï¼šåˆå¹¶åæ€»é•¿åº¦è¶…é™ï¼Œå·²æœ€ç»ˆæˆªå–åˆ°{max_combined_length//1000}Kå­—ç¬¦]"
        
        return combined_content
    
    def _clean_page_content(self, content: str, start_page: int) -> str:
        """æ¸…ç†åˆ†ç‰‡å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„é‡å¤æ ‡è®°å’Œå¤šä½™ä¿¡æ¯"""
        if not content:
            return content
        
        # ç§»é™¤å¤šä½™çš„é¡µé¢æ ‡è®°é‡å¤
        import re
        content = re.sub(r'\[ç¬¬\d+-\d+é¡µ\]\s*\[ç¬¬\d+-\d+é¡µ\]', f'[ç¬¬{start_page}é¡µå¼€å§‹]', content)
        
        # æ¸…ç†è¿‡å¤šçš„æ¢è¡Œ
        content = re.sub(r'\n{4,}', '\n\n\n', content)
        
        return content.strip()
    
    def _extract_pdf_pages_to_bytes(self, pdf_path: str, start_page: int, end_page: int) -> bytes:
        """çœŸæ­£çš„PDFåˆ†é¡µï¼šæå–æŒ‡å®šé¡µé¢èŒƒå›´ä¸ºæ–°çš„PDFå­—èŠ‚æ•°æ®"""
        try:
            from pypdf import PdfReader, PdfWriter
            import io
            
            # è¯»å–åŸPDF
            reader = PdfReader(pdf_path)
            total_pages = len(reader.pages)
            
            # è°ƒæ•´é¡µç èŒƒå›´ï¼ˆpypdfä½¿ç”¨0å¼€å§‹çš„ç´¢å¼•ï¼‰
            start_idx = max(0, start_page - 1)
            end_idx = min(total_pages, end_page)
            
            if start_idx >= total_pages:
                raise ValueError(f"èµ·å§‹é¡µç {start_page}è¶…å‡ºæ€»é¡µæ•°{total_pages}")
            
            # åˆ›å»ºæ–°çš„PDFå†™å…¥å™¨
            writer = PdfWriter()
            
            # æ·»åŠ æŒ‡å®šèŒƒå›´çš„é¡µé¢
            for page_idx in range(start_idx, end_idx):
                writer.add_page(reader.pages[page_idx])
            
            # å†™å…¥åˆ°å­—èŠ‚æµ
            output = io.BytesIO()
            writer.write(output)
            output.seek(0)
            
            pdf_bytes = output.read()
            output.close()
            
            self._log(f"    - [åˆ†ç‰‡] æˆåŠŸæå–ç¬¬{start_page}-{end_page}é¡µï¼Œå¤§å°: {len(pdf_bytes)/1024:.1f}KB")
            return pdf_bytes
            
        except Exception as e:
            self._log(f"    - [åˆ†ç‰‡é”™è¯¯] æ— æ³•æå–ç¬¬{start_page}-{end_page}é¡µ: {e}")
            # é™çº§å¤„ç†ï¼šè¿”å›åŸå§‹PDF
            return Path(pdf_path).read_bytes()
    
    def _extract_single_page_range(self, pdf_path: str, start_page: int, end_page: int, worker_id: int) -> str:
        """æå–å•ä¸ªé¡µé¢èŒƒå›´çš„å†…å®¹ï¼ˆçœŸæ­£çš„åˆ†ç‰‡å¤„ç†ï¼‰"""
        filename = self._safe_basename(pdf_path)
        
        # ä¸ºå¹¶å‘å¤„ç†çš„é¡µé¢èŒƒå›´ä½¿ç”¨å”¯ä¸€çš„ç¼“å­˜é”®ï¼Œé˜²æ­¢å†²çª
        import hashlib
        file_hash = hashlib.md5(pdf_path.encode()).hexdigest()[:8]
        cache_prefix = f"pages_{start_page}_{end_page}_file_{file_hash}"
        cached_content = self.cache_manager.get(pdf_path, cache_prefix)
        
        if cached_content:
            self._log(f"    - [Worker-{worker_id}] ä½¿ç”¨ç¼“å­˜: ç¬¬{start_page}-{end_page}é¡µ")
            return cached_content
        
        def ai_call_for_pages(client):
            try:
                # çœŸæ­£çš„åˆ†ç‰‡å¤„ç†ï¼šåªè¯»å–æŒ‡å®šé¡µé¢èŒƒå›´
                split_pdf_bytes = self._extract_pdf_pages_to_bytes(pdf_path, start_page, end_page)
                if len(split_pdf_bytes) == 0:
                    raise ValueError("åˆ†ç‰‡PDFä¸ºç©º")
                
                self._log(f"    - [Worker-{worker_id}] åˆ†ç‰‡å¤§å°: {len(split_pdf_bytes)/1024:.1f}KB (ç¬¬{start_page}-{end_page}é¡µ)")
                
                if start_page == 1 and end_page >= 999:
                    # å¤„ç†æ•´ä¸ªæ–‡æ¡£ï¼ˆæ— é¡µæ•°é™åˆ¶æ—¶ï¼‰
                    prompt = f"""è¯·å®Œæ•´æå–PDFæ–‡ä»¶ï¼ˆ{filename}ï¼‰çš„æ‰€æœ‰å†…å®¹ï¼š

**æå–è¦æ±‚**ï¼š
âœ“ æ‰€æœ‰æ–‡å­—ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ­£æ–‡ã€æ³¨é‡Šï¼‰
âœ“ è¡¨æ ¼æ•°æ®å’Œæ•°å€¼ä¿¡æ¯
âœ“ å›¾è¡¨æ ‡é¢˜å’Œè¯´æ˜æ–‡å­—
âœ“ é¡µçœ‰é¡µè„šä¿¡æ¯
âœ“ å›¾åƒä¸­çš„æ–‡å­—å†…å®¹ï¼ˆå¦‚OCRè¯†åˆ«ï¼‰

**å¤„ç†åŸåˆ™**ï¼š
1. ä¸è¦è·³è¿‡ä»»ä½•å†…å®¹ï¼Œå°½å¯èƒ½å®Œæ•´æå–
2. ä¿æŒæ–‡æ¡£çš„é€»è¾‘ç»“æ„
3. å¯¹äºå›¾åƒå†…å®¹ï¼Œå°è¯•è¯†åˆ«å…¶ä¸­çš„æ–‡å­—
4. ä½¿ç”¨æ¸…æ™°çš„æ ¼å¼è¾“å‡º

è¯·å¼€å§‹å®Œæ•´æå–ã€‚"""
                else:
                    # å¤„ç†æŒ‡å®šé¡µé¢èŒƒå›´
                    prompt = f"""è¯·å®Œæ•´æå–è¿™ä¸ªPDFåˆ†ç‰‡ï¼ˆæ¥è‡ª{filename}ç¬¬{start_page}-{end_page}é¡µï¼‰çš„æ‰€æœ‰å†…å®¹ï¼š

**æå–è¦æ±‚**ï¼š
âœ“ æ‰€æœ‰æ–‡å­—ä¿¡æ¯
âœ“ è¡¨æ ¼æ•°æ®å’Œæ•°å€¼
âœ“ å›¾è¡¨æ ‡é¢˜å’Œè¯´æ˜
âœ“ å›¾åƒä¸­çš„æ–‡å­—å†…å®¹

**è¾“å‡ºæ ¼å¼**: åœ¨å¼€å¤´æ ‡æ³¨[ç¬¬{start_page}-{end_page}é¡µ]ï¼Œç„¶åè¾“å‡ºå®Œæ•´å†…å®¹ã€‚"""
                    
                return client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=[
                        types.Part.from_bytes(
                            data=split_pdf_bytes,
                            mime_type='application/pdf'
                        ),
                        prompt
                    ]
                )
            except Exception as e:
                raise e
        
        try:
            response = self._rotated_api_call(ai_call_for_pages, max_retries=2)
            if response and response.text and response.text.strip():
                content = response.text.strip()
                
                # æ£€æŸ¥å•ä¸ªåˆ†ç‰‡å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢è¿‡å¤§ï¼ˆä½¿ç”¨å¤´å°¾æˆªå–ï¼‰
                if len(content) > 200000:  # å¦‚æœå•ä¸ªåˆ†ç‰‡è¶…è¿‡200Kå­—ç¬¦
                    self._log(f"    - [Worker-{worker_id}] è­¦å‘Š: ç¬¬{start_page}-{end_page}é¡µå†…å®¹è¿‡å¤§ ({len(content)}å­—ç¬¦)ï¼Œä½¿ç”¨å¤´å°¾æˆªå–")
                    content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                
                # å°†ç»“æœå­˜å…¥ç¼“å­˜
                self.cache_manager.set(pdf_path, content, cache_prefix)
                return content
            else:
                return f"[ç¬¬{start_page}-{end_page}é¡µå¤„ç†å¤±è´¥ï¼šè¿”å›ç©ºå†…å®¹]"
        except Exception as e:
            error_msg = str(e)[:100]
            return f"[ç¬¬{start_page}-{end_page}é¡µå¤„ç†å¤±è´¥ï¼š{error_msg}]"

    def _extract_pdf_with_ai(self, pdf_path: str) -> str:
        """ä½¿ç”¨Google Gemini AIè¯†åˆ«PDFæ–‡ä»¶å†…å®¹ï¼Œæ™ºèƒ½é€‰æ‹©æœ€ä¼˜å¤„ç†ç­–ç•¥ï¼ˆé‡æ„ç‰ˆï¼‰"""
        filename = self._safe_basename(pdf_path)
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œå­˜åœ¨æ€§
        try:
            file_size = os.path.getsize(pdf_path)
            max_upload_size = 100 * 1024 * 1024  # 100MBä¸Šé™
            
            if file_size > max_upload_size:
                self._log(f"    - [è­¦å‘Š] æ–‡ä»¶è¿‡å¤§ ({file_size/1024/1024:.1f}MB)ï¼Œè¶…è¿‡100MBé™åˆ¶: {filename}")
                return f"æ–‡ä»¶è¿‡å¤§({file_size/1024/1024:.1f}MB)ï¼Œè¶…è¿‡100MBé™åˆ¶: {filename}"
                
            if not os.path.exists(pdf_path):
                self._log(f"    - [é”™è¯¯] æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                return f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}"
                
        except Exception as e:
            self._log(f"    - [é”™è¯¯] æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯: {filename} - {e}")
            return f"æ–‡ä»¶è®¿é—®é”™è¯¯: {filename}"
        
        # ğŸš€ é‡æ„åçš„ç®€åŒ–ç­–ç•¥ï¼šåªçœ‹é¡µæ•°ï¼Œç»Ÿä¸€é˜ˆå€¼
        total_pages = self._get_pdf_page_count(pdf_path)
        api_count = len(self.api_rotator.api_keys)
        
        self._log(f"    - [åˆ†æ] {filename}: {file_size/1024/1024:.1f}MB, {total_pages}é¡µ, {api_count}ä¸ªAPIå¯ç”¨")
        
        # ğŸ¯ ç»Ÿä¸€åˆ†ç‰‡ç­–ç•¥ï¼šé¡µæ•°>8é¡µ ä¸” æœ‰å¤šä¸ªAPI æ‰å¯ç”¨åˆ†ç‰‡
        if total_pages > 8 and api_count > 1:
            self._log(f"    - [åˆ†ç‰‡ç­–ç•¥] {total_pages}é¡µ>8é¡µ + {api_count}ä¸ªAPIï¼šå¯ç”¨åˆ†ç‰‡å¹¶å‘å¤„ç†")
            return self._extract_pdf_with_concurrent_pages(pdf_path)
        
        # ğŸ“„ éåˆ†ç‰‡ç­–ç•¥ï¼šé€‰æ‹©æœ€é€‚åˆçš„å•APIå¤„ç†æ–¹å¼
        elif file_size > 10 * 1024 * 1024:  # >10MBä½¿ç”¨File API
            self._log(f"    - [File APIç­–ç•¥] {file_size/1024/1024:.1f}MB>10MBï¼šä½¿ç”¨File APIå¤„ç†")
            return self._extract_pdf_with_file_api(pdf_path)
        
        else:  # <=10MBä½¿ç”¨ç›´æ¥ä¼ è¾“
            self._log(f"    - [ç›´æ¥ä¼ è¾“ç­–ç•¥] {file_size/1024/1024:.1f}MBâ‰¤10MBï¼šä½¿ç”¨ç›´æ¥ä¼ è¾“å¤„ç†")
            return self._extract_pdf_direct_transfer(pdf_path)
    
    def _extract_pdf_with_concurrent_pages(self, pdf_path: str) -> str:
        """é‡æ„åçš„åˆ†ç‰‡å¤„ç†ç­–ç•¥ï¼šç¡®ä¿æ­£ç¡®è¯†åˆ«å¤§PDFï¼ˆä¿®å¤ç‰ˆï¼‰"""
        filename = self._safe_basename(pdf_path)
        
        try:
            # è·å–PDFé¡µæ•°
            total_pages = self._get_pdf_page_count(pdf_path)
            api_count = len(self.api_rotator.api_keys)
            
            self._log(f"    - [åˆ†ç‰‡å¼€å§‹] {filename}: {total_pages}é¡µï¼Œ{api_count}ä¸ªAPI")
            
            # ğŸ”§ ä¿®å¤åˆ†ç‰‡é€»è¾‘ï¼šç¡®ä¿é¡µæ•°æ£€æŸ¥å‡†ç¡®
            if total_pages <= 0:
                self._log(f"    - [é”™è¯¯] æ— æ³•è·å–é¡µæ•°ï¼Œé™çº§ä¸ºç›´æ¥å¤„ç†: {filename}")
                return self._extract_pdf_with_file_api(pdf_path)
            
            # ğŸš€ æ™ºèƒ½åˆ†ç‰‡ï¼šæ ¹æ®APIæ•°é‡å’Œé¡µæ•°åŠ¨æ€è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
            # ç›®æ ‡ï¼šè®©æ¯ä¸ªAPIéƒ½æœ‰é€‚é‡ä»»åŠ¡ï¼Œé¿å…ç©ºé—²
            target_chunks = api_count * 2  # æ¯ä¸ªAPIåˆ†é…2ä¸ªä»»åŠ¡
            optimal_chunk_size = max(2, total_pages // target_chunks)  # æ¯ç‰‡æœ€å°‘2é¡µ
            
            # é™åˆ¶å•ä¸ªåˆ†ç‰‡ä¸è¦å¤ªå¤§ï¼ˆé¿å…å•ç‰‡å¤„ç†æ—¶é—´è¿‡é•¿ï¼‰
            if optimal_chunk_size > 6:
                optimal_chunk_size = 6
            
            # ç”Ÿæˆé¡µé¢åˆ†ç‰‡èŒƒå›´
            page_ranges = self._split_pdf_pages(pdf_path, optimal_chunk_size)
            
            if not page_ranges:
                self._log(f"    - [é”™è¯¯] åˆ†ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œé™çº§å¤„ç†: {filename}")
                return self._extract_pdf_with_file_api(pdf_path)
            
            self._log(f"    - [åˆ†ç‰‡é…ç½®] {total_pages}é¡µ â†’ {len(page_ranges)}ä¸ªåˆ†ç‰‡(æ¯ç‰‡â‰ˆ{optimal_chunk_size}é¡µ) â†’ {api_count}ä¸ªAPIå¹¶å‘")
            
            # ğŸ”¥ æ‰§è¡Œå¹¶å‘åˆ†ç‰‡å¤„ç†
            result = self._extract_pdf_pages_concurrent(pdf_path, page_ranges)
            
            # âœ… éªŒè¯å¤„ç†ç»“æœ
            if result and len(result.strip()) > 100:
                self._log(f"    - [åˆ†ç‰‡æˆåŠŸ] {filename} å¤„ç†å®Œæˆï¼Œæå– {len(result)} å­—ç¬¦")
                return result
            else:
                self._log(f"    - [åˆ†ç‰‡å¤±è´¥] ç»“æœä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œé™çº§å¤„ç†: {filename}")
                return self._extract_pdf_with_file_api(pdf_path)
                
        except Exception as e:
            self._log(f"    - [åˆ†ç‰‡å¼‚å¸¸] é™çº§ä¸ºç›´æ¥å¤„ç†: {filename} - {str(e)[:100]}")
            return self._extract_pdf_with_file_api(pdf_path)
    
    def _extract_pdf_with_file_api(self, pdf_path: str) -> str:
        """ä½¿ç”¨ç›´æ¥ä¼ è¾“æ–¹å¼å¤„ç†PDFæ–‡ä»¶ï¼ˆæ”¾å¼ƒFile APIä¸Šä¼ æ–¹å¼ï¼‰"""
        filename = self._safe_basename(pdf_path)
        
        def direct_transfer_call(client):
            try:
                from pathlib import Path
                
                # ç›´æ¥è¯»å–PDFå­—èŠ‚æ•°æ®
                filepath = Path(pdf_path)
                pdf_bytes = filepath.read_bytes()
                
                if len(pdf_bytes) == 0:
                    raise ValueError("PDFæ–‡ä»¶ä¸ºç©º")
                
                self._log(f"    - [ç›´æ¥ä¼ è¾“] æ–‡ä»¶å¤§å°: {len(pdf_bytes)/1024/1024:.1f}MB - {filename}")
                
                # ä¼˜åŒ–çš„AIæç¤ºè¯ï¼šå®Œæ•´æå–æ‰€æœ‰å†…å®¹
                optimized_prompt = f"""è¯·å®Œæ•´æå–PDFæ–‡ä»¶ï¼ˆ{filename}ï¼‰ä¸­çš„æ‰€æœ‰å†…å®¹ï¼š

**æå–è¦æ±‚**ï¼š
1. æ‰€æœ‰å¯è¯»çš„æ–‡å­—ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ­£æ–‡ã€æ³¨é‡Šç­‰ï¼‰
2. è¡¨æ ¼ä¸­çš„æ–‡å­—æ•°æ®å’Œæ•°å€¼
3. å›¾è¡¨çš„æ–‡å­—è¯´æ˜å’Œæ ‡æ³¨
4. é¡µçœ‰ã€é¡µè„šä¸­çš„æ–‡å­—ä¿¡æ¯
5. å›¾åƒä¸­çš„æ–‡å­—å†…å®¹ï¼ˆå¦‚æœå¯è¯†åˆ«ï¼‰

**å¤„ç†ç­–ç•¥**ï¼š
- ä¸è¦è·³è¿‡ä»»ä½•å†…å®¹ï¼Œå°½å¯èƒ½å®Œæ•´æå–
- ä¿æŒåŸæœ‰çš„æ–‡æ¡£ç»“æ„å’Œå±‚æ¬¡
- å¯¹äºå›¾åƒå†…å®¹ï¼Œå°è¯•OCRè¯†åˆ«å…¶ä¸­çš„æ–‡å­—
- ä½¿ç”¨æ¸…æ´æ˜äº†çš„æ ¼å¼è¾“å‡º

è¯·å¼€å§‹å®Œæ•´æå–ã€‚"""
                
                # ä½¿ç”¨ç›´æ¥ä¼ è¾“æ–¹å¼ç”Ÿæˆå†…å®¹
                response = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=[
                        types.Part.from_bytes(
                            data=pdf_bytes,
                            mime_type='application/pdf'
                        ),
                        optimized_prompt
                    ]
                )
                
                return response
                
            except Exception as e:
                raise e
        
        # é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self._log(f"    - [ç›´æ¥ä¼ è¾“] å¼€å§‹å¤„ç† (å°è¯• {attempt+1}/{max_retries}): {filename}")
                
                response = self._rotated_api_call(direct_transfer_call, max_retries=1)
                
                if response and response.text and response.text.strip():
                    content = response.text.strip()
                    content_length = len(content)
                    
                    # å†…å®¹é•¿åº¦æ£€æŸ¥ï¼Œé˜²æ­¢è¿‡å¤§ï¼ˆä½¿ç”¨å¤´å°¾æˆªå–ï¼‰
                    if content_length > 200000:  # 200Kå­—ç¬¦é˜ˆå€¼
                        self._log(f"    - [ç›´æ¥ä¼ è¾“] è­¦å‘Š: æ–‡ä»¶å†…å®¹è¿‡å¤§ ({content_length}å­—ç¬¦)ï¼Œå¤´å°¾æˆªå–")
                        content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                        content_length = len(content)
                    
                    self._log(f"    - [ç›´æ¥ä¼ è¾“] å¤„ç†æˆåŠŸ: {filename} (æå– {content_length} å­—ç¬¦)")
                    return content
                else:
                    self._log(f"    - [è­¦å‘Š] ç›´æ¥ä¼ è¾“è¿”å›ç©ºå†…å®¹: {filename}")
                    if attempt < max_retries - 1:
                        self._log(f"    - [é‡è¯•] ç­‰å¾…åé‡è¯•...")
                        time.sleep(2)
                        continue
                    else:
                        return f"ç›´æ¥ä¼ è¾“è¯†åˆ«è¿”å›ç©ºå†…å®¹: {filename}"
                        
            except Exception as e:
                error_str = str(e)
                self._log(f"    - [é”™è¯¯] ç›´æ¥ä¼ è¾“å¤„ç†å¤±è´¥: {filename} - {error_str[:100]}...")
                
                # ç‰¹æ®Šé”™è¯¯å¤„ç†
                if "file too large" in error_str.lower() or "size limit" in error_str.lower():
                    return f"æ–‡ä»¶è¿‡å¤§ï¼Œç›´æ¥ä¼ è¾“æ— æ³•å¤„ç†: {filename}"
                elif "quota" in error_str.lower() or "rate limit" in error_str.lower():
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 5
                        self._log(f"    - [é™æµ] APIé™æµï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return f"ç›´æ¥ä¼ è¾“APIè°ƒç”¨é™æµ: {filename}"
                else:
                    if attempt < max_retries - 1:
                        self._log(f"    - [é‡è¯•] ç­‰å¾…åé‡è¯•...")
                        time.sleep(3)
                        continue
                    else:
                        return f"ç›´æ¥ä¼ è¾“å¤„ç†å¼‚å¸¸: {filename} - {error_str[:100]}"
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return f"ç›´æ¥ä¼ è¾“å¤„ç†å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {filename}"
    
    def _extract_pdf_direct_transfer(self, pdf_path: str) -> str:
        """ä½¿ç”¨ç›´æ¥ä¼ è¾“æ–¹å¼å¤„ç†PDFæ–‡ä»¶ï¼ˆé€‚ç”¨äºè¾ƒå°æ–‡ä»¶ï¼‰"""
        filename = self._safe_basename(pdf_path)
        
        def direct_transfer_call(client):
            try:
                pdf_bytes = Path(pdf_path).read_bytes()
                if len(pdf_bytes) == 0:
                    raise ValueError("PDFæ–‡ä»¶ä¸ºç©º")
                
                # ä¼˜åŒ–çš„AIæç¤ºè¯ï¼šå®Œæ•´æå–æ‰€æœ‰å†…å®¹
                smart_prompt = f"""è¯·å®Œæ•´æå–PDFæ–‡ä»¶ï¼ˆ{filename}ï¼‰ä¸­çš„æ‰€æœ‰å†…å®¹ï¼š

**æå–ç›®æ ‡**ï¼š
âœ“ æ‰€æœ‰æ–‡å­—ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ­£æ–‡ã€æ³¨é‡Šï¼‰
âœ“ è¡¨æ ¼æ•°æ®å’Œæ•°å€¼ä¿¡æ¯
âœ“ å›¾è¡¨æ ‡é¢˜å’Œè¯´æ˜æ–‡å­—
âœ“ é¡µçœ‰é¡µè„šä¿¡æ¯
âœ“ å›¾åƒä¸­çš„æ–‡å­—å†…å®¹ï¼ˆå¦‚OCRè¯†åˆ«ï¼‰

**å¤„ç†åŸåˆ™**ï¼š
1. ä¸è¦è·³è¿‡ä»»ä½•å†…å®¹ï¼Œå°½å¯èƒ½å®Œæ•´æå–
2. ä¿æŒæ–‡æ¡£çš„é€»è¾‘ç»“æ„
3. å¯¹äºå›¾åƒå†…å®¹ï¼Œå°è¯•è¯†åˆ«å…¶ä¸­çš„æ–‡å­—
4. ä½¿ç”¨æ¸…æ™°çš„æ ¼å¼è¾“å‡º

è¯·å¼€å§‹å®Œæ•´æå–ã€‚"""
                
                return client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=[
                        types.Part.from_bytes(
                            data=pdf_bytes,
                            mime_type='application/pdf'
                        ),
                        smart_prompt
                    ]
                )
            except Exception as e:
                raise e
        
        # é‡è¯•æœºåˆ¶
        max_retries = 2
        for attempt in range(max_retries):
            try:
                self._log(f"    - [ç›´æ¥ä¼ è¾“] å¼€å§‹å¤„ç† (å°è¯• {attempt+1}/{max_retries}): {filename}")
                
                response = self._rotated_api_call(direct_transfer_call, max_retries=1)
                
                if response and response.text and response.text.strip():
                    content = response.text.strip()
                    content_length = len(content)
                    
                    # å†…å®¹é•¿åº¦æ£€æŸ¥ï¼Œé˜²æ­¢è¿‡å¤§ï¼ˆä½¿ç”¨å¤´å°¾æˆªå–ï¼‰
                    if content_length > 200000:  # 200Kå­—ç¬¦é˜ˆå€¼
                        self._log(f"    - [ç›´æ¥ä¼ è¾“] è­¦å‘Š: æ–‡ä»¶å†…å®¹è¿‡å¤§ ({content_length}å­—ç¬¦)ï¼Œå¤´å°¾æˆªå–")
                        content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                        content_length = len(content)
                    
                    self._log(f"    - [ç›´æ¥ä¼ è¾“] å¤„ç†æˆåŠŸ: {filename} (æå– {content_length} å­—ç¬¦)")
                    return content
                else:
                    self._log(f"    - [è­¦å‘Š] ç›´æ¥ä¼ è¾“è¿”å›ç©ºå†…å®¹: {filename}")
                    if attempt < max_retries - 1:
                        self._log(f"    - [é‡è¯•] ç­‰å¾…åé‡è¯•...")
                        time.sleep(1)
                        continue
                    else:
                        return f"ç›´æ¥ä¼ è¾“è¯†åˆ«è¿”å›ç©ºå†…å®¹: {filename}"
                        
            except Exception as e:
                error_str = str(e)
                self._log(f"    - [é”™è¯¯] ç›´æ¥ä¼ è¾“å¤±è´¥: {filename} - {error_str[:100]}...")
                
                # ç‰¹æ®Šé”™è¯¯å¤„ç†
                if "file too large" in error_str.lower():
                    return f"æ–‡ä»¶è¿‡å¤§ï¼Œç›´æ¥ä¼ è¾“æ— æ³•å¤„ç†: {filename}"
                elif "rate limit" in error_str.lower() or "quota" in error_str.lower():
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 3
                        self._log(f"    - [é™æµ] APIé™æµï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return f"ç›´æ¥ä¼ è¾“APIè°ƒç”¨é™æµ: {filename}"
                else:
                    if attempt < max_retries - 1:
                        self._log(f"    - [é‡è¯•] ç­‰å¾…åé‡è¯•...")
                        time.sleep(2)
                        continue
                    else:
                        return f"ç›´æ¥ä¼ è¾“å¼‚å¸¸: {filename} - {error_str[:100]}"
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return f"ç›´æ¥ä¼ è¾“å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {filename}"
    
    def _process_files_with_enhanced_concurrency(self, files: List[str], mid: int, file_type: str) -> List[str]:
        """å¢å¼ºçš„å¹¶å‘å¤„ç†å‡½æ•°ï¼Œæ”¯æŒåŠ¨æ€APIè°ƒåº¦å’Œä¼˜å…ˆçº§ç®¡ç†"""
        if not files:
            return []
        
        self._log(f"      ğŸš€ å¼€å§‹{file_type}å¢å¼ºå¹¶å‘å¤„ç†: {len(files)} ä¸ªæ–‡ä»¶")
        
        import concurrent.futures
        import threading
        import queue
        
        results = []
        results_lock = threading.Lock()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒæ•´å¹¶å‘ç­–ç•¥ï¼ˆä¼˜åŒ–ï¼šæœ€å¤§åŒ–APIåˆ©ç”¨ç‡ï¼‰
        if file_type == "å¤§æ–‡ä»¶":
            # å¤§æ–‡ä»¶ï¼šæœ€å¤§åŒ–å¹¶å‘æ•°ï¼Œæ¯ä¸ªAPIæœ€å¤š2ä¸ªçº¿ç¨‹
            max_workers = min(len(files), len(self.api_rotator.api_keys) * 2, 16)  # æå‡åˆ°æœ€å¤š16ä¸ªå¹¶å‘
            timeout_seconds = 300  # 5åˆ†é’Ÿè¶…æ—¶
        elif file_type == "ä¸­ç­‰æ–‡ä»¶":
            # ä¸­ç­‰æ–‡ä»¶ï¼šä½¿ç”¨ä¸­ç­‰å¹¶å‘æ•°
            max_workers = min(len(files), len(self.api_rotator.api_keys) * 2, 12)  # æå‡åˆ°æœ€å¤š12ä¸ªå¹¶å‘
            timeout_seconds = 180  # 3åˆ†é’Ÿè¶…æ—¶
        else:  # å°æ–‡ä»¶
            # å°æ–‡ä»¶ï¼šä½¿ç”¨è¾ƒå°å¹¶å‘æ•°ï¼Œä½†ä¹Ÿå……åˆ†åˆ©ç”¨API
            max_workers = min(len(files), len(self.api_rotator.api_keys), 8)  # æå‡åˆ°æœ€å¤š8ä¸ªå¹¶å‘
            timeout_seconds = 120  # 2åˆ†é’Ÿè¶…æ—¶
        
        self._log(f"      ğŸ“€ {file_type}å¹¶å‘ç­–ç•¥: {max_workers}ä¸ªå·¥ä½œçº¿ç¨‹, è¶…æ—¶{timeout_seconds}ç§’")
        
        def process_single_file_enhanced(file_path: str, worker_id: int) -> str:
            """å¢å¼ºçš„å•æ–‡ä»¶å¤„ç†å‡½æ•°"""
            filename = self._safe_basename(file_path)
            
            try:
                # ä½¿ç”¨æ›´ç²¾ç¡®çš„ç¼“å­˜é”®ï¼ŒåŒ…å«æ–‡ä»¶ç±»å‹
                import hashlib
                file_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
                cache_prefix = f"enhanced_{file_type}_{mid}_{file_hash}"
                
                # æ£€æŸ¥ç¼“å­˜
                cached_content = self.cache_manager.get(file_path, cache_prefix)
                if cached_content:
                    self._log(f"      [Worker-{worker_id}] ğŸ’¾ ä½¿ç”¨ç¼“å­˜({file_type}): {filename}")
                    content = cached_content
                else:
                    # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥
                    self._log(f"      [Worker-{worker_id}] ğŸš€ å¼€å§‹å¤„ç†{file_type}: {filename}")
                    
                    if file_path.endswith('.pdf'):
                        if file_type == "å¤§æ–‡ä»¶":
                            # å¤§æ–‡ä»¶ä½¿ç”¨ä¼˜åŒ–çš„å¤„ç†ç­–ç•¥
                            content = self._extract_pdf_with_priority_handling(file_path, "high")
                        elif file_type == "ä¸­ç­‰æ–‡ä»¶":
                            # ä¸­ç­‰æ–‡ä»¶ä½¿ç”¨æ ‡å‡†å¤„ç†
                            content = self._extract_pdf_with_priority_handling(file_path, "medium")
                        else:
                            # å°æ–‡ä»¶ä½¿ç”¨å¿«é€Ÿå¤„ç†
                            content = self._extract_pdf_with_priority_handling(file_path, "low")
                    else:
                        content = f"è·³è¿‡éPDFæ–‡ä»¶: {filename}"
                    
                    # å­˜å…¥ç¼“å­˜
                    if content and content.strip():
                        self.cache_manager.set(file_path, content, cache_prefix)
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒæ•´å†…å®¹é•¿åº¦æ§åˆ¶ï¼ˆä½¿ç”¨æ–°çš„å¤´å°¾æˆªå–ï¼‰
                if content:
                    if file_type == "å¤§æ–‡ä»¶" and len(content) > 200000:
                        self._log(f"      [Worker-{worker_id}] âš ï¸ å¤§æ–‡ä»¶å†…å®¹è¿‡å¤§ï¼Œå¤´å°¾æˆªå–: {filename} ({len(content)}å­—ç¬¦)")
                        content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                    elif file_type == "ä¸­ç­‰æ–‡ä»¶" and len(content) > 200000:
                        self._log(f"      [Worker-{worker_id}] âš ï¸ ä¸­ç­‰æ–‡ä»¶å†…å®¹è¿‡å¤§ï¼Œå¤´å°¾æˆªå–: {filename} ({len(content)}å­—ç¬¦)")
                        content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                    elif file_type == "å°æ–‡ä»¶" and len(content) > 200000:
                        self._log(f"      [Worker-{worker_id}] âš ï¸ å°æ–‡ä»¶å†…å®¹è¿‡å¤§ï¼Œå¤´å°¾æˆªå–: {filename} ({len(content)}å­—ç¬¦)")
                        content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                
                # æ ¼å¼åŒ–å†…å®¹
                if content and content.strip():
                    formatted_content = f"--- {file_type}æ–‡ä»¶: {filename} ---\n{content}"
                    self._log(f"      [Worker-{worker_id}] âœ… {file_type}å¤„ç†æˆåŠŸ: {filename} ({len(content)}å­—ç¬¦)")
                    return formatted_content
                else:
                    self._log(f"      [Worker-{worker_id}] âš ï¸ {file_type}å†…å®¹ä¸ºç©º: {filename}")
                    return f"--- {file_type}æ–‡ä»¶: {filename} ---\næ–‡ä»¶å†…å®¹ä¸ºç©º"
                    
            except Exception as e:
                error_msg = str(e)
                self._log(f"      [Worker-{worker_id}] âŒ {file_type}å¤„ç†å¤±è´¥: {filename} - {error_msg[:50]}...")
                return f"--- {file_type}æ–‡ä»¶: {filename} ---\næ–‡ä»¶å¤„ç†å¤±è´¥: {error_msg}"
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_file = {}
            for i, file_path in enumerate(files):
                future = executor.submit(process_single_file_enhanced, file_path, i+1)
                future_to_file[future] = file_path
            
            # æ”¶é›†ç»“æœï¼Œä½¿ç”¨è¶…æ—¶æœºåˆ¶ï¼ˆæ”¹è¿›ç‰ˆï¼‰
            completed_count = 0
            unfinished_futures = set(future_to_file.keys())
            
            try:
                for future in concurrent.futures.as_completed(future_to_file, timeout=timeout_seconds):
                    file_path = future_to_file[future]
                    filename = self._safe_basename(file_path)
                    unfinished_futures.discard(future)  # ç§»é™¤å·²å®Œæˆçš„future
                    
                    try:
                        result = future.result(timeout=30)  # å•ä¸ªä»»åŠ¡è¶…æ—¶30ç§’
                        with results_lock:
                            results.append(result)
                        completed_count += 1
                        self._log(f"      âœ… {file_type}å¹¶å‘å¤„ç†å®Œæˆ: {filename} ({completed_count}/{len(files)})")
                        
                    except concurrent.futures.TimeoutError:
                        self._log(f"      â° {file_type}å¤„ç†è¶…æ—¶: {filename}")
                        with results_lock:
                            results.append(f"--- {file_type}æ–‡ä»¶: {filename} ---\næ–‡ä»¶å¤„ç†è¶…æ—¶")
                        completed_count += 1
                        
                    except Exception as e:
                        error_msg = str(e)[:50]
                        self._log(f"      âŒ {file_type}å¹¶å‘å¤„ç†å¤±è´¥: {filename} - {error_msg}...")
                        with results_lock:
                            results.append(f"--- {file_type}æ–‡ä»¶: {filename} ---\næ–‡ä»¶å¹¶å‘å¤„ç†å¤±è´¥: {error_msg}")
                        completed_count += 1
                        
            except concurrent.futures.TimeoutError:
                self._log(f"      â° {file_type}é˜¶æ®µå¤„ç†è¶…æ—¶ï¼Œæœ‰{len(unfinished_futures)}ä¸ªä»»åŠ¡æœªå®Œæˆ")
                
                # å¤„ç†æœªå®Œæˆçš„ä»»åŠ¡
                for future in unfinished_futures:
                    file_path = future_to_file[future]
                    filename = self._safe_basename(file_path)
                    try:
                        if not future.done():
                            future.cancel()
                        with results_lock:
                            results.append(f"--- {file_type}æ–‡ä»¶: {filename} ---\næ–‡ä»¶å¤„ç†è¶…æ—¶")
                        completed_count += 1
                        self._log(f"      â° {file_type}è¶…æ—¶å¤„ç†: {filename} ({completed_count}/{len(files)})")
                    except Exception as e:
                        self._log(f"      âŒ {file_type}è¶…æ—¶å¤„ç†å¤±è´¥: {filename} - {e}")
        
        self._log(f"      ğŸ {file_type}å¢å¼ºå¹¶å‘å¤„ç†å®Œæˆ: æˆåŠŸ {completed_count}/{len(files)} ä¸ªæ–‡ä»¶")
        return results
    

    

        """æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©æœ€ä¼˜PDFå¤„ç†ç­–ç•¥"""
    
    def _extract_pdf_with_priority_handling(self, pdf_path: str, priority: str) -> str:
        """æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©æœ€ä¼˜PDFå¤„ç†ç­–ç•¥"""
        filename = self._safe_basename(pdf_path)
        
        try:
            file_size = os.path.getsize(pdf_path)
            
            if priority == "high":
                # é«˜ä¼˜å…ˆçº§ï¼šå¤§æ–‡ä»¶ä½¿ç”¨æœ€ä¼˜ç­–ç•¥
                if file_size > 10 * 1024 * 1024:  # >10MB
                    return self._extract_pdf_with_file_api(pdf_path)
                else:
                    return self._extract_pdf_direct_transfer(pdf_path)
                    
            elif priority == "medium":
                # ä¸­ä¼˜å…ˆçº§ï¼šä¸­ç­‰æ–‡ä»¶ä½¿ç”¨æ ‡å‡†ç­–ç•¥
                if file_size > 15 * 1024 * 1024:  # >15MB
                    return self._extract_pdf_with_file_api(pdf_path)
                else:
                    return self._extract_pdf_direct_transfer(pdf_path)
                    
            else:  # low priority
                # ä½ä¼˜å…ˆçº§ï¼šå°æ–‡ä»¶ä½¿ç”¨å¿«é€Ÿç­–ç•¥
                if file_size > 20 * 1024 * 1024:  # >20MB
                    return self._extract_pdf_with_file_api(pdf_path)
                else:
                    return self._extract_pdf_direct_transfer(pdf_path)
                    
        except Exception as e:
            self._log(f"    - [ä¼˜å…ˆçº§é”™è¯¯] æ–‡ä»¶å¤„ç†å¤±è´¥: {filename} - {e}")
            # é”™è¯¯æ—¶é™çº§åˆ°ç›´æ¥ä¼ è¾“
            return self._extract_pdf_direct_transfer(pdf_path)

    def _process_large_files_concurrently(self, large_files: List[str], mid: int) -> List[str]:
        """å¹¶å‘å¤„ç†å¤§æ–‡ä»¶ï¼Œåˆ©ç”¨å¤šä¸ªAPIåŒæ—¶å¤„ç†"""
        if not large_files:
            return []
        
        self._log(f"      ğŸš€ å¼€å§‹å¤§æ–‡ä»¶å¹¶å‘å¤„ç†: {len(large_files)} ä¸ªæ–‡ä»¶")
        
        import concurrent.futures
        import threading
        
        results = []
        results_lock = threading.Lock()
        
        def process_single_large_file(file_path: str, worker_id: int) -> str:
            """å¤„ç†å•ä¸ªå¤§æ–‡ä»¶"""
            filename = self._safe_basename(file_path)
            
            try:
                # ä½¿ç”¨æ›´ç²¾ç¡®çš„ç¼“å­˜é”®
                import hashlib
                file_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
                cache_prefix = f"large_file_{mid}_{file_hash}"
                
                # æ£€æŸ¥ç¼“å­˜
                cached_content = self.cache_manager.get(file_path, cache_prefix)
                if cached_content:
                    self._log(f"      [Worker-{worker_id}] ğŸ’¾ ä½¿ç”¨ç¼“å­˜: {filename}")
                    content = cached_content
                else:
                    # AIè¯†åˆ«å¤§æ–‡ä»¶
                    self._log(f"      [Worker-{worker_id}] ğŸš€ å¼€å§‹å¤„ç†å¤§æ–‡ä»¶: {filename}")
                    if file_path.endswith('.pdf'):
                        content = self._extract_pdf_content(file_path)
                    else:
                        content = f"è·³è¿‡éPDFæ–‡ä»¶: {filename}"
                    
                    # å­˜å…¥ç¼“å­˜
                    if content and content.strip():
                        self.cache_manager.set(file_path, content, cache_prefix)
                
                # å†…å®¹é•¿åº¦æ£€æŸ¥ï¼ˆä½¿ç”¨å¤´å°¾æˆªå–ï¼‰
                if content and len(content) > 200000:
                    self._log(f"      [Worker-{worker_id}] âš ï¸ å¤§æ–‡ä»¶å†…å®¹è¿‡å¤§ï¼Œå¤´å°¾æˆªå–: {filename} ({len(content)}å­—ç¬¦)")
                    content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                
                # æ ¼å¼åŒ–å†…å®¹
                if content and content.strip():
                    formatted_content = f"--- æ–‡ä»¶: {filename} ---\n{content}"
                    self._log(f"      [Worker-{worker_id}] âœ… å¤§æ–‡ä»¶å¤„ç†æˆåŠŸ: {filename} ({len(content)}å­—ç¬¦)")
                    return formatted_content
                else:
                    self._log(f"      [Worker-{worker_id}] âš ï¸ å¤§æ–‡ä»¶å†…å®¹ä¸ºç©º: {filename}")
                    return f"--- æ–‡ä»¶: {filename} ---\nå¤§æ–‡ä»¶å†…å®¹ä¸ºç©º"
                    
            except Exception as e:
                error_msg = str(e)
                self._log(f"      [Worker-{worker_id}] âŒ å¤§æ–‡ä»¶å¤„ç†å¤±è´¥: {filename} - {error_msg[:50]}...")
                return f"--- æ–‡ä»¶: {filename} ---\nå¤§æ–‡ä»¶å¤„ç†å¤±è´¥: {error_msg}"
        
        # ç¡®å®šå¹¶å‘æ•°é‡ï¼ˆä¼˜åŒ–ï¼šæœ€å¤§åŒ–APIåˆ©ç”¨ç‡ï¼‰
        max_workers = min(len(large_files), len(self.api_rotator.api_keys) * 2, 12)  # æ¯ä¸ªAPIæœ€å¤š2ä¸ªçº¿ç¨‹ï¼Œæœ€å¤š12ä¸ªå¹¶å‘
        
        self._log(f"      ğŸ“Š ä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹å¹¶å‘å¤„ç† {len(large_files)} ä¸ªå¤§æ–‡ä»¶")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_file = {}
            for i, file_path in enumerate(large_files):
                future = executor.submit(process_single_large_file, file_path, i+1)
                future_to_file[future] = file_path
            
            # æ”¶é›†ç»“æœï¼ˆæ”¹è¿›è¶…æ—¶å¤„ç†ï¼‰
            completed_count = 0
            unfinished_futures = set(future_to_file.keys())
            
            try:
                for future in concurrent.futures.as_completed(future_to_file, timeout=300):  # 5åˆ†é’Ÿè¶…æ—¶
                    file_path = future_to_file[future]
                    filename = self._safe_basename(file_path)
                    unfinished_futures.discard(future)
                    
                    try:
                        result = future.result(timeout=180)  # 3åˆ†é’Ÿè¶…æ—¶
                        with results_lock:
                            results.append(result)
                        completed_count += 1
                        self._log(f"      âœ… å¤§æ–‡ä»¶å¹¶å‘å¤„ç†å®Œæˆ: {filename} ({completed_count}/{len(large_files)})")
                        
                    except concurrent.futures.TimeoutError:
                        self._log(f"      â° å¤§æ–‡ä»¶å¤„ç†è¶…æ—¶: {filename}")
                        with results_lock:
                            results.append(f"--- æ–‡ä»¶: {filename} ---\nå¤§æ–‡ä»¶å¤„ç†è¶…æ—¶")
                        completed_count += 1
                        
                    except Exception as e:
                        error_msg = str(e)[:50]
                        self._log(f"      âŒ å¤§æ–‡ä»¶å¹¶å‘å¤„ç†å¤±è´¥: {filename} - {error_msg}...")
                        with results_lock:
                            results.append(f"--- æ–‡ä»¶: {filename} ---\nå¤§æ–‡ä»¶å¹¶å‘å¤„ç†å¤±è´¥: {error_msg}")
                        completed_count += 1
                        
            except concurrent.futures.TimeoutError:
                self._log(f"      â° å¤§æ–‡ä»¶é˜¶æ®µè¶…æ—¶ï¼Œæœ‰{len(unfinished_futures)}ä¸ªä»»åŠ¡æœªå®Œæˆ")
                
                # å¤„ç†æœªå®Œæˆçš„ä»»åŠ¡
                for future in unfinished_futures:
                    file_path = future_to_file[future]
                    filename = self._safe_basename(file_path)
                    try:
                        if not future.done():
                            future.cancel()
                        with results_lock:
                            results.append(f"--- æ–‡ä»¶: {filename} ---\nå¤§æ–‡ä»¶å¤„ç†è¶…æ—¶")
                        completed_count += 1
                        self._log(f"      â° å¤§æ–‡ä»¶è¶…æ—¶å¤„ç†: {filename} ({completed_count}/{len(large_files)})")
                    except Exception as e:
                        self._log(f"      âŒ å¤§æ–‡ä»¶è¶…æ—¶å¤„ç†å¤±è´¥: {filename} - {e}")
        
        self._log(f"      ğŸ å¤§æ–‡ä»¶å¹¶å‘å¤„ç†å®Œæˆ: æˆåŠŸ {completed_count}/{len(large_files)} ä¸ªæ–‡ä»¶")
        return results
    
    def _extract_pdf_single_api(self, pdf_path: str) -> str:
        """ä½¿ç”¨å•ä¸ªAPIè¿›è¡Œå¸¸è§„PDFå¤„ç†"""
        filename = self._safe_basename(pdf_path)
        
        # ä½¿ç”¨APIè½®è¯¢æœºåˆ¶è¿›è¡ŒAIè¯†åˆ«ï¼Œå–æ¶ˆè¶…æ—¶é™åˆ¶
        def ai_call_with_full_content(client):
            import concurrent.futures
            import threading
            
            def actual_ai_call():
                try:
                    pdf_bytes = Path(pdf_path).read_bytes()
                    if len(pdf_bytes) == 0:
                        raise ValueError("PDFæ–‡ä»¶ä¸ºç©º")
                    
                    return client.models.generate_content(
                        model="gemini-2.5-flash", 
                        contents=[
                            types.Part.from_bytes(
                                data=pdf_bytes, 
                                mime_type='application/pdf'
                            ), 
                        f"è¯·å®Œæ•´æå–PDFæ–‡ä»¶ï¼ˆ{filename}ï¼‰çš„æ‰€æœ‰å†…å®¹ï¼š\n\n**æå–è¦æ±‚**ï¼šæ‰€æœ‰æ–‡å­—ã€è¡¨æ ¼æ•°æ®ã€å›¾è¡¨è¯´æ˜ã€å›¾åƒä¸­çš„æ–‡å­—å†…å®¹\n**å¤„ç†åŸåˆ™**ï¼šä¸è¦è·³è¿‡ä»»ä½•å†…å®¹ï¼Œå°½å¯èƒ½å®Œæ•´æå–ã€‚è¯·ä¿æŒç²¾å‡†ï¼Œä¼˜å…ˆå¤„ç†æ¸…æ™°å¯è¯»çš„æ–‡å­—ã€‚"
                        ]
                    )
                except Exception as e:
                    raise e
            
            # å–æ¶ˆè¶…æ—¶é™åˆ¶ï¼Œè®©AIå……åˆ†å¤„ç†
            return actual_ai_call()
        
        # é‡è¯•æœºåˆ¶ï¼ˆä¸ºå¤§å‹PDFä¼˜åŒ–ï¼‰
        max_retries = 2  # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œæå‡æ•´ä½“é€Ÿåº¦
        for attempt in range(max_retries):
            try:
                self._log(f"    - [AI] å¼€å§‹é«˜æ•ˆè¯†åˆ« (å°è¯• {attempt+1}/{max_retries}): {filename}")
                
                response = self._rotated_api_call(ai_call_with_full_content, max_retries=1)
                
                if response and response.text and response.text.strip(): 
                    content = response.text.strip()
                    content_length = len(content)
                    
                    # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢å•ä¸ªæ–‡ä»¶è¿‡å¤§ï¼ˆä½¿ç”¨å¤´å°¾æˆªå–ï¼‰
                    if content_length > 200000:  # å¦‚æœå•ä¸ªæ–‡ä»¶è¶…è¿‡200Kå­—ç¬¦
                        self._log(f"    - [AI] è­¦å‘Š: æ–‡ä»¶å†…å®¹è¿‡å¤§ ({content_length}å­—ç¬¦)ï¼Œä½¿ç”¨å¤´å°¾æˆªå–")
                        content = self._smart_truncate_content(content, max_length=200000, head_size=5000, tail_size=5000)
                        content_length = len(content)
                    
                    self._log(f"    - [AI] å®Œæ•´è¯†åˆ«æˆåŠŸ: {filename} (æå– {content_length} å­—ç¬¦)")
                    return content
                else:
                    self._log(f"    - [è­¦å‘Š] AIè¿”å›ç©ºå†…å®¹: {filename}")
                    if attempt < max_retries - 1:
                        self._log(f"    - [é‡è¯•] ç­‰å¾…åé‡è¯•...")
                        time.sleep(1)  # å‡å°‘ç­‰å¾…æ—¶é—´ä»¥æå‡é€Ÿåº¦
                        continue
                    else:
                        return f"AIè¯†åˆ«è¿”å›ç©ºå†…å®¹: {filename}"
                        
            except Exception as e:
                error_str = str(e)
                self._log(f"    - [é”™è¯¯] è¯†åˆ«å¤±è´¥: {filename} - {error_str[:100]}...")
                
                # ç‰¹æ®Šé”™è¯¯å¤„ç†
                if "file is too large" in error_str.lower():
                    return f"æ–‡ä»¶è¿‡å¤§ï¼ŒAPIæ— æ³•å¤„ç†: {filename}"
                elif "invalid pdf" in error_str.lower() or "not a pdf" in error_str.lower():
                    return f"æ— æ•ˆçš„PDFæ–‡ä»¶: {filename}"
                elif "rate limit" in error_str.lower() or "quota" in error_str.lower():
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 5  # å‡å°‘ç­‰å¾…æ—¶é—´ï¼Œæå‡æ•´ä½“é€Ÿåº¦
                        self._log(f"    - [é™æµ] APIé™æµï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return f"APIè°ƒç”¨é™æµ: {filename}"
                else:
                    if attempt < max_retries - 1:
                        self._log(f"    - [é‡è¯•] ç­‰å¾…2ç§’åé‡è¯•...")
                        time.sleep(2)  # å‡å°‘ç­‰å¾…æ—¶é—´
                        continue
                    else:
                        return f"AIè¯†åˆ«å¼‚å¸¸: {filename} - {error_str[:100]}"
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return f"AIè¯†åˆ«å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {filename}"

    def check_empty_materials(self):
        self._log("ğŸ” æ­£åœ¨æ£€æŸ¥ææ–™å®Œæ•´æ€§...")
        self.validation_results["empty_materials"] = [mid for mid, m in self.materials.items() if m.is_empty]
        self._log(f"  ğŸŸ¡ å‘ç° {len(self.validation_results['empty_materials'])} é¡¹ç¼ºå¤±æˆ–ç©ºææ–™")

    def _match_rules_to_materials(self):
        self._log("ğŸ”— æ­£åœ¨ä¸ºææ–™åŒ¹é…é€‚ç”¨è§„åˆ™...")
        
        # åˆ†ç¦»ä¸“é¡¹è§„åˆ™å’Œé€šç”¨è§„åˆ™
        specific_rules = []  # ä¸“é¡¹è§„åˆ™(æ¥è‡ªExcel)
        universal_rules = []  # é€šç”¨è§„åˆ™(æ¥è‡ªMarkdown)
        
        for r in self.rules:
            if hasattr(r, 'source_file'):
                if r.source_file.endswith('.md'):
                    universal_rules.append(r)
                elif r.source_file.endswith('.xlsx'):
                    specific_rules.append(r)
        
        self._log(f"  ğŸ“‹ è¯†åˆ«åˆ°ä¸“é¡¹è§„åˆ™: {len(specific_rules)} æ¡ï¼Œé€šç”¨è§„åˆ™: {len(universal_rules)} æ¡")
        
        for m in self.materials.values():
            if not m.is_empty:
                material_id = m.id
                m.applicable_rules = []
                
                # 1. åŒ¹é…ä¸“é¡¹è§„åˆ™ï¼šææ–™IDå¯¹åº”è§„åˆ™æ–‡ä»¶ç¼–å·
                # ä¾‹å¦‚ï¼šææ–™ID=2(å·¥ä½œç»å†) å¯¹åº” 2.å·¥ä½œç»å†è§„åˆ™é›†.xlsx
                specific_matched = 0
                for r in specific_rules:
                    if r.source_file.startswith(f"{material_id}."):
                        r.rule_type = "ä¸“é¡¹è§„åˆ™"  # æ ‡è®°è§„åˆ™ç±»å‹
                        m.applicable_rules.append(r)
                        specific_matched += 1
                
                # 2. æ·»åŠ é€šç”¨è§„åˆ™ï¼šæ‰€æœ‰ææ–™éƒ½éœ€è¦æ£€æŸ¥é€šç”¨è§„åˆ™
                for r in universal_rules:
                    r.rule_type = "é€šç”¨è§„åˆ™"  # æ ‡è®°è§„åˆ™ç±»å‹
                    m.applicable_rules.append(r)
                
                self._log(f"  ğŸ“Š ææ–™{material_id}({m.name}): ä¸“é¡¹è§„åˆ™ {specific_matched} æ¡ + é€šç”¨è§„åˆ™ {len(universal_rules)} æ¡ = æ€»è®¡ {len(m.applicable_rules)} æ¡")

    def generate_full_report(self) -> str:
        self._log("âš™ï¸ å¼€å§‹ç”Ÿæˆå®Œæ•´æŠ¥å‘Š...")
        self._log("---é˜¶æ®µ1: ç‹¬ç«‹éªŒè¯ä¸ä¿¡æ¯æå–---")
        
        # è®¡ç®—éœ€è¦å¤„ç†çš„ææ–™æ•°é‡
        valid_materials = [material for material in self.materials.values() if not material.is_empty]
        total_materials = len(valid_materials)
        
        if total_materials == 0:
            self._log("âš ï¸ æœªå‘ç°ä»»ä½•æœ‰æ•ˆææ–™ï¼Œè·³è¿‡å®¡æ ¸æµç¨‹")
            return self._assemble_report({}, "âš ï¸ æœªå‘ç°ä»»ä½•æœ‰æ•ˆææ–™ã€‚")
        
        self._log(f"ğŸ“Š éœ€è¦å¤„ç† {total_materials} ä¸ªæœ‰æ•ˆææ–™")
        
        # ä¼°ç®—å¤„ç†æ—¶é—´ï¼ˆæ¯ä¸ªææ–™çº¦10-30ç§’ï¼‰
        estimated_time = total_materials * 20  # å¹³å‡ä¼°ç®—
        self._log(f"ğŸ•°ï¸ é¢„è®¡å¤„ç†æ—¶é—´: {estimated_time//60}åˆ†{estimated_time%60}ç§’ï¼ˆæ ¹æ®APIå“åº”é€Ÿåº¦å¯èƒ½æœ‰æ‰€ä¸åŒï¼‰")
        
        results, core_infos = {}, {}
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ææ–™éªŒè¯å’Œä¿¡æ¯æå–
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:  # å‡å°‘å¹¶å‘çº¿ç¨‹æ•°ä»¥é¿å…èµ„æºç«äº‰
            # æäº¤éªŒè¯ä»»åŠ¡
            validation_futures = {}
            info_futures = {}
            
            for mid, material in self.materials.items():
                if not material.is_empty:
                    # æäº¤ç‹¬ç«‹éªŒè¯ä»»åŠ¡
                    validation_future = executor.submit(self._validate_single_material, material)
                    validation_futures[validation_future] = mid
                    
                    # æäº¤æ ¸å¿ƒä¿¡æ¯æå–ä»»åŠ¡
                    info_future = executor.submit(self._extract_core_info, material)
                    info_futures[info_future] = mid
            
            # æ”¶é›†éªŒè¯ç»“æœï¼Œæ·»åŠ è¶…æ—¶æœºåˆ¶
            self._log(f"  ğŸ” æ­£åœ¨å¤„ç† {len(validation_futures)} ä¸ªéªŒè¯ä»»åŠ¡...")
            validation_completed = 0
            
            try:
                for future in concurrent.futures.as_completed(validation_futures, timeout=600):  # 10åˆ†é’Ÿè¶…æ—¶
                    mid = validation_futures[future]
                    try:
                        validation_text = future.result(timeout=180)  # 3åˆ†é’Ÿè¶…æ—¶
                        results[mid] = validation_text
                        self._parse_and_log_violations(self.materials[mid], validation_text)
                        validation_completed += 1
                        self._log(f"  âœ… ç‹¬ç«‹éªŒè¯è¿›åº¦: {validation_completed}/{len(validation_futures)} - {self.materials[mid].name}")
                        
                        # æ¯å®Œæˆ5ä¸ªä»»åŠ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æç¤º
                        if validation_completed % 5 == 0 or validation_completed == len(validation_futures):
                            progress_percent = int((validation_completed / len(validation_futures)) * 100)
                            self._log(f"  ğŸ“Š éªŒè¯è¿›åº¦: {progress_percent}% ({validation_completed}/{len(validation_futures)})")
                            
                    except concurrent.futures.TimeoutError:
                        self._log(f"  â³ ç‹¬ç«‹éªŒè¯è¶…æ—¶ {self.materials[mid].name}")
                        results[mid] = f"AIåˆ†æè¶…æ—¶: {self.materials[mid].name}"
                    except Exception as e:
                        self._log(f"  âŒ ç‹¬ç«‹éªŒè¯å¤±è´¥ {self.materials[mid].name}: {e}")
                        results[mid] = f"AIåˆ†æå¤±è´¥: {e}"
                        
            except concurrent.futures.TimeoutError:
                self._log(f"  âš ï¸ éªŒè¯é˜¶æ®µæ•´ä½“è¶…æ—¶ï¼Œå·²å®Œæˆ {validation_completed}/{len(validation_futures)} ä¸ªä»»åŠ¡")
            
            # æ”¶é›†æ ¸å¿ƒä¿¡æ¯æå–ç»“æœï¼Œæ·»åŠ è¶…æ—¶æœºåˆ¶
            self._log(f"  ğŸ’¡ æ­£åœ¨å¤„ç† {len(info_futures)} ä¸ªä¿¡æ¯æå–ä»»åŠ¡...")
            info_completed = 0
            
            try:
                for future in concurrent.futures.as_completed(info_futures, timeout=600):  # 10åˆ†é’Ÿè¶…æ—¶
                    mid = info_futures[future]
                    try:
                        core_info = future.result(timeout=180)  # 3åˆ†é’Ÿè¶…æ—¶
                        core_infos[mid] = core_info
                        info_completed += 1
                        self._log(f"  âœ… ä¿¡æ¯æå–è¿›åº¦: {info_completed}/{len(info_futures)} - {self.materials[mid].name}")
                        
                        # æ¯å®Œæˆ5ä¸ªä»»åŠ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æç¤º
                        if info_completed % 5 == 0 or info_completed == len(info_futures):
                            progress_percent = int((info_completed / len(info_futures)) * 100)
                            self._log(f"  ğŸ“Š ä¿¡æ¯æå–è¿›åº¦: {progress_percent}% ({info_completed}/{len(info_futures)})")
                            
                    except concurrent.futures.TimeoutError:
                        self._log(f"  â³ ä¿¡æ¯æå–è¶…æ—¶ {self.materials[mid].name}")
                        core_infos[mid] = {"error": f"æ ¸å¿ƒä¿¡æ¯æå–è¶…æ—¶: {self.materials[mid].name}"}
                    except Exception as e:
                        self._log(f"  âŒ æ ¸å¿ƒä¿¡æ¯æå–å¤±è´¥ {self.materials[mid].name}: {e}")
                        core_infos[mid] = {"error": f"æ ¸å¿ƒä¿¡æ¯æå–å¤±è´¥: {e}"}
                        
            except concurrent.futures.TimeoutError:
                self._log(f"  âš ï¸ ä¿¡æ¯æå–é˜¶æ®µæ•´ä½“è¶…æ—¶ï¼Œå·²å®Œæˆ {info_completed}/{len(info_futures)} ä¸ªä»»åŠ¡")
        
        self._log("âœ… é˜¶æ®µ1å®Œæˆ")
        self._log("---é˜¶æ®µ2: æ ¸å¿ƒä¿¡æ¯äº¤å‰æ£€éªŒ---")
        cross_validation_report = self._perform_cross_validation(core_infos)
        self._log("âœ… é˜¶æ®µ2å®Œæˆ")
        self._log("---é˜¶æ®µ3: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š---")
        report = self._assemble_report(results, cross_validation_report)
        self._log("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæ¯•!")
        return report

    def _validate_single_material(self, material: MaterialInfo) -> str:
        """åˆ†é˜¶æ®µéªŒè¯ææ–™ï¼šå…ˆä¸“é¡¹è§„åˆ™ï¼Œå†é€šç”¨è§„åˆ™"""
        validation_results = []
        
        # åˆ†ç¦»ä¸“é¡¹è§„åˆ™å’Œé€šç”¨è§„åˆ™
        specific_rules = [r for r in material.applicable_rules if getattr(r, 'rule_type', '') == 'ä¸“é¡¹è§„åˆ™']
        universal_rules = [r for r in material.applicable_rules if getattr(r, 'rule_type', '') == 'é€šç”¨è§„åˆ™']
        
        # é˜¶æ®µ1ï¼šä¸“é¡¹è§„åˆ™éªŒè¯
        if specific_rules:
            self._log(f"    ğŸ“‹ é˜¶æ®µ1: éªŒè¯ä¸“é¡¹è§„åˆ™ ({len(specific_rules)}æ¡)")
            specific_result = self._validate_with_rules(material, specific_rules, "ä¸“é¡¹è§„åˆ™")
            validation_results.append(f"## ä¸“é¡¹è§„åˆ™éªŒè¯ç»“æœ\n{specific_result}")
        
        # é˜¶æ®µ2ï¼šé€šç”¨è§„åˆ™éªŒè¯
        if universal_rules:
            self._log(f"    ğŸ“‹ é˜¶æ®µ2: éªŒè¯é€šç”¨è§„åˆ™ ({len(universal_rules)}æ¡)")
            universal_result = self._validate_with_rules(material, universal_rules, "é€šç”¨è§„åˆ™")
            validation_results.append(f"## é€šç”¨è§„åˆ™éªŒè¯ç»“æœ\n{universal_result}")
        
        return "\n\n".join(validation_results)
    
    def _validate_with_rules(self, material: MaterialInfo, rules: list, rule_type: str) -> str:
        """ä½¿ç”¨æŒ‡å®šè§„åˆ™é›†éªŒè¯ææ–™"""
        sorted_rules = sorted(rules, key=lambda r: {"æé«˜": 4, "é«˜": 3, "ä¸­": 2, "ä½": 1}.get(r.ä¼˜å…ˆçº§, 0), reverse=True)
        rules_text = "\n".join([f"{i}. ã€{r.ä¼˜å…ˆçº§}ã€‘{r.æ ¸å¿ƒé—®é¢˜}: {r.è§„åˆ™å†…å®¹}" for i, r in enumerate(sorted_rules, 1)])
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„èŒç§°è¯„å®¡ä¸“å®¶ï¼Œè¯·å®¡æŸ¥ã€Š{material.name}ã€‹ææ–™ï¼Œæ£€æŸ¥æ˜¯å¦ç¬¦åˆ{rule_type}è¦æ±‚ã€‚

=== å¾…å®¡æŸ¥çš„{rule_type} ===
{rules_text}

=== ææ–™å†…å®¹ ===
{material.content[:5000] if material.content else 'ææ–™å†…å®¹ä¸ºç©º'}

=== å®¡æŸ¥è¦æ±‚ ===
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†æ ¼å¼é€æ¡æ£€æŸ¥æ¯ä¸ªè§„åˆ™ï¼š

è§„åˆ™X: [è§„åˆ™åç§°]
åˆ¤æ–­: âœ…ç¬¦åˆ / âŒè¿å
ç†ç”±: [å¦‚æœè¿åï¼Œè¯¦ç»†è¯´æ˜è¿åçš„å…·ä½“å†…å®¹ï¼›å¦‚æœç¬¦åˆï¼Œå¯ç®€è¿°ç¬¦åˆæƒ…å†µ]

=== è¾“å‡ºæ ¼å¼ç¤ºä¾‹ ===
è§„åˆ™1: æ—¶é—´é€»è¾‘ä¸€è‡´æ€§
åˆ¤æ–­: âŒè¿å
ç†ç”±: å‘ç°å·¥ä½œæ—¶é—´å­˜åœ¨é‡å ï¼Œ2020å¹´3æœˆåœ¨Aå…¬å¸å·¥ä½œçš„åŒæ—¶ï¼Œ2020å¹´2æœˆå·²åœ¨Bå…¬å¸ä»»èŒ

è§„åˆ™2: å•ä½ä¿¡æ¯ä¸€è‡´æ€§  
åˆ¤æ–­: âœ…ç¬¦åˆ
ç†ç”±: æ‰€æœ‰ææ–™ä¸­å•ä½åç§°è¡¨è¿°ä¸€è‡´

=== é‡è¦è¯´æ˜ ===
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§"è§„åˆ™X: [åç§°]\nåˆ¤æ–­: [ç»“æœ]\nç†ç”±: [è¯´æ˜]"çš„æ ¼å¼è¾“å‡º
2. åªæœ‰ç¡®å®å‘ç°æ˜ç¡®è¿åçš„æƒ…å†µï¼Œæ‰åˆ¤æ–­ä¸º"è¿å"
3. å¦‚æœææ–™å†…å®¹ä¸è¶³ä»¥åˆ¤æ–­ï¼Œè¯·è¯´æ˜"ææ–™ä¿¡æ¯ä¸è¶³ï¼Œæ— æ³•åˆ¤æ–­"
4. ä¸è¦ç¼–é€ æˆ–æ¨æµ‹æ²¡æœ‰åœ¨ææ–™ä¸­æ˜ç¡®å‡ºç°çš„ä¿¡æ¯
5. é‡ç‚¹å…³æ³¨æ—¶é—´é€»è¾‘ã€æ•°æ®ä¸€è‡´æ€§ã€èŒä¸šèµ„å†ç­‰å…³é”®é—®é¢˜
6. æ¯ä¸ªè§„åˆ™å¿…é¡»å•ç‹¬æˆæ®µï¼Œç”¨ç©ºè¡Œåˆ†éš”"""
        
        # ä½¿ç”¨APIè½®è¯¢æœºåˆ¶ï¼Œé™ä½æ¸©åº¦æé«˜å‡†ç¡®æ€§
        def ai_call(client):
            return client.models.generate_content(
                model="gemini-2.5-flash", 
                contents=prompt, 
            )
        
        try:
            response = self._rotated_api_call(ai_call)
            return response.text.strip()
        except Exception as e:
            return f"{rule_type}éªŒè¯å¤±è´¥: {e}"

    def _parse_and_log_violations(self, material: MaterialInfo, validation_text: str):
        sorted_rules = sorted(material.applicable_rules, key=lambda r: {"æé«˜": 4, "é«˜": 3, "ä¸­": 2, "ä½": 1}.get(r.ä¼˜å…ˆçº§, 0), reverse=True)
        violations = []  # å­˜å‚¨å½“å‰ææ–™çš„è¿è§„é—®é¢˜
        
        for i, rule in enumerate(sorted_rules, 1):
            for line in validation_text.split('\n'):
                if (f"è§„åˆ™{i}" in line or line.strip().startswith(f"{i}.")) and ("è¿å" in line or "ä¸ä¸€è‡´" in line):
                    violation_text = f"ã€{rule.ä¼˜å…ˆçº§}ã€‘{rule.æ ¸å¿ƒé—®é¢˜}: {line.strip()}"
                    violations.append(violation_text)
                    self.high_priority_violations.append(f"åœ¨ã€Š{material.name}ã€‹ä¸­å‘ç°{rule.ä¼˜å…ˆçº§}ä¼˜å…ˆçº§é—®é¢˜: {rule.æ ¸å¿ƒé—®é¢˜}")
                    self._log(f"    - âš ï¸ å‘ç°{rule.ä¼˜å…ˆçº§}ä¼˜å…ˆçº§è¿è§„: {rule.æ ¸å¿ƒé—®é¢˜}")
                    break
        
        # å°†è¿è§„é—®é¢˜å­˜å‚¨åˆ°ææ–™å¯¹è±¡ä¸­
        material.rule_violations = violations

    def _extract_core_info(self, material: MaterialInfo) -> Dict[str, Any]:
        """æå–ææ–™æ ¸å¿ƒä¿¡æ¯ - ç»Ÿä¸€æå–å§“åã€å·¥ä½œå•ä½ï¼Œå·¥ä½œç»å†ææ–™ç‰¹æ®Šå¤„ç†"""
        
        # æ ¹æ®ææ–™ç±»å‹æ„å»ºä¸åŒçš„æå–è§„åˆ™
        if material.id == 2:  # å·¥ä½œç»å†ææ–™ç‰¹æ®Šå¤„ç†
            prompt = f"""è¯·ä»è¿™ä»½ã€Š{material.name}ã€‹ææ–™ä¸­ï¼Œæå–ä»¥ä¸‹æ ¸å¿ƒä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š

**ç»Ÿä¸€æå–å­—æ®µ**ï¼š
- "å§“å": ç”³è¯·äººçš„å§“å
- "å·¥ä½œå•ä½": å½“å‰æˆ–ä¸»è¦å·¥ä½œå•ä½åç§°

**å·¥ä½œç»å†ç‰¹æ®Šå­—æ®µ**ï¼š
- "å·¥ä½œç»å†è¯¦æƒ…": åŒ…å«æ¯æ®µå·¥ä½œç»å†çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ ¼å¼ä¸ºæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
  - "èµ·å§‹æ—¶é—´": å¼€å§‹å·¥ä½œçš„æ—¶é—´ï¼ˆå¹´æœˆï¼‰
  - "ç»“æŸæ—¶é—´": ç»“æŸå·¥ä½œçš„æ—¶é—´ï¼ˆå¹´æœˆï¼Œå¦‚ä»åœ¨èŒè¯·æ ‡æ³¨"è‡³ä»Š"ï¼‰
  - "å·¥ä½œåœ°ç‚¹": å·¥ä½œæ‰€åœ¨çš„åŸå¸‚æˆ–åœ°åŒº
  - "å•ä½åç§°": å…·ä½“çš„å·¥ä½œå•ä½åç§°
  - "èŒåŠ¡": åœ¨è¯¥å•ä½æ‹…ä»»çš„èŒåŠ¡

**è¿”å›æ ¼å¼ç¤ºä¾‹**ï¼š
{{
  "å§“å": "å¼ ä¸‰",
  "å·¥ä½œå•ä½": "æŸæŸå¤§å­¦",
  "å·¥ä½œç»å†è¯¦æƒ…": [
    {{
      "èµ·å§‹æ—¶é—´": "2018å¹´9æœˆ",
      "ç»“æŸæ—¶é—´": "2021å¹´7æœˆ", 
      "å·¥ä½œåœ°ç‚¹": "åŒ—äº¬å¸‚",
      "å•ä½åç§°": "æŸæŸç§‘æŠ€æœ‰é™å…¬å¸",
      "èŒåŠ¡": "è½¯ä»¶å·¥ç¨‹å¸ˆ"
    }},
    {{
      "èµ·å§‹æ—¶é—´": "2021å¹´8æœˆ",
      "ç»“æŸæ—¶é—´": "è‡³ä»Š",
      "å·¥ä½œåœ°ç‚¹": "ä¸Šæµ·å¸‚", 
      "å•ä½åç§°": "æŸæŸå¤§å­¦",
      "èŒåŠ¡": "è®²å¸ˆ"
    }}
  ]
}}

---ææ–™å†…å®¹---
{material.content[:5000] if material.content else 'ææ–™å†…å®¹ä¸ºç©º'}"""
        else:
            # å…¶ä»–ææ–™çš„é€šç”¨æå–è§„åˆ™
            prompt = f"""è¯·ä»è¿™ä»½ã€Š{material.name}ã€‹ææ–™ä¸­ï¼Œæå–ä»¥ä¸‹æ ¸å¿ƒä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š

**å¿…é¡»æå–çš„å­—æ®µ**ï¼š
- "å§“å": ç”³è¯·äººçš„å§“å


**å¯é€‰æå–çš„å­—æ®µ**ï¼ˆå¦‚æœææ–™ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼‰ï¼š
- "å·¥ä½œå•ä½": å½“å‰æˆ–ä¸»è¦å·¥ä½œå•ä½åç§°
- "èº«ä»½è¯å·": èº«ä»½è¯å·ç ï¼ˆå¦‚æœ‰ï¼‰
- "èŒåŠ¡": èŒåŠ¡æˆ–èŒç§°ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
- "ä¸“ä¸š": ä¸“ä¸šé¢†åŸŸæˆ–å­¦ç§‘ï¼ˆå¦‚æœ‰ï¼‰
- "å­¦å†": å­¦å†ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
- "æ—¶é—´èŒƒå›´": ææ–™æ¶‰åŠçš„æ—¶é—´èŒƒå›´ï¼ˆå¦‚æœ‰ï¼‰

**è¿”å›æ ¼å¼ç¤ºä¾‹**ï¼š
{{
  "å§“å": "å¼ ä¸‰",
  "å·¥ä½œå•ä½": "æŸæŸå¤§å­¦",
  "èº«ä»½è¯å·": "1234567890",
  "èŒåŠ¡": "å‰¯æ•™æˆ",
  "ä¸“ä¸š": "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯"
}}

---ææ–™å†…å®¹---
{material.content[:5000] if material.content else 'ææ–™å†…å®¹ä¸ºç©º'}"""
        
        # ä½¿ç”¨APIè½®è¯¢æœºåˆ¶
        def ai_call(client):
            return client.models.generate_content(
                model="gemini-2.5-flash", 
                contents=prompt
            )
        
        try:
            response = self._rotated_api_call(ai_call)
            match = re.search(r'{{.*}}', response.text, re.DOTALL)
            if match: 
                result = json.loads(match.group())
                # ç¡®ä¿å¿…é¡»å­—æ®µå­˜åœ¨
                if "å§“å" not in result:
                    result["å§“å"] = None
                if "å·¥ä½œå•ä½" not in result:
                    result["å·¥ä½œå•ä½"] = None
                return result
            return {"error": "æœªæå–åˆ°JSONæ ¼å¼ä¿¡æ¯"}
        except Exception as e:
            return {"error": f"æ ¸å¿ƒä¿¡æ¯æå–å¤±è´¥: {e}"}

    def _perform_cross_validation(self, core_infos: Dict[int, Dict]) -> str:
        """æ‰§è¡Œæ ¸å¿ƒä¿¡æ¯äº¤å‰æ£€éªŒ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒå·¥ä½œç»å†è¯¦ç»†æ£€éªŒ"""
        report_lines = []
        
        # 1. åŸºæœ¬ä¿¡æ¯ä¸€è‡´æ€§æ£€éªŒ
        report_lines.append("### ğŸ” åŸºæœ¬ä¿¡æ¯ä¸€è‡´æ€§æ£€éªŒ")
        report_lines.append("")
        
        # ä½¿ç”¨å­—å…¸æ¨å¯¼å¼å’Œå¹¶è¡Œå¤„ç†æ¥æé«˜æ•ˆç‡
        all_values = {
            k: [info.get(k) for _, info in core_infos.items() 
                if info and not info.get('error') and info.get(k)] 
            for k in ["å§“å", "å·¥ä½œå•ä½", "èº«ä»½è¯å·"]
        }
        
        for key, values in all_values.items():
            if not values:  # æ²¡æœ‰å‘ç°è¯¥å­—æ®µä¿¡æ¯
                report_lines.append(f"- â„¹ï¸ **{key}**: æœªåœ¨ææ–™ä¸­å‘ç°ç›¸å…³ä¿¡æ¯")
                continue
                
            # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
            unique_values = set(filter(lambda x: x and str(x).strip(), values))
            
            if len(unique_values) > 1: 
                # å‘ç°ä¸ä¸€è‡´
                if key == "å§“å":
                    priority = "ğŸ”´ [æé«˜ä¼˜å…ˆçº§]"
                elif key == "èº«ä»½è¯å·":
                    priority = "ğŸ”´ [æé«˜ä¼˜å…ˆçº§]"
                else:
                    priority = "ğŸŸ  [é«˜ä¼˜å…ˆçº§]"
                    
                report_lines.append(f"- {priority} **{key}ä¸ä¸€è‡´**: å‘ç° {len(unique_values)} ç§ä¸åŒå€¼ - {', '.join(map(str, unique_values))}")
            elif unique_values: 
                report_lines.append(f"- âœ… **{key}ä¸€è‡´**: {list(unique_values)[0]}")
        
        report_lines.append("")
        
        # 2. å·¥ä½œç»å†è¯¦ç»†æ£€éªŒï¼ˆå¦‚æœå­˜åœ¨å·¥ä½œç»å†æ•°æ®ï¼‰
        work_experience_data = core_infos.get(2)  # ææ–™ID=2ä¸ºå·¥ä½œç»å†
        if work_experience_data and not work_experience_data.get('error') and work_experience_data.get('å·¥ä½œç»å†è¯¦æƒ…'):
            report_lines.append("### ğŸ“‹ å·¥ä½œç»å†æ—¶é—´é€»è¾‘æ£€éªŒ")
            report_lines.append("")
            
            work_history = work_experience_data['å·¥ä½œç»å†è¯¦æƒ…']
            if isinstance(work_history, list) and len(work_history) > 0:
                # æ£€æŸ¥æ—¶é—´é‡å 
                time_overlap_result = self._check_time_overlap(work_history)
                report_lines.extend(time_overlap_result)
                
                # æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
                time_continuity_result = self._check_time_continuity(work_history)
                report_lines.extend(time_continuity_result)
                
                # æ£€æŸ¥å·¥ä½œåœ°ç‚¹å˜è¿
                location_change_result = self._check_location_changes(work_history)
                report_lines.extend(location_change_result)
            else:
                report_lines.append("- â„¹ï¸ **å·¥ä½œç»å†æ ¼å¼**: æ•°æ®æ ¼å¼å¼‚å¸¸æˆ–ä¸ºç©º")
            
            report_lines.append("")
        
        # 3. æ•°æ®å®Œæ•´æ€§æ£€éªŒ
        report_lines.append("### ğŸ“ˆ æ•°æ®å®Œæ•´æ€§æ£€éªŒ")
        report_lines.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯æå–æƒ…å†µ
        total_materials = len([info for info in core_infos.values() if info])
        successful_extractions = len([info for info in core_infos.values() 
                                    if info and not info.get('error')])
        failed_extractions = total_materials - successful_extractions
        
        if failed_extractions == 0:
            report_lines.append(f"- âœ… **ä¿¡æ¯æå–æˆåŠŸç‡**: 100% ({successful_extractions}/{total_materials})")
        else:
            report_lines.append(f"- âš ï¸ **ä¿¡æ¯æå–æˆåŠŸç‡**: {successful_extractions/total_materials*100:.1f}% ({successful_extractions}/{total_materials})")
            report_lines.append(f"- ğŸŸ¡ [ä¸­ä¼˜å…ˆçº§] **ä¿¡æ¯æå–å¤±è´¥**: {failed_extractions} ä¸ªææ–™çš„ä¿¡æ¯æå–å¤±è´¥")
        
        # è¿”å›ç»“æœ
        final_report = "\n".join(report_lines).strip()
        return final_report if final_report else "- âœ… æœªå‘ç°æ˜æ˜¾çš„ä¸ä¸€è‡´ä¹‹å¤„ã€‚\n"
    
    def _check_time_overlap(self, work_history: List[Dict]) -> List[str]:
        """æ£€æŸ¥å·¥ä½œæ—¶é—´æ˜¯å¦å­˜åœ¨é‡å """
        results = []
        
        # è§£æå¹¶æ’åºæ—¶é—´æ®µ
        time_periods = []
        for i, job in enumerate(work_history):
            start_time = self._parse_time_string(job.get('èµ·å§‹æ—¶é—´', ''))
            end_time = self._parse_time_string(job.get('ç»“æŸæ—¶é—´', ''))
            if start_time:
                time_periods.append({
                    'index': i,
                    'start': start_time,
                    'end': end_time,
                    'unit': job.get('å•ä½åç§°', 'æœªçŸ¥å•ä½'),
                    'job_title': job.get('èŒåŠ¡', '')
                })
        
        # æ£€æŸ¥é‡å 
        overlaps_found = False
        for i in range(len(time_periods)):
            for j in range(i + 1, len(time_periods)):
                period1, period2 = time_periods[i], time_periods[j]
                
                # åˆ¤æ–­æ˜¯å¦é‡å ï¼ˆå®¹å¿1ä¸ªæœˆçš„äº¤æ¥æœŸï¼‰
                if self._periods_overlap(period1, period2, tolerance_months=1):
                    overlaps_found = True
                    overlap_type = "ğŸŸ  [é«˜ä¼˜å…ˆçº§]"
                    results.append(f"- {overlap_type} **æ—¶é—´é‡å **: ã€Š{period1['unit']}ã€‹ä¸ã€Š{period2['unit']}ã€‹å­˜åœ¨æ—¶é—´é‡å ")
        
        if not overlaps_found:
            results.append("- âœ… **æ—¶é—´é‡å æ£€æŸ¥**: æœªå‘ç°æ—¶é—´é‡å é—®é¢˜")
        
        return results
    
    def _check_time_continuity(self, work_history: List[Dict]) -> List[str]:
        """æ£€æŸ¥å·¥ä½œæ—¶é—´çš„è¿ç»­æ€§"""
        results = []
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_jobs = []
        for job in work_history:
            start_time = self._parse_time_string(job.get('èµ·å§‹æ—¶é—´', ''))
            if start_time:
                sorted_jobs.append((start_time, job))
        
        sorted_jobs.sort(key=lambda x: x[0])
        
        # æ£€æŸ¥ç©ºç™½æœŸ
        gaps_found = False
        for i in range(len(sorted_jobs) - 1):
            current_job = sorted_jobs[i][1]
            next_job = sorted_jobs[i + 1][1]
            
            current_end = self._parse_time_string(current_job.get('ç»“æŸæ—¶é—´', ''))
            next_start = self._parse_time_string(next_job.get('èµ·å§‹æ—¶é—´', ''))
            
            if current_end and next_start:
                gap_months = self._calculate_month_gap(current_end, next_start)
                if gap_months > 6:  # è¶…è¿‡6ä¸ªæœˆçš„ç©ºç™½æœŸ
                    gaps_found = True
                    results.append(f"- ğŸŸ¡ [ä¸­ä¼˜å…ˆçº§] **æ—¶é—´ç©ºç™½**: ã€Š{current_job.get('å•ä½åç§°', '')}ã€‹ä¸ã€Š{next_job.get('å•ä½åç§°', '')}ã€‹ä¹‹é—´å­˜åœ¨{gap_months}ä¸ªæœˆç©ºç™½æœŸ")
                elif gap_months > 1:
                    results.append(f"- âš ï¸ **çŸ­æœŸç©ºç™½**: ã€Š{current_job.get('å•ä½åç§°', '')}ã€‹ä¸ã€Š{next_job.get('å•ä½åç§°', '')}ã€‹ä¹‹é—´å­˜åœ¨{gap_months}ä¸ªæœˆé—´éš”")
        
        if not gaps_found:
            results.append("- âœ… **æ—¶é—´è¿ç»­æ€§**: å·¥ä½œç»å†æ—¶é—´è¿ç»­æ€§è‰¯å¥½")
        
        return results
    
    def _check_location_changes(self, work_history: List[Dict]) -> List[str]:
        """æ£€æŸ¥å·¥ä½œåœ°ç‚¹å˜è¿åˆç†æ€§"""
        results = []
        
        locations = [job.get('å·¥ä½œåœ°ç‚¹', '') for job in work_history if job.get('å·¥ä½œåœ°ç‚¹')]
        unique_locations = list(set(filter(None, locations)))
        
        if len(unique_locations) <= 1:
            results.append("- âœ… **åœ°ç‚¹å˜è¿**: å·¥ä½œåœ°ç‚¹ç›¸å¯¹ç¨³å®š")
        elif len(unique_locations) <= 3:
            results.append(f"- â„¹ï¸ **åœ°ç‚¹å˜è¿**: å·¥ä½œåœ°ç‚¹åŒ…æ‹¬ {', '.join(unique_locations)}ï¼Œå±äºæ­£å¸¸èŒƒå›´")
        else:
            results.append(f"- ğŸŸ¢ [ä½ä¼˜å…ˆçº§] **åœ°ç‚¹å˜è¿é¢‘ç¹**: å·¥ä½œåœ°ç‚¹è¾ƒå¤š ({len(unique_locations)}ä¸ª)ï¼Œå»ºè®®æ ¸å®å˜è¿åŸå› ")
        
        return results
    
    def _parse_time_string(self, time_str: str) -> Optional[tuple]:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸º(year, month)å…ƒç»„"""
        if not time_str or time_str == 'è‡³ä»Š':
            return None
        
        import re
        # åŒ¹é…å„ç§æ—¶é—´æ ¼å¼
        patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ',  # 2020å¹´1æœˆ
            r'(\d{4})\.(\d{1,2})',        # 2020.01
            r'(\d{4})-(\d{1,2})',        # 2020-01
            r'(\d{4})/(\d{1,2})',        # 2020/01
        ]
        
        for pattern in patterns:
            match = re.search(pattern, time_str)
            if match:
                year, month = int(match.group(1)), int(match.group(2))
                return (year, month)
        
        return None
    
    def _periods_overlap(self, period1: Dict, period2: Dict, tolerance_months: int = 0) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ—¶é—´æ®µæ˜¯å¦é‡å """
        if not period1['start'] or not period2['start']:
            return False
        
        # å¦‚æœæŸä¸ªæ—¶é—´æ®µæ²¡æœ‰ç»“æŸæ—¶é—´ï¼Œè¡¨ç¤ºè‡³ä»Š
        end1 = period1['end'] if period1['end'] else (2024, 12)  # é»˜è®¤å½“å‰æ—¶é—´
        end2 = period2['end'] if period2['end'] else (2024, 12)
        
        # è½¬æ¢ä¸ºæœˆä»½æ•°è¿›è¡Œæ¯”è¾ƒ
        start1_months = period1['start'][0] * 12 + period1['start'][1]
        end1_months = end1[0] * 12 + end1[1]
        start2_months = period2['start'][0] * 12 + period2['start'][1]
        end2_months = end2[0] * 12 + end2[1]
        
        # è€ƒè™‘å®¹å¿åº¦
        return not (end1_months + tolerance_months < start2_months or end2_months + tolerance_months < start1_months)
    
    def _calculate_month_gap(self, end_time: tuple, start_time: tuple) -> int:
        """è®¡ç®—ä¸¤ä¸ªæ—¶é—´ç‚¹ä¹‹é—´çš„æœˆä»½å·®"""
        end_months = end_time[0] * 12 + end_time[1]
        start_months = start_time[0] * 12 + start_time[1]
        return start_months - end_months - 1  # å‡1æ˜¯å› ä¸ºç›¸é‚»æœˆä»½é—´éš”ä¸º0

    def _assemble_report(self, results: Dict, cross_report: str) -> str:
        """ç»„è£…æ¨¡æ¿åŒ–æŠ¥å‘Šï¼ˆä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿æ ¼å¼ï¼Œä¸å…è®¸AIè‡ªç”±å‘æŒ¥ï¼‰"""
        empty_count = len(self.validation_results["empty_materials"])
        
        # è§£æå’Œåˆ†ç±»è¿è§„é—®é¢˜
        self._log(f"ğŸ” å¼€å§‹è§£æè¿è§„é—®é¢˜å¹¶ç”Ÿæˆæ¨¡æ¿åŒ–æŠ¥å‘Š...")
        violations_by_priority = self._parse_violations_from_results(results)
        
        # ç»Ÿè®¡æ•°æ®
        total_materials = 17
        valid_materials = total_materials - empty_count
        total_issues = sum(len(v) for v in violations_by_priority.values())
        
        # ğŸ“Š æ¨¡æ¿åŒ–æŠ¥å‘Šå¼€å§‹
        report = self._generate_templated_report(
            valid_materials=valid_materials,
            total_materials=total_materials,
            total_rules=len(self.rules),
            violations_by_priority=violations_by_priority,
            total_issues=total_issues,
            empty_materials=self.validation_results["empty_materials"],
            cross_validation_report=cross_report
        )
        
        return report
    
    def _generate_templated_report(self, valid_materials: int, total_materials: int, 
                                  total_rules: int, violations_by_priority: Dict, 
                                  total_issues: int, empty_materials: List, 
                                  cross_validation_report: str) -> str:
        """ç”Ÿæˆä¸¥æ ¼æ¨¡æ¿åŒ–çš„æŠ¥å‘Šï¼ˆä¸å…è®¸AIè‡ªç”±å‘æŒ¥ï¼‰"""
        
        # ğŸ“… 1. ææ–™æ¦‚è§ˆï¼ˆä¸¥æ ¼æ¨¡æ¿ï¼‰
        overview_section = self._generate_material_overview_template(
            valid_materials, total_materials, total_rules
        )
        
        # âš ï¸ 2. é—®é¢˜æ‘˜è¦ï¼ˆä¸¥æ ¼æ¨¡æ¿ï¼‰
        summary_section = self._generate_problem_summary_template(
            violations_by_priority, total_issues
        )
        
        # ğŸš¨ 3. è¯¦ç»†é—®é¢˜åˆ—è¡¨ï¼ˆä¸¥æ ¼æ¨¡æ¿ï¼‰
        details_section = self._generate_problem_details_template(
            violations_by_priority, total_issues
        )
        
        # ğŸ”„ 4. æ ¸å¿ƒä¿¡æ¯äº¤å‰æ£€éªŒï¼ˆä¸¥æ ¼æ¨¡æ¿ï¼‰
        cross_validation_section = self._generate_cross_validation_template(
            cross_validation_report
        )
        
        # ç¼ºå¤±ææ–™éƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
        missing_section = ""
        if empty_materials:
            missing_section = self._generate_missing_materials_template(empty_materials)
        
        # ç»„è£…æœ€ç»ˆæŠ¥å‘Š
        final_report = f"""
# ğŸ“‹ èŒç§°è¯„å®¡ææ–™äº¤å‰æ£€éªŒæŠ¥å‘Š

{overview_section}

{summary_section}

{details_section}

{cross_validation_section}
{missing_section}
""".strip()
        
        return final_report
    
    def _generate_material_overview_template(self, valid_materials: int, 
                                            total_materials: int, total_rules: int) -> str:
        """ç”Ÿæˆææ–™æ¦‚è§ˆæ¨¡æ¿ï¼ˆä¸¥æ ¼æ ¼å¼ï¼‰"""
        from datetime import datetime
        
        return f"""
## 1. ğŸ“ˆ ææ–™æ¦‚è§ˆ

| é¡¹ç›® | æ•°é‡ | çŠ¶æ€ |
|------|------|------|
| æ€»ææ–™æ•° | {total_materials} é¡¹ | æ ‡å‡† |
| æœ‰æ•ˆææ–™ | {valid_materials} é¡¹ | {'âœ… å……è¶³' if valid_materials >= 15 else 'âš ï¸ ä¸è¶³'} |
| é€‚ç”¨è§„åˆ™ | {total_rules} æ¡ | âœ… å·²åŠ è½½ |
| æ£€éªŒæ—¥æœŸ | {datetime.now().strftime('%Y-%m-%d %H:%M')} | âœ… å®Œæˆ |

**ææ–™å®Œæ•´æ€§è¯„ä¼°**: {'âœ… ä¼˜ç§€' if valid_materials >= 16 else 'âš ï¸ è‰¯å¥½' if valid_materials >= 14 else 'âŒ ä¸åˆæ ¼'}
""".strip()
    
    def _generate_problem_summary_template(self, violations_by_priority: Dict, 
                                          total_issues: int) -> str:
        """ç”Ÿæˆé—®é¢˜æ‘˜è¦æ¨¡æ¿ï¼ˆä¸¥æ ¼æ ¼å¼ï¼‰"""
        
        if total_issues == 0:
            return """
## 2. âœ… é—®é¢˜æ‘˜è¦

**æ£€éªŒç»“æœ**: æ‰€æœ‰ææ–™å‡ç¬¦åˆè¦æ±‚ï¼Œæœªå‘ç°ä»»ä½•é—®é¢˜ã€‚

| ä¼˜å…ˆçº§ | é—®é¢˜æ•°é‡ | çŠ¶æ€ |
|----------|----------|------|
| ğŸ”´ æé«˜ä¼˜å…ˆçº§ | 0 ä¸ª | âœ… é€šè¿‡ |
| ğŸŸ  é«˜ä¼˜å…ˆçº§ | 0 ä¸ª | âœ… é€šè¿‡ |
| ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ | 0 ä¸ª | âœ… é€šè¿‡ |
| ğŸŸ¢ ä½ä¼˜å…ˆçº§ | 0 ä¸ª | âœ… é€šè¿‡ |

**ç»¼åˆè¯„ä»·**: ğŸ† ä¼˜ç§€ï¼Œæ‰€æœ‰ææ–™å‡ç¬¦åˆå®¡æ ¸æ ‡å‡†ã€‚
""".strip()
        
        # ç»Ÿè®¡å„ä¼˜å…ˆçº§é—®é¢˜æ•°é‡
        extreme_count = len(violations_by_priority.get("æé«˜", []))
        high_count = len(violations_by_priority.get("é«˜", []))
        medium_count = len(violations_by_priority.get("ä¸­", []))
        low_count = len(violations_by_priority.get("ä½", []))
        
        # åˆ¤æ–­é—®é¢˜ä¸¥é‡ç¨‹åº¦
        if extreme_count > 0:
            severity = "âŒ ä¸¥é‡"
            recommendation = "å¿…é¡»ç«‹å³å¤„ç†æ‰€æœ‰æé«˜ä¼˜å…ˆçº§é—®é¢˜ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´ææ–™è¢«ç›´æ¥æ‹’ç»ã€‚"
        elif high_count > 0:
            severity = "âš ï¸ è¾ƒé‡"
            recommendation = "å»ºè®®ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§é—®é¢˜ï¼Œæå‡ææ–™è´¨é‡ã€‚"
        elif medium_count > 0:
            severity = "ğŸŸ¡ ä¸€èˆ¬"
            recommendation = "å»ºè®®å¤„ç†ä¸­ä¼˜å…ˆçº§é—®é¢˜ï¼Œè¿›ä¸€æ­¥æå‡ææ–™è´¨é‡ã€‚"
        else:
            severity = "ğŸŸ¢ è½»å¾®"
            recommendation = "ä»…æœ‰ä½ä¼˜å…ˆçº§é—®é¢˜ï¼Œå¯é€‰æ‹©æ€§å¤„ç†ã€‚"
        
        return f"""
## 2. âš ï¸ é—®é¢˜æ‘˜è¦

**æ£€éªŒç»“æœ**: å‘ç° **{total_issues}** ä¸ªé—®é¢˜ï¼Œé—®é¢˜ä¸¥é‡ç¨‹åº¦: {severity}

| ä¼˜å…ˆçº§ | é—®é¢˜æ•°é‡ | çŠ¶æ€ |
|----------|----------|------|
| ğŸ”´ æé«˜ä¼˜å…ˆçº§ | {extreme_count} ä¸ª | {'âŒ ä¸¥é‡' if extreme_count > 0 else 'âœ… é€šè¿‡'} |
| ğŸŸ  é«˜ä¼˜å…ˆçº§ | {high_count} ä¸ª | {'âš ï¸ è­¦å‘Š' if high_count > 0 else 'âœ… é€šè¿‡'} |
| ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ | {medium_count} ä¸ª | {'ğŸ”¸ æ³¨æ„' if medium_count > 0 else 'âœ… é€šè¿‡'} |
| ğŸŸ¢ ä½ä¼˜å…ˆçº§ | {low_count} ä¸ª | {'ğŸ”¹ å¿½ç•¥' if low_count > 0 else 'âœ… é€šè¿‡'} |

**å¤„ç†å»ºè®®**: {recommendation}
""".strip()
    
    def _generate_problem_details_template(self, violations_by_priority: Dict, 
                                          total_issues: int) -> str:
        """ç”Ÿæˆè¯¦ç»†é—®é¢˜åˆ—è¡¨æ¨¡æ¿ï¼ˆä¸¥æ ¼æ ¼å¼ï¼‰"""
        
        if total_issues == 0:
            return """
## 3. âœ… è¯¦ç»†é—®é¢˜åˆ—è¡¨

**æ£€éªŒç»“æœ**: æœªå‘ç°ä»»ä½•é—®é¢˜ï¼Œæ‰€æœ‰ææ–™å‡ç¬¦åˆè¦æ±‚ã€‚

### ğŸ† æ­£å¸¸çŠ¶æ€
- âœ… æ‰€æœ‰ä¸“é¡¹è§„åˆ™æ£€æŸ¥é€šè¿‡
- âœ… æ‰€æœ‰é€šç”¨è§„åˆ™æ£€æŸ¥é€šè¿‡
- âœ… æ‰€æœ‰äº¤å‰éªŒè¯æ£€æŸ¥é€šè¿‡
- âœ… ææ–™å†…å®¹å®Œæ•´æ€§è‰¯å¥½

**ç»“è®º**: æ‰€æœ‰ææ–™å‡ç¬¦åˆèŒç§°è¯„å®¡è¦æ±‚ï¼Œå»ºè®®é€šè¿‡ã€‚
""".strip()
        
        details = []
        details.append("## 3. ğŸš¨ è¯¦ç»†é—®é¢˜åˆ—è¡¨")
        details.append("")
        details.append(f"**é—®é¢˜æ€»æ•°**: {total_issues} ä¸ªï¼ŒæŒ‰ä¼˜å…ˆçº§åˆ†ç±»å¦‚ä¸‹ï¼š")
        details.append("")
        
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ˜¾ç¤ºé—®é¢˜
        priority_configs = [
            ("æé«˜", "ğŸ”´", "å¿…é¡»ç«‹å³å¤„ç†ï¼Œå¯èƒ½å¯¼è‡´ææ–™è¢«ç›´æ¥æ‹’ç»"),
            ("é«˜", "ğŸŸ ", "éœ€è¦é‡ç‚¹å…³æ³¨å’Œå¤„ç†"),
            ("ä¸­", "ğŸŸ¡", "å»ºè®®å¤„ç†ä»¥æå‡ææ–™è´¨é‡"),
            ("ä½", "ğŸŸ¢", "å¯é€‰æ‹©æ€§å¤„ç†")
        ]
        
        for priority, icon, description in priority_configs:
            issues = violations_by_priority.get(priority, [])
            if issues:
                details.append(f"### {icon} {priority}ä¼˜å…ˆçº§é—®é¢˜ ({len(issues)}ä¸ª)")
                details.append(f"> {description}")
                details.append("")
                
                # æŒ‰ææ–™åˆ†ç»„æ˜¾ç¤ºé—®é¢˜
                materials_with_issues = {}
                for violation in issues:
                    material_name = violation['material_name']
                    if material_name not in materials_with_issues:
                        materials_with_issues[material_name] = []
                    materials_with_issues[material_name].append(violation)
                
                for material_name, material_violations in materials_with_issues.items():
                    details.append(f"#### ğŸ“„ ã€Š{material_name}ã€‹")
                    for i, violation in enumerate(material_violations, 1):
                        formatted_issue = self._format_violation_description(violation)
                        details.append(f"**é—®é¢˜ {i}**: {formatted_issue}")
                        details.append("")
                
                details.append("---")
                details.append("")
        
        return "\n".join(details).strip()
    
    def _generate_cross_validation_template(self, cross_validation_report: str) -> str:
        """ç”Ÿæˆäº¤å‰æ£€éªŒæ¨¡æ¿ï¼ˆä¸¥æ ¼æ ¼å¼ï¼‰"""
        
        return f"""
## 4. ğŸ”„ æ ¸å¿ƒä¿¡æ¯äº¤å‰æ£€éªŒ

**æ£€éªŒèŒƒå›´**: å§“åã€å·¥ä½œå•ä½ã€èº«ä»½è¯å·ç­‰å…³é”®ä¿¡æ¯åœ¨ä¸åŒææ–™ä¸­çš„ä¸€è‡´æ€§

**æ£€éªŒæ–¹æ³•**: è‡ªåŠ¨æå–å„ææ–™ä¸­çš„æ ¸å¿ƒä¿¡æ¯ï¼Œè¿›è¡Œäº¤å‰å¯¹æ¯”åˆ†æ

### æ£€éªŒç»“æœ

{cross_validation_report}

**ç»“è®º**: {'âœ… äº¤å‰æ£€éªŒé€šè¿‡' if 'ä¸ä¸€è‡´' not in cross_validation_report else 'âš ï¸ å­˜åœ¨ä¸ä¸€è‡´é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥æ ¸å®'}
""".strip()
    
    def _generate_missing_materials_template(self, empty_materials: List) -> str:
        """ç”Ÿæˆç¼ºå¤±ææ–™æ¨¡æ¿ï¼ˆä¸¥æ ¼æ ¼å¼ï¼‰"""
        
        missing_list = []
        for mid in empty_materials:
            material_name = self.MATERIAL_NAMES.get(mid, f"æœªçŸ¥ææ–™{mid}")
            missing_list.append(f"- **{mid}.** {material_name}")
        
        return f"""

## âŒ ç¼ºå¤±ææ–™è¯¦æƒ…

**ç¼ºå¤±æ•°é‡**: {len(empty_materials)} é¡¹

**å¤„ç†å»ºè®®**: è¯·å°½å¿«è¡¥å……ä»¥ä¸‹ç¼ºå¤±ææ–™ï¼Œç¡®ä¿ç”³æŠ¥ææ–™çš„å®Œæ•´æ€§

### ç¼ºå¤±æ¸…å•

{chr(10).join(missing_list)}

**æ³¨æ„äº‹é¡¹**: ç¼ºå¤±çš„ææ–™å¯èƒ½å½±å“æ•´ä½“è¯„å®¡ç»“æœï¼Œå»ºè®®ä¼˜å…ˆè¡¥å……ã€‚
""".strip()
    
    def _parse_violations_from_results(self, results: Dict) -> Dict[str, List[Dict]]:
        """ä»AIç»“æœä¸­è§£æè¿è§„é—®é¢˜ï¼ŒæŒ‰ä¼˜å…ˆçº§åˆ†ç±»ã€‚å¢å¼ºç‰ˆã€‚"""
        violations_by_priority = {
            "æé«˜": [],
            "é«˜": [],
            "ä¸­": [],
            "ä½": []
        }
        
        total_violations = 0
        self._log(f"  ğŸ” å¼€å§‹è§£ææ‰€æœ‰ææ–™çš„è¿è§„é—®é¢˜...")
        
        for mid, result in results.items():
            material = self.materials[mid]
            if material.is_empty:
                continue
                
            # è§£æAIç»“æœ
            violations = self._extract_violations_from_text(result, material.name)
            material_violation_count = len(violations)
            total_violations += material_violation_count
            
            if material_violation_count > 0:
                self._log(f"    ğŸ“ ã€Š{material.name}ã€‹: å‘ç° {material_violation_count} ä¸ªè¿è§„é—®é¢˜")
            
            # æŒ‰ä¼˜å…ˆçº§åˆ†ç±»
            for violation in violations:
                priority = violation.get('priority', 'ä¸­')
                if priority in violations_by_priority:
                    violations_by_priority[priority].append(violation)
                    self._log(f"      â†’ è®°å½•{priority}ä¼˜å…ˆçº§é—®é¢˜: {violation.get('rule_title', 'æœªçŸ¥è§„åˆ™')}")
                else:
                    # å¦‚æœä¼˜å…ˆçº§ä¸åœ¨é¢„æœŸåˆ—è¡¨ä¸­ï¼Œé»˜è®¤å½’ç±»ä¸ºä¸­ä¼˜å…ˆçº§
                    violations_by_priority["ä¸­"].append(violation)
                    self._log(f"      âš ï¸ æœªçŸ¥ä¼˜å…ˆçº§'{priority}'ï¼Œå½’ç±»ä¸ºä¸­ä¼˜å…ˆçº§: {violation.get('rule_title', 'æœªçŸ¥è§„åˆ™')}")
        
        # ç»Ÿè®¡æ€»ä½“æƒ…å†µ
        priority_counts = {p: len(v) for p, v in violations_by_priority.items() if len(v) > 0}
        if priority_counts:
            stats_summary = ", ".join([f"{p}: {c}ä¸ª" for p, c in priority_counts.items()])
            self._log(f"  ğŸ“Š è¿è§„é—®é¢˜ç»Ÿè®¡ - æ€»è®¡: {total_violations}ä¸ªï¼Œåˆ†å¸ƒ: {stats_summary}")
        else:
            self._log(f"  âœ… æœªå‘ç°ä»»ä½•è¿è§„é—®é¢˜")
        
        return violations_by_priority
    
    def _extract_violations_from_text(self, text: str, material_name: str) -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­æå–è¿è§„é—®é¢˜ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šç§AIè¾“å‡ºæ ¼å¼ï¼‰"""
        violations = []
        lines = text.split('\n')
        
        current_rule_info = {
            'rule_number': '',
            'rule_title': '',
            'priority': 'ä¸­',
            'judgment': '',
            'reason': ''
        }
        
        # è®°å½•è°ƒè¯•ä¿¡æ¯
        self._log(f"    ğŸ” å¼€å§‹è§£æã€Š{material_name}ã€‹çš„AIéªŒè¯ç»“æœ...")
        
        for line_idx, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # åŒ¹é…å¤šç§è§„åˆ™æ ‡é¢˜æ ¼å¼
            # æ ¼å¼1: "è§„åˆ™1: æ—¶é—´é€»è¾‘ä¸€è‡´æ€§"
            rule_match = re.search(r'è§„åˆ™(\d+)[:ï¼š]\s*(.+)', line)
            if rule_match:
                current_rule_info['rule_number'] = rule_match.group(1)
                current_rule_info['rule_title'] = rule_match.group(2).strip()
                # ä»æ ‡é¢˜ä¸­æå–ä¼˜å…ˆçº§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                priority_in_title = re.search(r'ã€(æé«˜|é«˜|ä¸­|ä½)ã€‘', current_rule_info['rule_title'])
                if priority_in_title:
                    current_rule_info['priority'] = priority_in_title.group(1)
                    current_rule_info['rule_title'] = re.sub(r'ã€[^ã€‘]*ã€‘', '', current_rule_info['rule_title']).strip()
                self._log(f"      ğŸ“‹ è§£æè§„åˆ™{current_rule_info['rule_number']}: {current_rule_info['rule_title']} [ä¼˜å…ˆçº§: {current_rule_info['priority']}]")
                continue
            
            # æ ¼å¼2: "1. ã€é«˜ã€‘æ—¶é—´é€»è¾‘ä¸€è‡´æ€§" æˆ– "1. ã€é«˜ã€‘æ—¶é—´é€»è¾‘ä¸€è‡´æ€§: xxx"
            priority_rule_match = re.search(r'(\d+)\.\s*ã€(æé«˜|é«˜|ä¸­|ä½)ã€‘(.+)', line)
            if priority_rule_match:
                current_rule_info['rule_number'] = priority_rule_match.group(1)
                current_rule_info['priority'] = priority_rule_match.group(2)
                title_part = priority_rule_match.group(3).strip()
                # å¦‚æœæ ‡é¢˜åŒ…å«å†’å·ï¼Œå–å†’å·å‰çš„éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
                if ':' in title_part or 'ï¼š' in title_part:
                    current_rule_info['rule_title'] = re.split(r'[:ï¼š]', title_part)[0].strip()
                else:
                    current_rule_info['rule_title'] = title_part
                self._log(f"      ğŸ“‹ è§£æè§„åˆ™{current_rule_info['rule_number']}: {current_rule_info['rule_title']} [ä¼˜å…ˆçº§: {current_rule_info['priority']}]")
                continue
            
            # æ ¼å¼3: ç›´æ¥çš„ä¼˜å…ˆçº§æ ‡è®°è¡Œ "ã€æé«˜ã€‘" æˆ– "ä¼˜å…ˆçº§: é«˜"
            priority_only_match = re.search(r'ã€(æé«˜|é«˜|ä¸­|ä½)ã€‘|ä¼˜å…ˆçº§[:ï¼š]\s*(æé«˜|é«˜|ä¸­|ä½)', line)
            if priority_only_match:
                priority_found = priority_only_match.group(1) or priority_only_match.group(2)
                if priority_found:
                    current_rule_info['priority'] = priority_found
                    self._log(f"      ğŸ¯ æ›´æ–°ä¼˜å…ˆçº§ä¸º: {current_rule_info['priority']}")
                continue
            
            # åŒ¹é…åˆ¤æ–­ç»“æœ
            judgment_match = re.search(r'åˆ¤æ–­[:ï¼š]\s*(âŒè¿å|âœ…ç¬¦åˆ|è¿å|ç¬¦åˆ|ä¸ç¬¦åˆ)', line)
            if judgment_match:
                current_rule_info['judgment'] = judgment_match.group(1)
                self._log(f"      âš–ï¸ åˆ¤æ–­ç»“æœ: {current_rule_info['judgment']}")
                continue
            
            # åŒ¹é…ç†ç”±
            reason_match = re.search(r'ç†ç”±[:ï¼š]\s*(.+)', line)
            if reason_match:
                current_rule_info['reason'] = reason_match.group(1).strip()
                self._log(f"      ğŸ“ ç†ç”±: {current_rule_info['reason'][:50]}...")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºè¿åæƒ…å†µ
                is_violation = (
                    'è¿å' in current_rule_info.get('judgment', '') or 
                    'âŒ' in current_rule_info.get('judgment', '') or
                    'ä¸ç¬¦åˆ' in current_rule_info.get('judgment', '')
                )
                
                if is_violation:
                    # ç¡®ä¿æœ‰æœ‰æ•ˆçš„è§„åˆ™ä¿¡æ¯å’Œç†ç”±
                    skip_keywords = ['ææ–™ä¿¡æ¯ä¸è¶³', 'æ— æ³•åˆ¤æ–­', 'ç¬¦åˆè¦æ±‚', 'æ— æ˜æ˜¾é—®é¢˜', 'æš‚æ— å‘ç°']
                    if current_rule_info['reason'] and not any(skip_word in current_rule_info['reason'] for skip_word in skip_keywords):
                        violation = {
                            'material_name': material_name,
                            'priority': current_rule_info.get('priority', 'ä¸­'),
                            'rule_title': current_rule_info.get('rule_title', 'è§„åˆ™æ£€æŸ¥'),
                            'rule_number': current_rule_info.get('rule_number', ''),
                            'problem_description': f"è¿åè§„åˆ™{current_rule_info.get('rule_number', '')}: {current_rule_info.get('rule_title', '')}",
                            'reason': current_rule_info['reason'],
                            'suggestion': ''  # å¯ä»¥åç»­ä»AIè¾“å‡ºä¸­æå–å»ºè®®
                        }
                        violations.append(violation)
                        self._log(f"      ğŸš¨ è®°å½•{current_rule_info['priority']}ä¼˜å…ˆçº§è¿è§„: {current_rule_info['rule_title']}")
                    else:
                        self._log(f"      â„¹ï¸ è·³è¿‡æ— æ•ˆè¿è§„è®°å½•: {current_rule_info['reason'][:30]}...")
                
                # é‡ç½®å½“å‰è§„åˆ™ä¿¡æ¯ï¼Œå‡†å¤‡å¤„ç†ä¸‹ä¸€ä¸ªè§„åˆ™
                current_rule_info = {
                    'rule_number': '',
                    'rule_title': '',
                    'priority': 'ä¸­',
                    'judgment': '',
                    'reason': ''
                }
                continue
        
        self._log(f"    âœ… ã€Š{material_name}ã€‹è§£æå®Œæˆï¼Œå‘ç° {len(violations)} ä¸ªè¿è§„é—®é¢˜")
        
        # æŒ‰ä¼˜å…ˆçº§ç»Ÿè®¡
        priority_stats = {}
        for violation in violations:
            priority = violation['priority']
            priority_stats[priority] = priority_stats.get(priority, 0) + 1
        
        if priority_stats:
            stats_str = ', '.join([f"{p}: {c}ä¸ª" for p, c in priority_stats.items()])
            self._log(f"    ğŸ“Š ä¼˜å…ˆçº§åˆ†å¸ƒ: {stats_str}")
        
        return violations
    
    def _format_violation_description(self, violation: Dict[str, str]) -> str:
        """æ ¼å¼åŒ–è¿è§„é—®é¢˜æè¿°"""
        formatted_parts = []
        
        # è§„åˆ™ä¿¡æ¯ - æ¸…æ™°æ˜¾ç¤º
        rule_title = violation.get('rule_title', '').strip()
        if rule_title and rule_title != 'è§„åˆ™æ£€æŸ¥':
            formatted_parts.append(f"ğŸ“‹ **è§„åˆ™**: {rule_title}")
        
        # é—®é¢˜æè¿° - ç®€æ´æ˜äº†
        reason = violation.get('reason', '').strip()
        if reason:
            # æ¸…ç†ç†ç”±æ–‡æœ¬ï¼Œç§»é™¤å¤šä½™çš„ç¬¦å·å’Œç©ºæ ¼
            clean_reason = re.sub(r'^[*\-\s]+', '', reason)  # ç§»é™¤å¼€å¤´çš„ç¬¦å·
            clean_reason = re.sub(r'\s+', ' ', clean_reason)  # è§„èŒƒåŒ–ç©ºæ ¼
            if clean_reason:
                formatted_parts.append(f"âš ï¸ **é—®é¢˜**: {clean_reason}")
        
        # å¤„ç†å»ºè®®
        suggestion = violation.get('suggestion', '').strip()
        if suggestion:
            clean_suggestion = re.sub(r'^[*\-\s]+', '', suggestion)
            clean_suggestion = re.sub(r'\s+', ' ', clean_suggestion)
            if clean_suggestion:
                formatted_parts.append(f"ğŸ’¡ **å»ºè®®**: {clean_suggestion}")
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå†…å®¹ï¼Œè¿”å›é€šç”¨æè¿°
        if not formatted_parts:
            return "âš ï¸ å‘ç°é—®é¢˜ï¼Œä½†å…·ä½“ä¿¡æ¯ä¸å®Œæ•´"
        
        return '\n   '.join(formatted_parts)


