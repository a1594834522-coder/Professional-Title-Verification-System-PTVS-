#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„ç¼“å­˜ç®¡ç†å™¨
æ”¯æŒæŒä¹…åŒ–ç¼“å­˜ã€è¿‡æœŸæœºåˆ¶å’ŒåŸºäºMD5å“ˆå¸Œçš„æ™ºèƒ½ç¼“å­˜é”®
"""

import os
import json
import hashlib
import time
import pickle
import tempfile
import threading
from typing import Dict, Optional, Any, Union, Callable
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®æ•°æ®ç»“æ„"""
    content: str
    file_hash: str
    file_size: int
    file_mtime: float
    cache_time: float
    access_count: int = 0
    last_access: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CacheEntry':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(**data)
    
    def is_expired(self, max_age_hours: int = 24) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¿‡æœŸ"""
        return time.time() - self.cache_time > max_age_hours * 3600
    
    def is_file_changed(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²æ›´æ”¹"""
        try:
            stat = os.stat(file_path)
            return (stat.st_size != self.file_size or 
                   stat.st_mtime != self.file_mtime)
        except (OSError, FileNotFoundError):
            return True

class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, 
                 cache_dir: Optional[str] = None,
                 max_age_hours: int = 24,
                 max_memory_items: int = 100,
                 max_disk_size_mb: int = 1000,
                 enable_redis: bool = False,
                 redis_url: Optional[str] = None,
                 progress_callback: Optional[Callable] = None):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºç³»ç»Ÿä¸´æ—¶ç›®å½•
            max_age_hours: ç¼“å­˜æœ€å¤§ç”Ÿå­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            max_memory_items: å†…å­˜ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
            max_disk_size_mb: ç£ç›˜ç¼“å­˜æœ€å¤§å¤§å°ï¼ˆMBï¼‰
            enable_redis: æ˜¯å¦å¯ç”¨Redisç¼“å­˜
            redis_url: Redisè¿æ¥URL
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        """
        self.max_age_hours = max_age_hours
        self.max_memory_items = max_memory_items
        self.max_disk_size_mb = max_disk_size_mb
        self.enable_redis = enable_redis
        self.progress_callback = progress_callback or (lambda msg: None)
        self.lock = threading.RLock()
        
        # å†…å­˜ç¼“å­˜
        self.memory_cache: Dict[str, CacheEntry] = {}
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        if cache_dir:
            self.cache_dir = Path(cache_dir)
        else:
            self.cache_dir = Path(tempfile.gettempdir()) / "pdf_content_cache"
        
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            'hits': 0,
            'misses': 0,
            'disk_hits': 0,
            'memory_hits': 0,
            'redis_hits': 0,
            'files_cached': 0,
            'cache_size_mb': 0.0
        }
        
        # Redisé…ç½®
        self.redis_client = None
        if enable_redis:
            self._init_redis(redis_url)
        
        # å¯åŠ¨æ—¶æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_expired_cache()
        
        self._log("ğŸ’¾ æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        self._log(f"   ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        self._log(f"   â° ç¼“å­˜æœ‰æ•ˆæœŸ: {max_age_hours} å°æ—¶")
        self._log(f"   ğŸ§  å†…å­˜ç¼“å­˜é™åˆ¶: {max_memory_items} ä¸ªæ¡ç›®")
        self._log(f"   ğŸ’½ ç£ç›˜ç¼“å­˜é™åˆ¶: {max_disk_size_mb} MB")
        if self.redis_client:
            self._log(f"   ğŸ”´ Redisç¼“å­˜: å·²å¯ç”¨")
    
    def _log(self, message: str):
        """è®°å½•æ—¥å¿—"""
        if self.progress_callback:
            self.progress_callback(message)
    
    def _init_redis(self, redis_url: Optional[str]):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            try:
                import redis
            except ImportError:
                self._log("âš ï¸ RedisåŒ…æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ–‡ä»¶ç¼“å­˜")
                self.redis_client = None
                self.enable_redis = False
                return
                
            if redis_url:
                self.redis_client = redis.from_url(redis_url)
            else:
                # å°è¯•è¿æ¥é»˜è®¤çš„æœ¬åœ°Redis
                self.redis_client = redis.Redis(
                    host='localhost', 
                    port=6379, 
                    db=0,
                    decode_responses=False,  # ä¿æŒäºŒè¿›åˆ¶æ¨¡å¼ç”¨äºpickle
                    socket_timeout=2
                )
            
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            self._log("ğŸ”´ Redisç¼“å­˜è¿æ¥æˆåŠŸ")
            
        except Exception as e:
            self._log(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æ–‡ä»¶ç¼“å­˜: {e}")
            self.redis_client = None
            self.enable_redis = False
    
    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        
        try:
            # è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
            stat = os.stat(file_path)
            file_info = f"{file_path}_{stat.st_size}_{stat.st_mtime}"
            
            # å¦‚æœæ–‡ä»¶å¾ˆå¤§ï¼Œåªè®¡ç®—å‰1MBå’Œå1MBçš„å“ˆå¸Œ
            if stat.st_size > 10 * 1024 * 1024:  # 10MB
                with open(file_path, 'rb') as f:
                    # è¯»å–å‰1MB
                    chunk = f.read(1024 * 1024)
                    hash_md5.update(chunk)
                    
                    # å¦‚æœæ–‡ä»¶å¤§äº2MBï¼Œè·³åˆ°æœ«å°¾è¯»å–æœ€å1MB
                    if stat.st_size > 2 * 1024 * 1024:
                        f.seek(-1024 * 1024, 2)
                        chunk = f.read(1024 * 1024)
                        hash_md5.update(chunk)
                        
                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                    hash_md5.update(file_info.encode())
            else:
                # å°æ–‡ä»¶ç›´æ¥è®¡ç®—å®Œæ•´å“ˆå¸Œ
                with open(file_path, 'rb') as f:
                    for chunk in iter(lambda: f.read(8192), b""):
                        hash_md5.update(chunk)
                        
            return hash_md5.hexdigest()
            
        except Exception as e:
            # å¦‚æœæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶ä¿¡æ¯ä½œä¸ºå“ˆå¸Œ
            self._log(f"âš ï¸ æ— æ³•è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼Œä½¿ç”¨æ–‡ä»¶ä¿¡æ¯: {e}")
            try:
                stat = os.stat(file_path)
                file_info = f"{file_path}_{stat.st_size}_{stat.st_mtime}"
                hash_md5.update(file_info.encode())
                return hash_md5.hexdigest()
            except Exception:
                # å¦‚æœè¿æ–‡ä»¶ä¿¡æ¯éƒ½è·å–ä¸åˆ°ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾„
                hash_md5.update(file_path.encode())
                return hash_md5.hexdigest()
    
    def _get_cache_key(self, file_path: str, file_hash: str, prefix: str = "") -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if prefix:
            return f"{prefix}_{file_hash}"
        return file_hash
    
    def _get_cache_file_path(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨å‰ä¸¤ä¸ªå­—ç¬¦ä½œä¸ºå­ç›®å½•ï¼Œé¿å…å•ä¸ªç›®å½•æ–‡ä»¶è¿‡å¤š
        subdir = cache_key[:2]
        cache_subdir = self.cache_dir / subdir
        cache_subdir.mkdir(exist_ok=True)
        return cache_subdir / f"{cache_key}.cache"
    
    def get(self, file_path: str, prefix: str = "") -> Optional[str]:
        """
        è·å–ç¼“å­˜å†…å®¹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            prefix: ç¼“å­˜é”®å‰ç¼€ï¼ˆç”¨äºåŒºåˆ†ä¸åŒç±»å‹çš„ç¼“å­˜ï¼‰
            
        Returns:
            ç¼“å­˜çš„å†…å®¹ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        with self.lock:
            try:
                # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                file_hash = self._get_file_hash(file_path)
                cache_key = self._get_cache_key(file_path, file_hash, prefix)
                
                # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
                if cache_key in self.memory_cache:
                    entry = self.memory_cache[cache_key]
                    
                    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸæˆ–æ–‡ä»¶å·²æ›´æ”¹
                    if entry.is_expired(self.max_age_hours) or entry.is_file_changed(file_path):
                        del self.memory_cache[cache_key]
                    else:
                        entry.access_count += 1
                        entry.last_access = time.time()
                        self.stats['hits'] += 1
                        self.stats['memory_hits'] += 1
                        return entry.content
                
                # 2. æ£€æŸ¥Redisç¼“å­˜
                if self.redis_client:
                    try:
                        cached_data = self.redis_client.get(f"pdf_cache:{cache_key}")
                        if cached_data:
                            # ç¡®ä¿æ•°æ®æ˜¯bytesç±»å‹
                            if isinstance(cached_data, str):
                                cached_data = cached_data.encode('utf-8')
                            
                            if isinstance(cached_data, bytes):
                                entry_dict = pickle.loads(cached_data)
                                entry = CacheEntry.from_dict(entry_dict)
                                
                                if not entry.is_expired(self.max_age_hours) and not entry.is_file_changed(file_path):
                                    # å°†çƒ­ç‚¹æ•°æ®åŠ è½½åˆ°å†…å­˜
                                    if len(self.memory_cache) < self.max_memory_items:
                                        self.memory_cache[cache_key] = entry
                                    
                                    entry.access_count += 1
                                    entry.last_access = time.time()
                                    self.stats['hits'] += 1
                                    self.stats['redis_hits'] += 1
                                    return entry.content
                                else:
                                    # åˆ é™¤è¿‡æœŸçš„Redisç¼“å­˜
                                    self.redis_client.delete(f"pdf_cache:{cache_key}")
                    except Exception as e:
                        self._log(f"âš ï¸ Redisç¼“å­˜è¯»å–å¤±è´¥: {e}")
                
                # 3. æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
                cache_file = self._get_cache_file_path(cache_key)
                if cache_file.exists():
                    try:
                        with open(cache_file, 'rb') as f:
                            entry_dict = pickle.load(f)
                            entry = CacheEntry.from_dict(entry_dict)
                            
                        if not entry.is_expired(self.max_age_hours) and not entry.is_file_changed(file_path):
                            # å°†æ•°æ®åŠ è½½åˆ°å†…å­˜
                            if len(self.memory_cache) < self.max_memory_items:
                                self.memory_cache[cache_key] = entry
                            
                            # å¦‚æœå¯ç”¨Redisï¼Œä¹Ÿå­˜å‚¨åˆ°Redis
                            if self.redis_client:
                                try:
                                    self.redis_client.setex(
                                        f"pdf_cache:{cache_key}",
                                        self.max_age_hours * 3600,
                                        pickle.dumps(entry_dict)
                                    )
                                except Exception as e:
                                    self._log(f"âš ï¸ Redisç¼“å­˜å†™å…¥å¤±è´¥: {e}")
                            
                            entry.access_count += 1
                            entry.last_access = time.time()
                            self.stats['hits'] += 1
                            self.stats['disk_hits'] += 1
                            return entry.content
                        else:
                            # åˆ é™¤è¿‡æœŸçš„æ–‡ä»¶ç¼“å­˜
                            cache_file.unlink(missing_ok=True)
                    except Exception as e:
                        self._log(f"âš ï¸ æ–‡ä»¶ç¼“å­˜è¯»å–å¤±è´¥: {e}")
                        cache_file.unlink(missing_ok=True)
                
                # ç¼“å­˜æœªå‘½ä¸­
                self.stats['misses'] += 1
                return None
                
            except Exception as e:
                self._log(f"âŒ ç¼“å­˜è·å–å¤±è´¥: {e}")
                self.stats['misses'] += 1
                return None
    
    def set(self, file_path: str, content: str, prefix: str = "") -> bool:
        """
        è®¾ç½®ç¼“å­˜å†…å®¹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            content: è¦ç¼“å­˜çš„å†…å®¹
            prefix: ç¼“å­˜é”®å‰ç¼€
            
        Returns:
            æ˜¯å¦æˆåŠŸè®¾ç½®ç¼“å­˜
        """
        with self.lock:
            try:
                # è®¡ç®—æ–‡ä»¶å“ˆå¸Œå’ŒåŸºæœ¬ä¿¡æ¯
                file_hash = self._get_file_hash(file_path)
                cache_key = self._get_cache_key(file_path, file_hash, prefix)
                
                stat = os.stat(file_path)
                current_time = time.time()
                
                # åˆ›å»ºç¼“å­˜æ¡ç›®
                entry = CacheEntry(
                    content=content,
                    file_hash=file_hash,
                    file_size=stat.st_size,
                    file_mtime=stat.st_mtime,
                    cache_time=current_time,
                    access_count=1,
                    last_access=current_time
                )
                
                # 1. å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜
                if len(self.memory_cache) >= self.max_memory_items:
                    self._evict_memory_cache()
                
                self.memory_cache[cache_key] = entry
                
                # 2. å­˜å‚¨åˆ°Redisç¼“å­˜
                if self.redis_client:
                    try:
                        self.redis_client.setex(
                            f"pdf_cache:{cache_key}",
                            self.max_age_hours * 3600,
                            pickle.dumps(entry.to_dict())
                        )
                    except Exception as e:
                        self._log(f"âš ï¸ Redisç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
                
                # 3. å­˜å‚¨åˆ°æ–‡ä»¶ç¼“å­˜
                try:
                    cache_file = self._get_cache_file_path(cache_key)
                    with open(cache_file, 'wb') as f:
                        pickle.dump(entry.to_dict(), f)
                except Exception as e:
                    self._log(f"âš ï¸ æ–‡ä»¶ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
                
                self.stats['files_cached'] += 1
                self._update_cache_size()
                
                return True
                
            except Exception as e:
                self._log(f"âŒ ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")
                return False
    
    def _evict_memory_cache(self):
        """å†…å­˜ç¼“å­˜æ·˜æ±°ç­–ç•¥ï¼šLRU"""
        if not self.memory_cache:
            return
            
        # æŒ‰æœ€åè®¿é—®æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›®
        sorted_items = sorted(
            self.memory_cache.items(),
            key=lambda x: x[1].last_access
        )
        
        # åˆ é™¤æœ€è€çš„25%æ¡ç›®
        remove_count = max(1, len(sorted_items) // 4)
        for i in range(remove_count):
            cache_key, _ = sorted_items[i]
            del self.memory_cache[cache_key]
    
    def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        cleaned_files = 0
        cleaned_size = 0
        
        try:
            # æ¸…ç†æ–‡ä»¶ç¼“å­˜
            for cache_file in self.cache_dir.rglob("*.cache"):
                try:
                    with open(cache_file, 'rb') as f:
                        entry_dict = pickle.load(f)
                        entry = CacheEntry.from_dict(entry_dict)
                    
                    if entry.is_expired(self.max_age_hours):
                        file_size = cache_file.stat().st_size
                        cache_file.unlink()
                        cleaned_files += 1
                        cleaned_size += file_size
                        
                except Exception:
                    # æŸåçš„ç¼“å­˜æ–‡ä»¶ï¼Œç›´æ¥åˆ é™¤
                    try:
                        file_size = cache_file.stat().st_size
                        cache_file.unlink()
                        cleaned_files += 1
                        cleaned_size += file_size
                    except Exception:
                        pass
            
            if cleaned_files > 0:
                self._log(f"ğŸ§¹ æ¸…ç†äº† {cleaned_files} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶ï¼Œé‡Šæ”¾ {cleaned_size/1024/1024:.1f} MBç©ºé—´")
                
        except Exception as e:
            self._log(f"âš ï¸ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def _update_cache_size(self):
        """æ›´æ–°ç¼“å­˜å¤§å°ç»Ÿè®¡"""
        try:
            total_size = 0
            for cache_file in self.cache_dir.rglob("*.cache"):
                try:
                    total_size += cache_file.stat().st_size
                except Exception:
                    pass
            
            self.stats['cache_size_mb'] = total_size / 1024 / 1024
            
            # å¦‚æœç¼“å­˜å¤§å°è¶…è¿‡é™åˆ¶ï¼Œæ¸…ç†æ—§æ–‡ä»¶
            if self.stats['cache_size_mb'] > self.max_disk_size_mb:
                self._cleanup_large_cache()
                
        except Exception as e:
            self._log(f"âš ï¸ ç¼“å­˜å¤§å°ç»Ÿè®¡å¤±è´¥: {e}")
    
    def _cleanup_large_cache(self):
        """æ¸…ç†è¿‡å¤§çš„ç¼“å­˜"""
        try:
            # æ”¶é›†æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ä¿¡æ¯
            cache_files = []
            for cache_file in self.cache_dir.rglob("*.cache"):
                try:
                    stat = cache_file.stat()
                    cache_files.append((cache_file, stat.st_mtime, stat.st_size))
                except Exception:
                    pass
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€è€çš„æ–‡ä»¶
            cache_files.sort(key=lambda x: x[1])
            
            cleaned_size = 0
            cleaned_files = 0
            target_size = self.max_disk_size_mb * 1024 * 1024 * 0.8  # æ¸…ç†åˆ°80%
            
            for cache_file, mtime, size in cache_files:
                if cleaned_size >= (self.stats['cache_size_mb'] * 1024 * 1024 - target_size):
                    break
                    
                try:
                    cache_file.unlink()
                    cleaned_size += size
                    cleaned_files += 1
                except Exception:
                    pass
            
            if cleaned_files > 0:
                self._log(f"ğŸ§¹ ç¼“å­˜ç©ºé—´æ¸…ç†: åˆ é™¤äº† {cleaned_files} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {cleaned_size/1024/1024:.1f} MB")
                self._update_cache_size()
                
        except Exception as e:
            self._log(f"âš ï¸ ç¼“å­˜ç©ºé—´æ¸…ç†å¤±è´¥: {e}")
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        with self.lock:
            # æ¸…ç©ºå†…å­˜ç¼“å­˜
            self.memory_cache.clear()
            
            # æ¸…ç©ºRedisç¼“å­˜
            if self.redis_client:
                try:
                    # ä½¿ç”¨scan_iteræ›´å®‰å…¨åœ°éå†å’Œåˆ é™¤é”®
                    keys_to_delete = []
                    for key in self.redis_client.scan_iter(match="pdf_cache:*"):
                        keys_to_delete.append(key)
                    
                    if keys_to_delete:
                        # åˆ†æ‰¹åˆ é™¤ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ é™¤å¤ªå¤šé”®
                        batch_size = 1000
                        for i in range(0, len(keys_to_delete), batch_size):
                            batch = keys_to_delete[i:i+batch_size]
                            self.redis_client.delete(*batch)
                except Exception as e:
                    self._log(f"âš ï¸ Redisç¼“å­˜æ¸…ç©ºå¤±è´¥: {e}")
            
            # æ¸…ç©ºæ–‡ä»¶ç¼“å­˜
            try:
                import shutil
                if self.cache_dir.exists():
                    shutil.rmtree(self.cache_dir)
                    self.cache_dir.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                self._log(f"âš ï¸ æ–‡ä»¶ç¼“å­˜æ¸…ç©ºå¤±è´¥: {e}")
            
            # é‡ç½®ç»Ÿè®¡
            self.stats = {
                'hits': 0,
                'misses': 0,
                'disk_hits': 0,
                'memory_hits': 0,
                'redis_hits': 0,
                'files_cached': 0,
                'cache_size_mb': 0.0
            }
            
            self._log("ğŸ§¹ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_requests = self.stats['hits'] + self.stats['misses']
            hit_rate = (self.stats['hits'] / total_requests * 100) if total_requests > 0 else 0
            
            return {
                **self.stats,
                'memory_items': len(self.memory_cache),
                'hit_rate_percent': round(hit_rate, 2),
                'total_requests': total_requests
            }
    
    def print_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        
        self._log("ğŸ“Š ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:")
        self._log(f"   ğŸ¯ å‘½ä¸­ç‡: {stats['hit_rate_percent']:.1f}% ({stats['hits']}/{stats['total_requests']})")
        self._log(f"   ğŸ§  å†…å­˜å‘½ä¸­: {stats['memory_hits']}")
        self._log(f"   ğŸ’½ ç£ç›˜å‘½ä¸­: {stats['disk_hits']}")
        if self.enable_redis:
            self._log(f"   ğŸ”´ Rediså‘½ä¸­: {stats['redis_hits']}")
        self._log(f"   ğŸ“ ç¼“å­˜æ–‡ä»¶æ•°: {stats['files_cached']}")
        self._log(f"   ğŸ’¾ å†…å­˜ç¼“å­˜æ¡ç›®: {stats['memory_items']}")
        self._log(f"   ğŸ“ ç£ç›˜ç¼“å­˜å¤§å°: {stats['cache_size_mb']:.1f} MB")